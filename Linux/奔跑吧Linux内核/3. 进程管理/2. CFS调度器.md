- 1 权重计算

- 2 进程创建

- 3 进程调度

- 4 scheduler tick

- 5 组调度

- 6 PELT算法改进

- 7 小结


本节思考题:

4. 请简述对进程调度器的理解，早期Linux内核调度器(包括0(N)和0(1))是如何工作的？
5. 请简述进程优先级、nice值和权重之间的关系。
6. 请简述CFS调度器是如何工作的。
7. CFS调度器中vruntime是如何计算的？
8. vmntime是何时更新的？
9. CFS调度器中的min\_vruntime有什么作用？
10. CFS调度器对新创建的进程和刚唤醒的进程有何关照？
11. 如何计算普通进程的平均负载load\_avg\_contrib?runnable\_avg\_sum和runnable\_avg\_period分别是什么含义？
12. 内核代码中定义了若干个表，请分别说出它们的含义，比如prio\_to\_weight、prio\_to\_wmult、runnable\_avg\_yN\_inv、runnable\_avg\_yN\_sum.
13. 如果一个普通进程在就绪队列里等待了很长时间才被调度，那么它的平均负载该如何计算？

Linux内核作为一个通用操作系统，需要兼顾各种**各样类型的进程**，包括**实时进程、交互式进程、批处理进程**等。每种类型进程都有其特别的行为特征，总结如下。

- **交互式进程**：与人机交互的进程，和鼠标、键盘、触摸屏等相关的应用，例如vim编辑器等，它们一直在睡眠同时等待用户召唤它们。这类进程的特点是系统**响应时间越快越好**，否则用户就会抱怨系统卡顿。

- **批处理进程**：此类进程默默地工作和付出，可能会占用比较**多的系统资源**，例如编译代码等。

- **实时进程**：有些应用对**整体时延有严格要求**，例如现在很火的VR设备，从头部转动到视频显示需要控制到19毫秒以内，否则会使人出现眩晕感。对于工业控制系统，不符合要求的时延可能会导致严重的事故。

本节主要讲述**普通进程的调度**，包括**交互进程**和**批处理进程**等。在CFS调度器出现之前，早期Linux内核中曾经出现过两个调度器，分别是**0(N)和0(1)调度器**。

**0(N)调度器**发布于1992年，该调度器算法比较简洁，从**就绪队列**中比较所有进程的**优先级**，然后选择一个最高优先级的进程作为下一个调度进程。**每个进程**有一个**固定时间片**，当进程**时间片使用完**之后，调度器会**选择下一个调度进程**，当所有进程都运行一遍后再重新分配时间片。这个调度器选择下一个调度进程前需要**遍历整个就绪队列**，花费0(N)时间。

Linux2.6.23内核之前有一款名为0(1)的调度器，优化了**选择下一个进程的时间**。它为**每个CPU维护一组进程优先级队列，每个优先级一个队列**，这样在选择下一个进程时，只需要**查询优先级队列相应的位图**即可知道哪个队列中有就绪进程，所以查询时间为常数0(1)。

0(1)调度器在处理某些交互式进程时依然存在问题，特别是有一些测试场景下导致交互式进程反应缓慢，另外对NUMA支持也不完善，因此大量难以维护和阅读的代码被加入该调度器中。

后来产生了CFS调度算法.**不同进程**采用**不同的调度策略**,目前Linux内核中默认实现了**4种调度策略**,分别是**deadline**、**realtime**、**CFS**和**idle**，它们分别使用struct **sched\_class**来定义**调度类**。

这**4种调度类**通过**next指针**串联在一起，**用户空间**程序可以使用调度策略API函数(sched\_setscheduler())来**设定用户进程的调度策略**。其中，SCHED\_NORMAL和SCHED\_BATCH使用CFS调度器，SCHED\_FIFO和SCHED\_RR使用realtime调度器，SCHED\_IDLE指idle调度，SCHED\_DEADLINE指deadline调度器。

```cpp
[include/uapi/linux/sched.h]
/*
 * Scheduling policies
 */
#define SCHED_NORMAL		0
#define SCHED_FIFO		1
#define SCHED_RR		2
#define SCHED_BATCH		3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE		5
#define SCHED_DEADLINE		6
```

注: sched\_setscheduler(),sched\_getscheduler()---------用户空间程序系统调用API设置和获取内核调度器的调度策略和参数。

# 1 权重计算

内核使用0〜139的数值表示进程的优先级，数值越低优先级越高。优先级0〜99给**实时进程**使用，100〜139给**普通进程**使用。另外在用户空间有一个传统的变量nice值映射到普通进程的优先级，即100〜139。

进程PCB描述符struct task\_struct数据结构中有3个成员描述进程的优先级。

```cpp
[include/linux/sched.h]

struct task_struct {
    ...
    int prio;
    int static_prio;
    int normal_prio;
	unsigned int rt_priority;
	...
};
```

StatiC\_prio是**静态优先级**，在**进程启动时分配**。内核**不存储nice值**，取而代之的是static\_prio。内核中的**宏NICE\_TO\_PRIO**()实现由nice值转换成static\_prio。它之所以被称为静态优先级是因为它**不会随着时间而改变**，用户可以**通过nice或 sched\_setscheduler等系统调用来修改该值**。

normal\_prio是**基于static\_prio和调度策略**计算出来的优先级，在创建进程时会**继承父进程的normal\_prio**。对于**普通进程**来说，normal\_prio等同于static\_prio，对于**实时进程**，会根据**rt\_priority**重新计算normal\_prio，详见effective_prio()函数。

prio保存着进程的**动态优先级**，是**调度类**考虑的优先级，有些情况下需要暂时提高进程优先级，例如实时互斥量等。

rt\_priority是**实时进程**的优先级。

内核使用struct load\_weight数据结构来记录**调度实体的权重信息**(weight).

```cpp
[include/linux/sched.h]
struct load_weight {
	unsigned long weight;
	u32 inv_weight;
};
```

其中，weight是**调度实体的权重**，inv\_weight是inverse weight的缩写，它是权重的一个中间计算结果，稍后会介绍如何使用。调度实体的数据结构中己经内嵌了 struct load\_weight结构体，用于描述调度实体的权重。

```cpp
[include/linux/sched.h]
struct sched_entity {
	struct load_weight	load;		/* for load-balancing */
	...
}
```

因此代码中经常通过p\->se.load来获取**进程p的权重信息**。

**nice值**的范围是**从\-20〜19**,进程**默认的nice值为0**。这些值含义类似级别，可以理解成有**40个等级**，nice值**越高**，则优先级**越低**，反之亦然。例如一个**CPU密集型的应用**程序**nice值从0增加到1**，那么它**相对于其他nice值为0**的应用程序将**减少10%的CPU时间**。因此进程**每降低一个nice级别**，**优先级**则**提高一个级别**，相应的进程**多获得10%的CPU时间**；反之每提升一个nice级别,优先级则降低一个级别，相应的进程少获得10%的CPU时间。为了**计算方便**，内核约定**nice值为0的权重值为1024**,其他**nice值对应的权重值**可以通过**查表的方式**来获取，内核**预先计算好了一个表prio\_to\_weight**[40]，**表下标对应nice值**[\-20〜19]。

```cpp
[kernel/sched/sched.h]
static const int prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};
```

前文所述的**10%**的影响是**相对及累加**的，例如一个进程增加了10%的CPU时间，则另外一个进程减少10%,那么差距大约是20%,因此这里使用一个系数1.25来计算的。举个例子，**进程A和进程B**的**nice值都为0**,那么**权重值都是1024**,它们获得**CPU的时间都是50%**,计算公式为1024/(1024+1024)=50%。假设进程A增加一个nice值，即nice=1, 进程B的nice值不变，那么**进程B**应该获得55%的CPU时间，进程A应该是45%。我们利用prio\_to\_weight[]表来计算，进程A=820/(1024+820)=45%,而进程B=1024/(1024+820)=55%,注意是近似等于。

内核中还提供另外一个表prio\_to\_wmult[40]，也是预先计算好的。

```cpp
[kernel/sched/sched.h]
static const u32 prio_to_wmult[40] = {
 /* -20 */     48388,     59856,     76040,     92818,    118348,
 /* -15 */    147320,    184698,    229616,    287308,    360437,
 /* -10 */    449829,    563644,    704093,    875809,   1099582,
 /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
 /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
 /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
 /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
 /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
};
```

prio\_to\_wmult[]表的计算公式如下：

![config](images/4.png)

其中，inv\_weight是inverse weight的缩写，指**权重被倒转**了，作用是为后面计算方便。

内核提供一个函数来查询这两个表，然后把值存放在p->se.load数据结构中，即struct load\_weight结构中。

```cpp
[kernel/sched/core.c]
static void set_load_weight(struct task_struct *p)
{
	int prio = p->static_prio - MAX_RT_PRIO;
	struct load_weight *load = &p->se.load;

	/*
	 * SCHED_IDLE tasks get minimal weight:
	 */
	if (p->policy == SCHED_IDLE) {
		load->weight = scale_load(WEIGHT_IDLEPRIO);
		load->inv_weight = WMULT_IDLEPRIO;
		return;
	}

	load->weight = scale_load(prio_to_weight[prio]);
	load->inv_weight = prio_to_wmult[prio];
}
```

prio\_to\_wmult[]表有什么用途呢？

在CFS调度器中有一个计算虚拟时间的核心函数calc\_delta\_fair(),它的计算公式为:

![config](images/5.png)

其中，**vruntime**表示**进程虚拟的运行时间**，delta\_exec表示**实际运行时间**，nice\_0\_weight表示nice为0的权重值，weight表示**该进程的权重值**。

vruntime该如何理解呢？如图3.4所示，假设系统中只有3个进程A、B和C,它们的NICE都为0, 也就是**权重值都是1024**。它们分配到的**运行时间相同**，即都应该分配到1/3的运行时间。如果A 、B 、C 三个进程的权重值不同呢?

![config](images/6.png)

**CFS调度器**抛弃以前**固定时间片**和**固定调度周期**的算法，而采用**进程权重值的比重来量化和计算实际运行时间**。另外引入**虚拟时钟**的概念，每个进程的**虚拟时间**是**实际运行时间相对NICE值为0的权重的比例值**。进程按照各自**不同的速率比**在**物理时钟节拍内前进**。**NICE值小**的进程，**优先级高**且**权重大**，其**虚拟时钟**比真实时钟**跑得慢**，但是可以获得**比较多的运行时间**；反之，NICE值大的进程，优先级低，权重也低，其虚拟时钟比真实时钟跑得快，反而获得比较少的运行时间。**CFS调度器**总是选择**虚拟时钟跑得慢**的进程，它像一个**多级变速箱**，**NICE为0**的进程是**基准齿轮**，其他各个进程在不同的变速比下相互追赶，从而达到公正公平。

假设某个进程nice值为1，其权重值为820,delta\_exec=10ms，导入公式计算vruntime=(10\*1024)/820，这里会涉及浮点运算。为了计算高效，函数**calc\_delta\_fair**()的计算方式变成乘法和移位运行公式如下：

>vruntime = (delta\_exec \* nice\_0\_weight \* inv\_weight) >> shift

把 inv\_weight带入计算公式后，得到如下计算公式：

![config](images/7.png)

这里巧妙地运用prio\_to\_wmult[]表预先做了除法，因此实际的计算只有乘法和移位操作，2\^32是为了预先做除法和移位操作。calc\_delta\_fair()函数等价于如下代码片段：

```cpp
[kernel/sched/fair.c]
static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
{
	if (unlikely(se->load.weight != NICE_0_LOAD))
		delta = __calc_delta(delta, NICE_0_LOAD, &se->load);

	return delta;
}

static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)
{
	u64 fact = scale_load_down(weight);
	int shift = WMULT_SHIFT;

	__update_inv_weight(lw);

	if (unlikely(fact >> 32)) {
		while (fact >> 32) {
			fact >>= 1;
			shift--;
		}
	}

	/* hint to use a 32x32->64 mul */
	fact = (u64)(u32)fact * lw->inv_weight;

	while (fact >> 32) {
		fact >>= 1;
		shift--;
	}

	return mul_u64_u32_shr(delta_exec, fact, shift);
}
```

以上讲述了进程权重、优先级和vruntime的计算方法。

下面来关注**CPU的负载计算问题**。计算一个**CPU的负载**，最简单的方法是计算CPU上**就绪队列上所有进程的权重**。仅考虑优先级权重是**有问题的**，因为没有考虑该**进程的行为**，有的进程使用的**CPU是突发性的**，有的是**恒定的**，有的是**CPU密集型**，也有的是**IO密集型**。**进程调度**考虑**优先级权重**的方法可行，但是如果延伸到**多CPU之间的负载均衡**就显得不准确了，因此从Linux3.8内核05以后进程的负载计算**不仅考虑权重**，而且跟踪**每个调度实体的负载情况**，该方法称为PELT (Pre-entity Load Tracking,详见详见 https://lwn.net/Articles/531853/)。**调度实体**数据结构中有一个**struct sched\_avg**用于描述**进程的负载**。

```cpp
[include/linux/sched.h]
struct sched_avg {
	/*
	 * These sums represent an infinite geometric series and so are bound
	 * above by 1024/(1-y).  Thus we only need a u32 to store them for all
	 * choices of y < 1-2^(-32)*1024.
	 */
	u32 runnable_avg_sum, runnable_avg_period;
	u64 last_runnable_update;
	s64 decay_count;
	unsigned long load_avg_contrib;
};

[include/linux/sched.h]
struct sched_entity {
...
// 注意在SMP情况下才有用
#ifdef CONFIG_SMP
	/* Per-entity load-tracking */
	struct sched_avg	avg;
#endif
};
```

注: SMP情况下,进程的负载才考虑.

