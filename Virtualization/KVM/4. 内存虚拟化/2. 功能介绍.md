

# QEMU中物理内存的注册

通过`kvm_vm_ioctl(KVM_SET_USER_MEMORY_REGION)`实现

本质是创建并填充了一个临时kvm_memslots结构，并把其赋值给kvm->memslots（全局的）。

# 处理用户态虚拟地址

https://blog.csdn.net/jinzhuojun/article/details/8147463

# KVM同步脏页位图到Qemu

https://frankjkl.github.io/2019/04/07/QemuKVM-Qemu%E5%90%8C%E6%AD%A5KVM%E8%84%8F%E9%A1%B5%E4%BD%8D%E5%9B%BE/

脏页位图: http://www.oenhan.com/linux-cache-writeback

插播qemu对内存条的模拟管理，是通过`RAMBlock`和`ram_list`管理的，**RAMBlock**就是**每次申请的内存池**，`ram_list`则是RAMBlock的**链表**，他们结构如下：

# EPT页表映射

内存的添加说完了，看一下[EPT页表](http://www.oenhan.com/kernel-program-exec)的映射，在`kvm_arch_vcpu_setup`中有`kvm_mmu_setup`，是mmu的初始化，EPT的初始化是`init_kvm_tdp_mmu`，所谓的初始化就是填充了`vcpu->arch.mmu`结构体，里面有很多回调函数都会用到，最终的是`tdp_page_fault`。

```cpp
context->page_fault = tdp_page_fault;
context->sync_page = nonpaging_sync_page;
context->invlpg = nonpaging_invlpg;
context->update_pte = nonpaging_update_pte;
context->shadow_root_level = kvm_x86_ops->get_tdp_level();
context->root_hpa = INVALID_PAGE;
context->direct_map = true;
context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
context->get_cr3 = get_cr3;
context->get_pdptr = kvm_pdptr_read;
context->inject_page_fault = kvm_inject_page_fault;
```

当guest访问物理内存时发生vm-exit，进入vmx_handle_exit函数，根据EXIT_REASON_EPT_VIOLATION走到handle_ept_violation函数，exit_qualification = vmcs_readl(EXIT_QUALIFICATION)获取vm-exit的退出原因，进入kvm_mmu_page_fault函数：vcpu->arch.mmu.page_fault(vcpu, cr2, error_code, false)，即是tdp_page_fault，handle_mmio_page_fault的流程不提。

```cpp
//填充kvm mmu专用的slab
r = mmu_topup_memory_caches(vcpu);
//获取gfn使用的level，即hugepage的问题
force_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);
if (likely(!force_pt_level)) {
    level = mapping_level(vcpu, gfn);
    gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
} else
    level = PT_PAGE_TABLE_LEVEL;
 
//顾名思义，快速处理一个简单的page fault
//即present同时有写权限的非mmio page fault
//参考page_fault_can_be_fast函数
//一部分处理没有写权限的page fault
//一部分处理 TLB lazy
//fast_pf_fix_direct_spte也就是将pte获取的写权限
if (fast_page_fault(vcpu, gpa, level, error_code))
    return 0;
//下面函数主要就一件事情，gfn_to_pfn
if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
      return 0;
//direct map就是映射ept页表的过程
r = __direct_map(vcpu, gpa, write, map_writable,
      level, gfn, pfn, prefault);
```

在try_async_pf中就是gfn转换成hva，然后hva转换成pfn的过程，gfn转换到hva:

```cpp
static pfn_t
__gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
             bool *async, bool write_fault, bool *writable)
{
    unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
    if (addr == KVM_HVA_ERR_RO_BAD)
        return KVM_PFN_ERR_RO_FAULT;
 
    if (kvm_is_error_hva(addr))
        return KVM_PFN_NOSLOT;
 
    /* Do not map writable pfn in the readonly memslot. */
    if (writable && memslot_is_readonly(slot)) {
        *writable = false;
        writable = NULL;
    }
 
    return hva_to_pfn(addr, atomic, async, write_fault,
              writable);
}
```

gfn2hva本质就是

```cpp
static inline unsigned long
__gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
{
    return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
}
```

而hva_to_pfn则就是host的线性区进行地址转换的问题了，不提。

```cpp
static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write,
            int map_writable, int level, gfn_t gfn, pfn_t pfn,
            bool prefault)
{
    struct kvm_shadow_walk_iterator iterator;
    struct kvm_mmu_page *sp;
    int emulate = 0;
    gfn_t pseudo_gfn;
 
    if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
        return 0;
//遍历ept四级页表
    for_each_shadow_entry(vcpu, (u64)gfn << PAGE_SHIFT, iterator) {
//如果是最后一级，level是hugepage下的level
        if (iterator.level == level) {
//设置pte，页表下一级的page地址就是pfn写入到pte
            mmu_set_spte(vcpu, iterator.sptep, ACC_ALL,
                     write, &emulate, level, gfn, pfn,
                     prefault, map_writable);
            direct_pte_prefetch(vcpu, iterator.sptep);
            ++vcpu->stat.pf_fixed;
            break;
        }
 
        drop_large_spte(vcpu, iterator.sptep);
//mmu page不在位的情况，也就是缺页
        if (!is_shadow_present_pte(*iterator.sptep)) {
            u64 base_addr = iterator.addr;
//获取指向的具体mmu page entry的index
            base_addr &= PT64_LVL_ADDR_MASK(iterator.level);
            pseudo_gfn = base_addr >> PAGE_SHIFT;
//获取mmu page
            sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,
                          iterator.level - 1,
                          1, ACC_ALL, iterator.sptep);
//将当前的mmu page的地址写入到上一级别mmu page的pte中
            link_shadow_page(iterator.sptep, sp, true);
        }
    }
    return emulate;
}
 
static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
                         gfn_t gfn,
                         gva_t gaddr,
                         unsigned level,
                         int direct,
                         unsigned access,
                         u64 *parent_pte)
{
    union kvm_mmu_page_role role;
    unsigned quadrant;
    struct kvm_mmu_page *sp;
    bool need_sync = false;
 
    role = vcpu->arch.mmu.base_role;
    role.level = level;
    role.direct = direct;
    if (role.direct)
        role.cr4_pae = 0;
    role.access = access;
    if (!vcpu->arch.mmu.direct_map
        && vcpu->arch.mmu.root_level <= PT32_ROOT_LEVEL) {
        quadrant = gaddr >> (PAGE_SHIFT + (PT64_PT_BITS * level));
        quadrant &= (1 << ((PT32_PT_BITS - PT64_PT_BITS) * level)) - 1;
        role.quadrant = quadrant;
    }
//根据一个hash索引来的
    for_each_gfn_sp(vcpu->kvm, sp, gfn) {
//检查整个mmu ept是否被失效了
        if (is_obsolete_sp(vcpu->kvm, sp))
            continue;
 
        if (!need_sync && sp->unsync)
            need_sync = true;
 
        if (sp->role.word != role.word)
            continue;
 
        if (sp->unsync && kvm_sync_page_transient(vcpu, sp))
            break;
 
        mmu_page_add_parent_pte(vcpu, sp, parent_pte);
        if (sp->unsync_children) {
            kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);
            kvm_mmu_mark_parents_unsync(sp);
        } else if (sp->unsync)
            kvm_mmu_mark_parents_unsync(sp);
 
        __clear_sp_write_flooding_count(sp);
        trace_kvm_mmu_get_page(sp, false);
        return sp;
    }
    ++vcpu->kvm->stat.mmu_cache_miss;
    sp = kvm_mmu_alloc_page(vcpu, parent_pte, direct);
    if (!sp)
        return sp;
    sp->gfn = gfn;
    sp->role = role;
//新的mmu page加入hash索引，所以前面的for循环中才能知道gfn对应的mmu有没有
//被分配
    hlist_add_head(&sp->hash_link,
        &vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]);
    if (!direct) {
        if (rmap_write_protect(vcpu->kvm, gfn))
            kvm_flush_remote_tlbs(vcpu->kvm);
        if (level > PT_PAGE_TABLE_LEVEL && need_sync)
            kvm_sync_pages(vcpu, gfn);
 
        account_shadowed(vcpu->kvm, gfn);
    }
    sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
    init_shadow_page_table(sp);
    trace_kvm_mmu_get_page(sp, true);
    return sp;
}
```

这样看每次缺页都会分配新的mmu page，虚拟机每次启动是根据guest不停的进行`EXIT_REASON_EPT_VIOLATION`，整个页表就建立起来了。