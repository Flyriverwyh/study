
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 背景](#1-背景)
- [2. EPT地址转换机制](#2-ept地址转换机制)
- [3. MMU初始化](#3-mmu初始化)
  - [3.1. 相关变量初始化: kvm模块初始化时](#31-相关变量初始化-kvm模块初始化时)
  - [3.2. MMU相关操作函数初始化: vcpu创建时](#32-mmu相关操作函数初始化-vcpu创建时)
    - [3.2.1. MMU的创建](#321-mmu的创建)
    - [3.2.2. MMU的初始化: vcpu->arch.mmu](#322-mmu的初始化-vcpu-archmmu)
- [4. SPT(影子页表/EPT页表)逻辑结构](#4-spt影子页表ept页表逻辑结构)
  - [4.1. 未开启大页](#41-未开启大页)
  - [4.2. 开启大页](#42-开启大页)
- [5. EPT构建过程](#5-ept构建过程)
  - [5.1. EPT页表的建立流程](#51-ept页表的建立流程)
  - [5.2. 虚拟机EPT和CR3初始化](#52-虚拟机ept和cr3初始化)
  - [5.3. EPT异常处理流程](#53-ept异常处理流程)
    - [5.3.1. EPT异常处理入口](#531-ept异常处理入口)
    - [5.3.2. kvm_tdp_page_fault(): 建立页表项](#532-kvm_tdp_page_fault-建立页表项)
- [6. 参考](#6-参考)

<!-- /code_chunk_output -->

# 1. 背景

在虚拟化环境下，intel CPU在处理器级别加入了对内存虚拟化的支持。即扩展页表EPT，而AMD也有类似的成为NPT。在此之前，内存虚拟化使用的一个重要技术为影子页表。

在**虚拟化环境**下，虚拟机使用的是**客户机虚拟地址GVA**，而其**本身页表机制**只能把**客户机的虚拟地址**转换成**客户机的物理地址**也就是完成`GVA->GPA`的转换，但是GPA并不是被用来真正的访存，所以需要想办法把**客户机的物理地址GPA**转换成**宿主机的物理地址HPA**。

**影子页表**采用的是一步到位式，即完成**客户机虚拟地址GVA**到**宿主机物理地址HPA**的转换，由VMM为每个客户机进程维护。本节对于影子页表不做过多描述，重点在于EPT。

内容第一部分根据intel手册分析EPT地址转换机制；第二部分借助于KVM源代码分析EPT构建过程。

# 2. EPT地址转换机制

具体见`<系统虚拟化/处理器虚拟化技术>`, 当然最权威是Intel手册

当一个逻辑CPU处于非根模式下运行客户机代码时，使用的地址是客户机虚拟地址，而访问这个虚拟地址时，同样会发生地址的转换，这里的转换还没有设计到VMM层，和正常的系统一样，这里依然是采用CR3作为基址，利用客户机页表进行地址转换，只是到这里虽然已经转换成物理地址，但是由于是客户机物理地址，不等同于宿主机的物理地址，所以并不能直接访问，需要借助于第二次的转换，也就是EPT的转换。注意EPT的维护有VMM维护，其转换过程由硬件完成，所以其比影子页表有更高的效率。

我们假设已经获取到了客户机的物理地址，下面分析下如何利用一个客户机的物理地址，通过EPT进行寻址。

![2020-03-17-16-14-05.png](./images/2020-03-17-16-14-05.png)

注意不管是32位客户机还是64位客户机，这里统一按照64位物理地址来寻址。EPT页表是4级页表，页表的大小仍然是一个页即4KB，但是一个表项是8个字节，所以一张表只能容纳512个表项，需要9位来定位具体的表项。客户机的物理地址使用低48位来完成这一工作。从上图可以看到，一个48位的客户机物理地址被分为5部分，前4部分按9位划分，最后12位作为页内偏移。当处于非根模式下的CPU使用客户机操作一个客户机虚拟地址时，首先使用客户机页表进行地址转换，得到客户机物理地址，然后CPU根据此物理地址查询EPT，在VMCS结构中有一个EPTP的指针，其中的12-51位指向EPT页表的一级目录即PML4 Table.这样根据客户机物理地址的首个9位就可以定位一个PML4 entry，一个PML4 entry理论上可以控制512GB的区域，这里不是重点，我们不在多说。PML4 entry的格式如下：

![2020-03-17-16-14-17.png](./images/2020-03-17-16-14-17.png)

1、其实这里我们只需要知道PML4 entry的`12-51`位记录下一级页表的地址，而这40位肯定是用不完的，根据CPU的架构，采取不同的位数，具体如下：

在Intel中使用MAXPHYADDR来表示最大的物理地址，我们可以通过CPUID的指令来获得处理支持的最大物理地址，然而这已经不在此次的讨论范围之内，我们需要知道的只是：
- 当MAXPHYADDR 为36位，在Intel平台的桌面处理器上普遍实现了36位的最高物理地址值，也就是我们普通的个人计算机，可寻址64G空间；
- 当MAXPHYADDR 为40位，在Inter的服务器产品和AMD 的平台上普遍实现40位的最高物理地址，可寻址达1TB；
- 当MAXPHYADDR为52位，这是x64体系结构描述最高实现值，目前尚未有处理器实现。

而对下级表的物理地址的存储4K页面寻址遵循如下规则：

① 当MAXPHYADDR为52位时，上一级table entry的`12~51`位提供下一级table物理基地址的高40位，低12位补零，达到基地址在4K边界对齐；

② 当MAXPHYADDR为40位时，上一级table entry的`12~39`位提供下一级table物理基地址的高28位，此时`40~51`是保留位，必须置0，低12位补零，达到基地址在4K边界对齐；

③ 当MAXPHYADDR为36位时，上一级table entry的`12~35`位提供下一级table物理基地址的高24位，此时`36~51`是保留位，必须置0，低12位补零，达到基地址在4K边界对齐。

而MAXPHYADDR为36位正是普通32位机的PAE模式。

2、就这么定位为下一级的页表EPT Page-Directory-Pointer-Table ，根据客户物理地址的30-38位定位此页表中的一个表项EPT Page-Directory-Pointer-Table entry。注意这里如果该表项的第7位为1，该表项指向一个1G字节的page.为0，则指向下一级页表。下面我们只考虑的是指向页表的情况。

3、然后根据表项中的12-51位，继续往下定位到第三级页表EPT Page-Directory-Pointer-Table，在根据客户物理地址的21-29位来定位到一个EPT Page-Directory-Pointer-Table Entry。如果此entry的第7位为1，则表示该entry指向一个2M的page，为0就指向下一级页表。

4、根据entry的12-51位定位第四级页表EPT Page-Directory ，然后根据客户物理地址的12-20位定位一个PDE。

PDE的12-51位指向一个4K物理页面，最后根据客户物理地址的最低12位作为偏移，定位到具体的物理地址。

# 3. MMU初始化

## 3.1. 相关变量初始化: kvm模块初始化时

```cpp
vmx_init()                               // 初始化入口
 ├─ kvm_init(KVM_GET_API_VERSION)        // 初始化KVM框架
 |   ├─ kvm_arch_init()                  // 架构相关初始化
 |   |   ├─ kvm_mmu_module_init()         // mmu模块初始化
 |   |   ├─ kvm_mmu_set_mask_ptes()       // shadow pte mask设置
 |   ├─ kvm_arch_hardware_setup()        // 
 |   |   ├─ kvm_x86_ops->hardware_setup() // 
 |   |   |  ├─ kvm_configure_mmu()        // 硬件判断和全局变量
```

```cpp
int kvm_mmu_module_init(void)
{
        int ret = -ENOMEM;

        if (nx_huge_pages == -1)
                __set_nx_huge_pages(get_nx_auto_mode());

        /*
         * MMU roles use union aliasing which is, generally speaking, an
         * undefined behavior. However, we supposedly know how compilers behave
         * and the current status quo is unlikely to change. Guardians below are
         * supposed to let us know if the assumption becomes false.
         */
        BUILD_BUG_ON(sizeof(union kvm_mmu_page_role) != sizeof(u32));
        BUILD_BUG_ON(sizeof(union kvm_mmu_extended_role) != sizeof(u32));
        BUILD_BUG_ON(sizeof(union kvm_mmu_role) != sizeof(u64));

        kvm_mmu_reset_all_pte_masks();
        // mmio
        kvm_set_mmio_spte_mask();
        // 建立缓存, 用于反向映射
        pte_list_desc_cache = kmem_cache_create("pte_list_desc",
                                            sizeof(struct pte_list_desc),
                                            0, SLAB_ACCOUNT, NULL);
        if (!pte_list_desc_cache)
                goto out;
        // 建立缓存, 用于分配 struct kvm_mmu_page
        mmu_page_header_cache = kmem_cache_create("kvm_mmu_page_header",
                                                  sizeof(struct kvm_mmu_page),
                                                  0, SLAB_ACCOUNT, NULL);
        if (!mmu_page_header_cache)
                goto out;

        if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
                goto out;
        // 当系统内存回收时的回调函数
        ret = register_shrinker(&mmu_shrinker);
        if (ret)
                goto out;

        return 0;

out:
        mmu_destroy_caches();
        return ret;
}
```

在`hardware_setup`时确认全局变量

```cpp
static __init int hardware_setup(void)
{
        ......
        if (!cpu_has_vmx_ept() ||
                !cpu_has_vmx_ept_4levels() ||
                !cpu_has_vmx_ept_mt_wb() ||
                !cpu_has_vmx_invept_global())
                enable_ept = 0;
        if (!enable_ept)
                // ept没有启用, 0
                ept_lpage_level = 0;
        else if (cpu_has_vmx_ept_1g_page())
                // 支持1G大页, 3
                ept_lpage_level = PT_PDPE_LEVEL;
        else if (cpu_has_vmx_ept_2m_page())
                // 支持2M大页, 2
                ept_lpage_level = PT_DIRECTORY_LEVEL;
        else
                // 4K页面, 1
                ept_lpage_level = PT_PAGE_TABLE_LEVEL;
        kvm_configure_mmu(enable_ept, ept_lpage_level);
        ......
}

void kvm_configure_mmu(bool enable_tdp, int tdp_page_level)
{
        tdp_enabled = enable_tdp;

        /*
         * max_page_level reflects the capabilities of KVM's MMU irrespective
         * of kernel support, e.g. KVM may be capable of using 1GB pages when
         * the kernel is not.  But, KVM never creates a page size greater than
         * what is used by the kernel for any given HVA, i.e. the kernel's
         * capabilities are ultimately consulted by kvm_mmu_hugepage_adjust().
         */
        if (tdp_enabled)
                // 1G大页, 3
                // 2M大页, 2
                // 4K页面, 1
                max_page_level = tdp_page_level;
        else if (boot_cpu_has(X86_FEATURE_GBPAGES))
                max_page_level = PT_PDPE_LEVEL;
        else
                max_page_level = PT_DIRECTORY_LEVEL;
}
EXPORT_SYMBOL_GPL(kvm_configure_mmu);
```

## 3.2. MMU相关操作函数初始化: vcpu创建时

KVM在**vcpu创建时**创建和初始化MMU，所以说KVM的**MMU**是**每个VCPU独有的**（但是有一些是共享的内容，后面会说到）。

```cpp
kvm_vm_ioctl()  // 虚拟机vm的ioctl入口
 ├─ kvm_vm_ioctl_create_vcpu()       // 创建vcpu的ioctl
 |   ├─ kvm_arch_vcpu_create()   // 初始化kvm_vcpu_arch
 |   |   |─ kvm_mmu_create(vcpu);   // 创建mmu
 |   |   |   ├─ alloc_mmu_pages(vcpu, &vcpu->arch.guest_mmu);  // 分配内存
 |   |   |   └─ alloc_mmu_pages(vcpu, &vcpu->arch.root_mmu);   // 分配内存
 |   |   |─ kvm_init_mmu(vcpu, false);   // 初始化mmu
 |   |   |   ├─ init_kvm_tdp_mmu(vcpu);  // 设置回调函数
```

### 3.2.1. MMU的创建

MMU的创建在`kvm_vm_ioctl()` --> `kvm_vm_ioctl_create_vcpu()` --> `kvm_arch_vcpu_create()` --> `kvm_mmu_create()`

```cpp
int kvm_mmu_create(struct kvm_vcpu *vcpu)
{
        uint i;
        int ret;

        vcpu->arch.mmu = &vcpu->arch.root_mmu;
        vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
        // 存储paging structure中根目录的地址(如EPT中的EPTP), 这是HPA
        vcpu->arch.root_mmu.root_hpa = INVALID_PAGE;
        // 虚拟机本身页表的cr3地址, GPA
        vcpu->arch.root_mmu.root_cr3 = 0;
        vcpu->arch.root_mmu.translate_gpa = translate_gpa;
        for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
                vcpu->arch.root_mmu.prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;

        vcpu->arch.guest_mmu.root_hpa = INVALID_PAGE;
        vcpu->arch.guest_mmu.root_cr3 = 0;
        vcpu->arch.guest_mmu.translate_gpa = translate_gpa;
        for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
                vcpu->arch.guest_mmu.prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;

        vcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;
        // guest_mmu是嵌套的情况下, L1虚拟机mmu
        ret = alloc_mmu_pages(vcpu, &vcpu->arch.guest_mmu);
        if (ret)
                return ret;
        // 非嵌套情况下的虚拟机MMU
        ret = alloc_mmu_pages(vcpu, &vcpu->arch.root_mmu);
        if (ret)
                goto fail_allocate_root;

        return ret;
 fail_allocate_root:
        free_mmu_pages(&vcpu->arch.guest_mmu);
        return ret;
}

static int alloc_mmu_pages(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
{
        struct page *page;
        int i;

        /*
         * When using PAE paging, the four PDPTEs are treated as 'root' pages,
         * while the PDP table is a per-vCPU construct that's allocated at MMU
         * creation.  When emulating 32-bit mode, cr3 is only 32 bits even on
         * x86_64.  Therefore we need to allocate the PDP table in the first
         * 4GB of memory, which happens to fit the DMA32 zone.  Except for
         * SVM's 32-bit NPT support, TDP paging doesn't use PAE paging and can
         * skip allocating the PDP table.
         */
        if (tdp_enabled && kvm_x86_ops->get_tdp_level(vcpu) > PT32E_ROOT_LEVEL)
                return 0;
        // 分配内存
        page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_DMA32);
        if (!page)
                return -ENOMEM;
        // 后面会用到
        mmu->pae_root = page_address(page);
        for (i = 0; i < 4; ++i)
                mmu->pae_root[i] = INVALID_PAGE;

        return 0;
}
```

该函数指定了`arch.walk_mmu`就是arch.mmu的地址，在KVM MMU相关的代码中经常会把`arch.walk_mmu`和`arch.mmu`混用，在这里指定了他们其实是一回事。


### 3.2.2. MMU的初始化: vcpu->arch.mmu

`kvm_vm_ioctl()` --> `kvm_vm_ioctl_create_vcpu()` --> `kvm_arch_vcpu_create()` --> `kvm_init_mmu()`:

```cpp
// arch/x86/kvm/mmu/mmu.c
static int kvm_init_mmu(struct kvm_vcpu *vcpu)
{
    if (mmu_is_nested(vcpu))
        // 嵌套虚拟化
        return init_kvm_nested_mmu(vcpu);
    else if (tdp_enabled) // 是否支持EPT
        /* 
        * EPT(Extended page table，Intel x86硬件提供的内存虚拟化技术)相关初始化
        * 主要是设置一些函数指针，其中比较重要的如缺页异常处理函数
        */
        return init_kvm_tdp_mmu(vcpu);
    else
        // 影子页表(软件实现内存虚拟化技术)相关初始化
        return init_kvm_softmmu(vcpu);
}
```

在支持EPT情况下, 调用`init_kvm_tdp_mmu`初始化MMU. 

```cpp
static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
{
        // vcpu->arch.mmu
        struct kvm_mmu *context = vcpu->arch.mmu;
        union kvm_mmu_role new_role =
                kvm_calc_tdp_mmu_root_page_role(vcpu, false);

        if (new_role.as_u64 == context->mmu_role.as_u64)
                return;

        context->mmu_role.as_u64 = new_role.as_u64;
        // page_fault 函数被初始化为 kvm_tdp_page_fault
        context->page_fault = kvm_tdp_page_fault;
        context->sync_page = nonpaging_sync_page;
        context->invlpg = nonpaging_invlpg;
        context->update_pte = nonpaging_update_pte;
        context->shadow_root_level = kvm_x86_ops->get_tdp_level(vcpu);
        context->direct_map = true;
        context->get_guest_pgd = get_cr3;
        context->get_pdptr = kvm_pdptr_read;
        context->inject_page_fault = kvm_inject_page_fault;

        if (!is_paging(vcpu)) {
                context->nx = false;
                context->gva_to_gpa = nonpaging_gva_to_gpa;
                // 没开启分页, 
                context->root_level = 0;
        } else if (is_long_mode(vcpu)) {
                context->nx = is_nx(vcpu);
                // 64位: 5级页表/4级页表
                context->root_level = is_la57_mode(vcpu) ?
                                PT64_ROOT_5LEVEL : PT64_ROOT_4LEVEL;
                reset_rsvds_bits_mask(vcpu, context);
                context->gva_to_gpa = paging64_gva_to_gpa;
        } else if (is_pae(vcpu)) {
                context->nx = is_nx(vcpu);
                // 32位开启pae, 值为3: 3级页表
                context->root_level = PT32E_ROOT_LEVEL;
                reset_rsvds_bits_mask(vcpu, context);
                context->gva_to_gpa = paging64_gva_to_gpa;
        } else {
                context->nx = false;
                // 32位非pae, 值为2: 2级页表
                context->root_level = PT32_ROOT_LEVEL;
                reset_rsvds_bits_mask(vcpu, context);
                context->gva_to_gpa = paging32_gva_to_gpa;
        }

        update_permission_bitmask(vcpu, context, false);
        update_pkru_bitmask(vcpu, context, false);
        update_last_nonleaf_level(vcpu, context);
        reset_tdp_shadow_zero_bits_mask(vcpu, context);
}
```

所谓初始化就是填充 `vcpu->arch.mmu` 结构体，里面有很多回调函数都会用到

# 4. SPT(影子页表/EPT页表)逻辑结构

KVM在还**没有EPT硬件支持**的时候，采用的是**影子页表**（shadow page table）机制，为了**和之前的代码兼容**，在当前的实现中，**EPT机制**是在**影子页表机制代码**的基础上实现的，所以**EPT**里面的**pte**和之前一样被叫做 `shadow pte`。

## 4.1. 未开启大页

在**未开启大页**的情况下，**64位**机器上，**影子页表**是**4级**结构，

- **非叶子节点表**的**表项**指向**下一级页表基地址**，如绿色所示；
- **叶子节点表**的**表项**指向**一个真正的物理页面**，如下图所示：

![2020-03-30-15-43-27.png](./images/2020-03-30-15-43-27.png)

**最开始的根表**的**级别**是`level = 4`，其次是level = 3, 一直到level = 1的表，**level1表**的**表项**指向4K的**真实物理页面**（当然这里**只是分配了HPA**，还需要配合HOST上的PF分配真实物理页面）；

**level4表**就是**影子页表(EPT**)的**根表**，其**物理地址**就被记录在**VMCS**的`EPT Pointer`中，**每个VCPU**都有一个EPT Pointer，也就是**每个VCPU！！！** 都有自己的**MMU！！！** 和**一套页表！！！**。

**gaddr**就是发生`EPT voilation`的**guest物理地址**，**GFN**就是gaddr对应的**页框号**，转换公式如下 `gfn = gaddr / PAGE_SIZE`。gfn是1的倍数。位置关系如上图所示

## 4.2. 开启大页

在开启大页的情况下，64位机器上，这里以2M大页，影子页表3级结构为例说明，如下图所示：

![2020-03-30-15-47-18.png](./images/2020-03-30-15-47-18.png)

在这里，**叶子表**变成了**level 2表**；其他的同普通影子页表相同；

在**2M大页**的情况下，**一个大页内**可以包含**512个小页**。(一个2M大页是`2^11`KB大小, 一个标准页4K`<2^2>`, 所以 `2^9` 个)

一个大页包含的小页的数目, 在KVM的代码中通过`KVM_PAGES_PER_HPAGE(level)`宏可以获得，**level**代表了**第几级页表！！！** 是**叶子节点页表！！！**。

```cpp
// arch/x86/include/asm/kvm_host.h
/* KVM Hugepage definitions for x86 */
enum {
        // 4K页面, PT(页表)是最后一级表, 里面是页表项, 即指向页面, 1
        PT_PAGE_TABLE_LEVEL   = 1,
        // 2M大页, PDT(页目录表)是最后一级表, 里面是页目录项, 即指向页面, 2
        PT_DIRECTORY_LEVEL    = 2,
        // 1G大页, PDPT(页目录指针表)是最后一级表, 里面是页目录指针项, 即指向页面, 3
        PT_PDPE_LEVEL         = 3,
        /* set max level to the biggest one */
        PT_MAX_HUGEPAGE_LEVEL = PT_PDPE_LEVEL,
};
#define KVM_NR_PAGE_SIZES       (PT_MAX_HUGEPAGE_LEVEL - \
                                 PT_PAGE_TABLE_LEVEL + 1)

/*
 * level-1 级叶子页表所管理的页面是 level级叶子页表所管理的页面大小的 2^9倍。
 * KVM_HPAGE_GFN_SHIFT 表示以level级页表为叶子页表的情况下，所管理的页面是标准页面的多少倍
 */
#define KVM_HPAGE_GFN_SHIFT(x)  (((x) - 1) * 9)
/*基本页面大小是 PAGE_SHIFT，KVM_HPAGE_GFN_SHIFT(x)是倍数关系*/
#define KVM_HPAGE_SHIFT(x)      (PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
/*当前所管理的页面大小 */
#define KVM_HPAGE_SIZE(x)       (1UL << KVM_HPAGE_SHIFT(x))
#define KVM_HPAGE_MASK(x)       (~(KVM_HPAGE_SIZE(x) - 1))
/*当前所管理的页面，是标准页面的多少倍*/
#define KVM_PAGES_PER_HPAGE(x)  (KVM_HPAGE_SIZE(x) / PAGE_SIZE)
```

gaddr就是发生`EPT voilation`的**guest物理地址**，`fn = gaddr / PAGE_SIZE`，这里的**FN**是**标准页面情况**下，**gaddr的页框号**；如果**一个大页面**中含有**512个标准页面**的话，**大页面**的**起始页框号**就应该是**512的整倍数**，如`0`，`512`, `1024`等。**对FN向下取元整！！！** 就可以得到**大页面的起始页帧号**。如下图

![2020-03-30-16-05-39.png](./images/2020-03-30-16-05-39.png)

举个例子，如果 `fn = 513`，则`gfn = 512`; 如果 `fn = 511`，`gfn = 0`;

转换为数学公式如下：

```
a = a & ~(b-1) ；
```

就是**A对B去元整**，得到的是**B的整倍数**。 如  `5 & ~(4-1) = 4`; `5 & ~(8-1) = 0`

这在我们的代码里面也是有体现的

```cpp
	gfn_t gfn = gpa >> PAGE_SHIFT; 
......
	level = mapping_level(vcpu, gfn);
	gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
......
```

`KVM_PAGES_PER_HPAGE(level) `按照上面的分析，就是**大页**是**普通页面多少倍大小**，也就是对其关系；那么这里计算出来的`gfn`就是按照**大页倍数对齐后**的**起始gfn编号**。

# 5. EPT构建过程

下面借助于KVM源代码分析下**EPT的构建过程**，其构建模式和普通页表一样，属于**中断触发式**。

即**初始页表是空的**，只有在**访问未命中**的时候**引发缺页中断**，然后缺页处理程序**构建页表**。

## 5.1. EPT页表的建立流程

1. 初始情况下：**Guest CR3**指向的Guest物理页面为空页面；
2. **Guest页表缺页异常**，KVM采用**不处理Guest页表缺页**的机制，**不会导致VM Exit**，由**Guest的缺页异常处理函数**负责分配**一个Guest物理页面（GPA**），将该页面物理地址回填，建立**Guest页表结构**；
3. 完成该映射的过程需要将**GPA翻译到HPA**，此时**该进程**相应的**EPT页表为空**，产生`EPT_VIOLATION`，虚拟机退出到**根模式**下执行，由KVM捕获该异常，建立**该GPA到HOST物理地址HPA的映射**，完成一套EPT页表的建立，**中断返回**，切换到**非根模式**继续运行。
4. **VCPU的mmu**查询下一级**Guest页表**，根据GVA的偏移产生一条**新的GPA**，Guest寻址该GPA对应页面，产生**Guest缺页**，**不发生VM_Exit**，由Guest系统的缺页处理函数捕获该异常，从Guest物理内存中选择一个空闲页，将该Guest物理地址GPA回填给Guest页表；
5. 此时该**GPA**对应的**EPT页表项不存在**，发生`EPT_VIOLATION`，切换到**根模式**下，由KVM负责建立该`GPA->HPA`映射，再切换回非根模式；
6. 如此往复，直到**非根模式下GVA**最后的偏移建立**最后一级Guest页表**，分配GPA，缺页异常退出到根模式建立最后一套EPT页表。
7. 至此，**一条GVA**对应在真实物理内存单元中的内容，便可通过这**一套二维页表结构**获得。

## 5.2. 虚拟机EPT和CR3初始化

在虚拟机vcpu启动`VCPU_RUN`时初始化

```cpp
vcpu_enter_guest()
  kvm_mmu_reload(vcpu)
    kvm_mmu_load(vcpu);
      mmu_topup_memory_caches(vcpu); // 分配缓存池
      mmu_alloc_roots(vcpu);
      kvm_mmu_sync_roots(vcpu);
      kvm_mmu_load_pgd(vcpu); // 加载pgd, 即cr3
        kvm_x86_ops->load_mmu_pgd(vcpu, vcpu->arch.mmu->root_hpa); // 调用 vmx_load_mmu_pgd()
      kvm_x86_ops->tlb_flush(vcpu, true);
```

```cpp
void vmx_load_mmu_pgd(struct kvm_vcpu *vcpu, unsigned long cr3)
{
        struct kvm *kvm = vcpu->kvm;
        bool update_guest_cr3 = true;
        unsigned long guest_cr3;
        u64 eptp;
        // 初始状态, 传入的是 INVALID_PAGE
        guest_cr3 = cr3;
        // 开启ept
        if (enable_ept) {
                eptp = construct_eptp(vcpu, cr3);
                vmcs_write64(EPT_POINTER, eptp);

                if (kvm_x86_ops->tlb_remote_flush) {
                        spin_lock(&to_kvm_vmx(kvm)->ept_pointer_lock);
                        to_vmx(vcpu)->ept_pointer = eptp;
                        to_kvm_vmx(kvm)->ept_pointers_match
                                = EPT_POINTERS_CHECK;
                        spin_unlock(&to_kvm_vmx(kvm)->ept_pointer_lock);
                }

                /* Loading vmcs02.GUEST_CR3 is handled by nested VM-Enter. */
                if (is_guest_mode(vcpu))
                        update_guest_cr3 = false;
                else if (!enable_unrestricted_guest && !is_paging(vcpu))
                        guest_cr3 = to_kvm_vmx(kvm)->ept_identity_map_addr;
                else if (test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))
                        guest_cr3 = vcpu->arch.cr3;
                else /* vmcs01.GUEST_CR3 is already up-to-date. */
                        update_guest_cr3 = false;
                ept_load_pdptrs(vcpu);
        }

        if (update_guest_cr3)
                vmcs_writel(GUEST_CR3, guest_cr3);
}
```


## 5.3. EPT异常处理流程

```cpp
vmx_handle_exit()  // vmxexit的入口
 ├─ kvm_vmx_exit_handlers[exit_reason](vcpu);      // 调用对应函数
 |   ├─ handle_ept_violation()   // ept violation异常的处理
 |   |   |─ exit_qualification = vmcs_readl(EXIT_QUALIFICATION);   // 读取exit_qualification字段
 |   |   |─ gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);   // 读取虚拟机的物理地址
 |   |   |─ error_code = XXXX;   // 拼凑error
 |   |   |─ kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);   // 处理page fault异常
 |   |   |   ├─ kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false); // 处理内存访问异常, mmio的见io部分
 |   |   |   |   └─ vcpu->arch.mmu->page_fault(vcpu, cr2_or_gpa, err, prefault);   // EPT下会调用kvm_tdp_page_fault
 |   |   |   |       └─ direct_page_fault(vcpu, gpa, error_code, prefault, max_level, true);   // EPT下会调用kvm_tdp_page_fault
 |   |   |   |           ├─ gfn = gpa >> PAGE_SHIFT;   // 虚拟机物理地址右移12位得到虚拟机物理页框号, 这是标准页面下的 GFN
 |   |   |   |           ├─ mmu_topup_memory_caches(vcpu);   // 分配缓存池
 |   |   |   |           ├─ fast_page_fault();   // 快速page fault处理
 |   |   |   |           ├─ try_async_pf();   // 根据gfn, 在memslots中查找, 得到pfn, 主机物理页框号, HPA
 |   |   |   |           |   ├─ slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);   // 得到该gfn(虚拟机页框号)对应的kvm_memory_slot
 |   |   |   |           |   └─ *pfn = __gfn_to_pfn_memslot();   // 得到该gfn(虚拟机页框号)对应的pfn(主机物理页框号), 即GPA到HPA的转换
 |   |   |   |           |        ├─ addr = __gfn_to_hva_many();   // 得到该gfn(虚拟机页框号)对应的qemu中分配的页面的HVA
 |   |   |   |           |        |   └─ return __gfn_to_hva_memslot();
 |   |   |   |           |        |       └─ return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE; // 这是gfn(虚拟机页框号)转换成主机虚拟地址(hva)
 |   |   |   |           |        └─ return hva_to_pfn(); // 得到这个主机虚拟地址(HVA)的主机物理页框号(HPA), 当然这只是一个PFN, 还需要PF完成真正页面的分配
 |   |   |   |           ├─ handle_abnormal_pfn();   // 处理反常的物理页
 |   |   |   |           └─ __direct_map();   // 完成EPT页表的构造，并在最后一级页表项中将gfn同pfn映射起来
 |   |   |   |               ├─ level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn);   // 得到该gfn(虚拟机页框号)对应的kvm_memory_slot
 |   |   |   |           |   └─ mmu_set_spte();   // 得到该gfn(虚拟机页框号)对应的pfn(主机物理页框号), 即GPA到HPA的转换
 |   |   |   └─ x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn, insn_len);   // 
```

### 5.3.1. EPT异常处理入口

**初始状态EPT页表为空**，当**客户机运行**时，其使用的**GVA转化成GPA**后，还需要CPU根据**GPA查找EPT**，从而**定位具体的HPA**，但是由于此时**EPT为空**，所以会引发**缺页中断**，发生**VM-exit**, 此时**CPU进入到根模式**，运行VMM（这里指KVM），进入`vmx_handle_exit`函数, 在KVM中定义了一个**异常处理数组**来处理**对应的VM-exit**

```cpp
static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
    ......
        [EXIT_REASON_EPT_VIOLATION]           = handle_ept_violation,
        [EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,
    ......
};
```

注意: 内存虚拟化异常有这两类, 这里讨论第一种

所以在发生EPT violation的时候，根据`EXIT_REASON_EPT_VIOLATION`走到`handle_ept_violation`函数, KVM中会执行`handle_ept_violation`：

```cpp
// arch/x86/kvm/vmx/vmx.c
static int handle_ept_violation(struct kvm_vcpu *vcpu)
{
        unsigned long exit_qualification;
        gpa_t gpa;
        u64 error_code;
        // 读取exit明细信息
        exit_qualification = vmcs_readl(EXIT_QUALIFICATION);

        /*
         * EPT violation happened while executing iret from NMI,
         * "blocked by NMI" bit has to be set before next VM entry.
         * There are errata that may cause this bit to not be set:
         * AAK134, BY25.
         */
        if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
                        enable_vnmi &&
                        (exit_qualification & INTR_INFO_UNBLOCK_NMI))
                vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);
        // 获取正在执行的虚拟机物理地址, GPA, 这就是产生异常的GPA
        gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
        trace_kvm_page_fault(gpa, exit_qualification);

        /* Is it a read fault? */
        error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
                     ? PFERR_USER_MASK : 0;
        /* Is it a write fault? */
        error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
                      ? PFERR_WRITE_MASK : 0;
        /* Is it a fetch fault? */
        error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
                      ? PFERR_FETCH_MASK : 0;
        /* ept page table entry is present? */
        error_code |= (exit_qualification &
                       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
                        EPT_VIOLATION_EXECUTABLE))
                      ? PFERR_PRESENT_MASK : 0;

        error_code |= (exit_qualification & 0x100) != 0 ?
               PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
        // exit明细信息
        vcpu->arch.exit_qualification = exit_qualification;
        return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
}
```

而该函数并没有做具体的工作，只是获取一下发生`VM-exit`的时候的一些**状态信息**如发生此VM-exit的时候正在执行的**客户物理地址GPA**、**退出原因**等，然后作为参数继续往下传递，调用`kvm_mmu_page_fault`

```cpp
// arch/x86/kvm/mmu/mmu.c
/*
 * Return values of handle_mmio_page_fault and mmu.page_fault:
 * RET_PF_RETRY: let CPU fault again on the address.
 * RET_PF_EMULATE: mmio page fault, emulate the instruction directly.
 *
 * For handle_mmio_page_fault only:
 * RET_PF_INVALID: the spte is invalid, let the real page fault path update it.
 */
enum {
        RET_PF_RETRY = 0,
        RET_PF_EMULATE = 1,
        RET_PF_INVALID = 2,
};

int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
                       void *insn, int insn_len)
{
        int r, emulation_type = EMULTYPE_PF;
        bool direct = vcpu->arch.mmu->direct_map;

        if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa)))
                return RET_PF_RETRY;

        r = RET_PF_INVALID;
        // mmio 引起的, 其实从上面的路径走不到这里, 而是从 handle_ept_misconfig()过来
        // 关于 mmio , 见io部分
        if (unlikely(error_code & PFERR_RSVD_MASK)) {
                // mmio pagefault的处理
                r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
                // mmio page fault, 直接进行指令模拟
                if (r == RET_PF_EMULATE)
                        goto emulate;
        }
        // 处理内存访问异常
        if (r == RET_PF_INVALID) {
                // spte(影子页表项/EPT页表项)无效
                r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
                                          lower_32_bits(error_code), false);
                WARN_ON(r == RET_PF_INVALID);
        }
        // 再次fault
        if (r == RET_PF_RETRY)
                return 1;
        if (r < 0)
                return r;

        /*
         * Before emulating the instruction, check if the error code
         * was due to a RO violation while translating the guest page.
         * This can occur when using nested virtualization with nested
         * paging in both guests. If true, we simply unprotect the page
         * and resume the guest.
         */
        if (vcpu->arch.mmu->direct_map &&
            (error_code & PFERR_NESTED_GUEST_PAGE) == PFERR_NESTED_GUEST_PAGE) {
                kvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(cr2_or_gpa));
                return 1;
        }

        /*
         * vcpu->arch.mmu.page_fault returned RET_PF_EMULATE, but we can still
         * optimistically try to just unprotect the page and let the processor
         * re-execute the instruction that caused the page fault.  Do not allow
         * retrying MMIO emulation, as it's not only pointless but could also
         * cause us to enter an infinite loop because the processor will keep
         * faulting on the non-existent MMIO address.  Retrying an instruction
         * from a nested guest is also pointless and dangerous as we are only
         * explicitly shadowing L1's page tables, i.e. unprotecting something
         * for L1 isn't going to magically fix whatever issue cause L2 to fail.
         */
        if (!mmio_info_in_cache(vcpu, cr2_or_gpa, direct) && !is_guest_mode(vcpu))
                emulation_type |= EMULTYPE_ALLOW_RETRY_PF;
emulate:
        /*
         * On AMD platforms, under certain conditions insn_len may be zero on #NPF.
         * This can happen if a guest gets a page-fault on data access but the HW
         * table walker is not able to read the instruction page (e.g instruction
         * page is not present in memory). In those cases we simply restart the
         * guest, with the exception of AMD Erratum 1096 which is unrecoverable.
         */
        if (unlikely(insn && !insn_len)) {
                if (!kvm_x86_ops->need_emulation_on_page_fault(vcpu))
                        return 1;
        }

        return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
                                       insn_len);
}
EXPORT_SYMBOL_GPL(kvm_mmu_page_fault);
```

首先就判断**本次exit**是否是**MMIO**引起的，如果是，则调用`handle_mmio_page_fault`函数处理MMIO pagefault,具体为何这么判断**可参考intel手册**, mmio的fault处理见io部分.

针对SPTE invalid(shadow/EPT 页表项无效), 调用`kvm_mmu_do_page_fault`

```cpp
static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
                                        u32 err, bool prefault)
{
#ifdef CONFIG_RETPOLINE
        if (likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault))
                return kvm_tdp_page_fault(vcpu, cr2_or_gpa, err, prefault);
#endif
        return vcpu->arch.mmu->page_fault(vcpu, cr2_or_gpa, err, prefault);
}
```

这里就会调用上面初始化的`kvm_tdp_page_fault()`, 该函数用于完成EPT表项的建立. 

### 5.3.2. kvm_tdp_page_fault(): 建立页表项

而**GFN**和**PFN**就一个**转换关系**，这就是**影子页表(或EPT)完成的映射**

```cpp
// arch/x86/kvm/mmu/mmu.c
int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
                       bool prefault)
{
        int max_level;
        // 从 3 开始, 大于1, 也就是3和2, 代表1G大页和2M大页
        for (max_level = PT_MAX_HUGEPAGE_LEVEL;
             max_level > PT_PAGE_TABLE_LEVEL;
             max_level--) {
                // 一个大页是一个普通页面多少倍大小
                // 1G大页是2^18倍; 2M大页是512倍
                int page_num = KVM_PAGES_PER_HPAGE(max_level);
                // gpa按照大页倍数对齐后的起始gfn编号
                gfn_t base = (gpa >> PAGE_SHIFT) & ~(page_num - 1);
                // 检查gfn(即base)范围一致性, 从而确定最大级别
                // 得到的max_level是叶子页表, 也就是其项指向页面的那一级??
                if (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))
                        break;
        }

        return direct_page_fault(vcpu, gpa, error_code, prefault,
                                 max_level, true);
}
```

调用`direct_page_fault()`

```cpp
static int direct_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
                             bool prefault, int max_level, bool is_tdp)
{
        bool write = error_code & PFERR_WRITE_MASK;
        bool exec = error_code & PFERR_FETCH_MASK;
        bool lpage_disallowed = exec && is_nx_huge_page_enabled();
        bool map_writable;
        // 虚拟机物理地址右移12位得到虚拟机物理页框号(相对于虚拟机而言)
        // 标准页面情况下的 gfn
        gfn_t gfn = gpa >> PAGE_SHIFT;
        unsigned long mmu_seq;
        kvm_pfn_t pfn;
        int r;

        if (page_fault_handle_page_track(vcpu, error_code, gfn))
                return RET_PF_EMULATE;
        // 分配缓存池
        r = mmu_topup_memory_caches(vcpu);
        if (r)
                return r;
        // 大页不开, 那就是4K页面, max_level是最后一级(叶子页表, 也就是其项指向页面的那一级)
        if (lpage_disallowed)
                max_level = PT_PAGE_TABLE_LEVEL;
        // 快速处理 violation
        // 只有当GFN对应的物理页存在且violation是由读写操作引起的，才可以使用快速处理，因为这样不用加MMU-lock.
        if (fast_page_fault(vcpu, gpa, error_code))
                return RET_PF_RETRY;

        mmu_seq = vcpu->kvm->mmu_notifier_seq;
        smp_rmb();
        // 根据虚拟机物理页框号gfn和虚拟机物理地址gpa得到qemu中分配的页面的HVA, 然后得到这个HVA页面的pfn, 当然这只是个pfn, 还需要PF完成真正页面的分配
        // 得到pfn, 主机物理页框号
        if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
                return RET_PF_RETRY;
        // 处理反常的物理页框
        if (handle_abnormal_pfn(vcpu, is_tdp ? 0 : gpa, gfn, pfn, ACC_ALL, &r))
                return r;

        r = RET_PF_RETRY;
        spin_lock(&vcpu->kvm->mmu_lock);
        if (mmu_notifier_retry(vcpu->kvm, mmu_seq))
                goto out_unlock;
        if (make_mmu_pages_available(vcpu) < 0)
                goto out_unlock;
        // 建立EPT页表结构
        r = __direct_map(vcpu, gpa, write, map_writable, max_level, pfn,
                         prefault, is_tdp && lpage_disallowed);

out_unlock:
        spin_unlock(&vcpu->kvm->mmu_lock);
        kvm_release_pfn_clean(pfn);
        return r;
}
```

调用`mmu_topup_memory_caches`函数进行**缓存池的分配**，官方的解释是为了避免在运行时分配空间失败，这里提前分配浩足额的空间，便于运行时使用。该部分内容最后单独详解。

接着调用了`fast_page_fault`尝试快速处理violation，只有当GFN对应的物理页存在且violation是由读写操作引起的，才可以使用快速处理，因为这样不用加`MMU-lock`.

假设这里**不能快速处理**，那么到后面就调用`try_async_pf`函数根据**GFN**获取对应的**PFN**，这个过程具体来说需要首先获取GFN对应的slot，转化成HVA，接着就是**正常的HOST地址翻译的过程**了，如果HVA对应的地址并不在内存中，还需要**HOST自己处理缺页中断**。

`__direct_map()`是构建页表的核心函数, 

```cpp
static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, int write,
                        int map_writable, int max_level, kvm_pfn_t pfn,
                        bool prefault, bool account_disallowed_nx_lpage)
{
        struct kvm_shadow_walk_iterator it;
        struct kvm_mmu_page *sp;
        int level, ret;
        gfn_t gfn = gpa >> PAGE_SHIFT;
        gfn_t base_gfn = gfn;

        if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa)))
                return RET_PF_RETRY;
        // 对level和gfn、pfn做出调整
        level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn);

        trace_kvm_mmu_spte_requested(gpa, level, pfn);
        // 遍历EPT页表的对应项
        // 起始状态:
        // it.level = vcpu->arch.mmu->shadow_root_level, 影子页表的级数，EPT情况下这个是4
        // it.add = gpa
        // it.shadow_addr = vcpu->arch.mmu->root_hpa, 影子页表level4页表页物理地址，EPT情况下，该值就是VMCS的EPT_pointer
        // 终止条件:
        // it.level < PT_PAGE_TABLE_LEVEL(1)
        // 当it.spte(页表项)是最后一level时, 会设置it.level=0
        // 循环:
        // --it.level
        // it.shadow_addr = spte & PT64_BASE_ADDR_MASK
        for_each_shadow_entry(vcpu, gpa, it) {
                /*
                 * We cannot overwrite existing page tables with an NX
                 * large page, as the leaf could be executable.
                 */
                disallowed_hugepage_adjust(it, gfn, &pfn, &level);

                base_gfn = gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
                // entry的level和请求的level相等, 说明该entry引起的violation, 
                // 即该entry对应的下级页或者页表不在内存中，或者直接为NULL。
                // 跳出
                if (it.level == level)
                        break;
                // 
                drop_large_spte(vcpu, it.sptep);
                /* 判断当前entry指向的页表是否存在，不存在的话需要建立 */
                if (!is_shadow_present_pte(*it.sptep)) {
                        // 当前entry指向的页表不存在
                        // 获取一个页
                        sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
                                              it.level - 1, true, ACC_ALL);
                        // 设置页表项的sptep指针指向sp
                        link_shadow_page(vcpu, it.sptep, sp);
                        if (account_disallowed_nx_lpage)
                                account_huge_nx_page(vcpu->kvm, sp);
                }
        }

        ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
                           write, level, base_gfn, pfn, prefault,
                           map_writable);
        direct_pte_prefetch(vcpu, it.sptep);
        ++vcpu->stat.pf_fixed;
        return ret;
}
```

先调用 `kvm_mmu_hugepage_adjust()` 对level和gfn、pfn做出调整

```cpp
static int kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, gfn_t gfn,
                                   int max_level, kvm_pfn_t *pfnp)
{
        struct kvm_memory_slot *slot;
        struct kvm_lpage_info *linfo;
        kvm_pfn_t pfn = *pfnp;
        kvm_pfn_t mask;
        int level;

        if (unlikely(max_level == PT_PAGE_TABLE_LEVEL))
                return PT_PAGE_TABLE_LEVEL;

        if (is_error_noslot_pfn(pfn) || kvm_is_reserved_pfn(pfn))
                return PT_PAGE_TABLE_LEVEL;

        slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, true);
        if (!slot)
                return PT_PAGE_TABLE_LEVEL;

        max_level = min(max_level, max_page_level);
        for ( ; max_level > PT_PAGE_TABLE_LEVEL; max_level--) {
                linfo = lpage_info_slot(gfn, slot, max_level);
                if (!linfo->disallow_lpage)
                        break;
        }

        if (max_level == PT_PAGE_TABLE_LEVEL)
                return PT_PAGE_TABLE_LEVEL;

        level = host_pfn_mapping_level(vcpu, gfn, pfn, slot);
        if (level == PT_PAGE_TABLE_LEVEL)
                return level;

        level = min(level, max_level);

        /*
         * mmu_notifier_retry() was successful and mmu_lock is held, so
         * the pmd can't be split from under us.
         */
        mask = KVM_PAGES_PER_HPAGE(level) - 1;
        VM_BUG_ON((gfn & mask) != (pfn & mask));
        *pfnp = pfn & ~mask;

        return level;
}
```


然后调用`mapping_level_dirty_bitmap`函数判断**当前gfn对应的slot是否可用**，当然绝大多数情况下是可用的。**为什么**要进行这样的判断呢？在if内部可以看到是**获取level**，如果**当前GPN对应的slot可用**，我们就可以获取分配slot的pagesize，然后得到**最低级的level**，比如如果是**2M的页**，那么**level就为2**，为**4K的页**，**level就为1**.

构建页表的过程即在level相等之前，发现需要的某一级的页表项为NULL，就调用kvm_mmu_get_page获取一个page，然后调用link_shadow_page设置页表项指向page

对**非叶子节点**的处理调用 `kvm_mmu_get_page` 获得**一页新的mmu page**，之后调用 `link_shadow_page` 调用 `mmu_set_spte`将其加入到EPT paging structure中。对于**叶子节点**，则**直接调用** `mmu_set_spte`。 `mmu_set_spte`调用 `set_spte` 来**设置spte**。`set_spte`是**设置页表结构的最终调用函数**。

`kvm_mmu_get_page`函数是**创建mmu页结构**的函数，在该函数主要流程如下：

1. 设置role
2. 通过role和gfn在反向映射表中查找kvm mmu page，如果存在之前创建过的page，则返回该page
3. 调用`kvm_mmu_alloc_page`创建新的`struct kvm_mmu_page`
4. 调用`hlist_add_head`将该页加入到`kvm->arch.mmu_page_hash`哈希表中
5. 调用`init_shadow_page_table`初始化对应的spt

# 6. 参考

https://www.cnblogs.com/ck1020/p/6043054.html