
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. KVM模型架构](#1-kvm模型架构)
  - [KVM运行时的三种模式](#kvm运行时的三种模式)
  - [理解KVM与Qemu的关系](#理解kvm与qemu的关系)
- [2. KVM工作原理](#2-kvm工作原理)
- [3. CPU虚拟化](#3-cpu虚拟化)
- [4. Mem虚拟化](#4-mem虚拟化)
- [参考](#参考)

<!-- /code_chunk_output -->

# 1. KVM模型架构

在所有虚拟化方案中，都是由**hypervisor取代原生的OS**去控制具体硬件资源，而同时hypervisor将资源分配具体的VM，VM中运行的是没有修改过的OS，如果让VM中的OS能正常运行，hypervisor的任务就是模拟具体的硬件资源，让OS不能识别出是真是假。

![config](images/1.png)

上面的模型是Xen示例，OS对应用而言是硬件资源管理中心，那么hypervisor就是具体VM的OS了，**KVM**是就利用了这一点，利用现有的**kernel**代码，构建了一个hypervisor，这个样子[内存分配](http://www.oenhan.com/kernel-program-exec)，[进程调度](http://www.oenhan.com/task-group-sched)等就无需重写代码，如此hypervisor就是所谓的host，VM中的OS就是guest。

guest OS保证具体运行场景中的程序正常执行，而**KVM的代码**则部署在**HOST**上，**Userspace**对应的是**QEMU**，**Kernel**对应的是**KVM Driver**，**KVM Driver**负责模拟**虚拟机的CPU运行**，[**内存管理**](http://www.oenhan.com/size-512-slab-kmalloc)，**设备管理**等；**QEMU**则**模拟虚拟机的IO设备接口**以及**用户态控制接口**。QEMU通过KVM等fd进行IOCTL控制KVM驱动的运行过程。

![config](images/2.png)

如上图所示，guest自身有自己的用户模式和[内核模式](http://www.oenhan.com/iowait-wa-vmstat)；

**guest**在**host**中是作为一个**用户态进程**存在的，这个进程就是**qemu**，qemu本身就是一个**虚拟化程序**，只是纯软件虚拟化效率很低，它被KVM进行改造后，作为**KVM的前端**存在，用来进行[**创建进程**](http://www.oenhan.com/cpu-load-balance)或者**IO交互**等；

而**KVM Driver**则是Linux内核模式，它提供**KVM fd**给**qemu**调用，用来进行**cpu虚拟化**，**内存虚拟化**等。

kvm已经是**内核模块**，被看作是一个标准的**linux 字符集设备**（/**dev/kvm**）。

Qemu通过**libkvm应用程序接口**，用**fd**通过**ioctl**向设备驱动来发送创建，运行虚拟机命令。

设备驱动kvm就会来解析命令（kvm\_dev\_ioctl函数在kvm\_main.c文件中）,如下图：


```cpp
// virt/kvm/kvm_main.c
static long kvm_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
	long r = -EINVAL;

	switch (ioctl) {
	case KVM_GET_API_VERSION:
		if (arg)
			goto out;
		r = KVM_API_VERSION;
		break;
	case KVM_CREATE_VM:
		r = kvm_dev_ioctl_create_vm(arg);
		break;
	case KVM_CHECK_EXTENSION:
		r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
		break;
	case KVM_GET_VCPU_MMAP_SIZE:
		if (arg)
			goto out;
		r = PAGE_SIZE;     /* struct kvm_run */
#ifdef CONFIG_X86
		r += PAGE_SIZE;    /* pio data page */
#endif
#ifdef CONFIG_KVM_MMIO
		r += PAGE_SIZE;    /* coalesced mmio ring page */
#endif
		break;
	case KVM_TRACE_ENABLE:
	case KVM_TRACE_PAUSE:
	case KVM_TRACE_DISABLE:
		r = -EOPNOTSUPP;
		break;
	default:
		return kvm_arch_dev_ioctl(filp, ioctl, arg);
	}
out:
	return r;
}
```

KVM Driver使得整个Linux成为一个虚拟机监控器，负责接收**qemu模拟效率很低**的命令。

## KVM运行时的三种模式

所以在虚拟机运行时，有三种模式，分别是：

- 客户模式：我们可以简单理解成客户机操作系统运行在的模式，它本身又分为自己的内核模式和用户模式。

- 用户模式：为用户提供虚拟机管理的用户空间工具以及代表用户执行I/O，Qemu运行在这个模式之下。

- 内核模式：模拟CPU以及内存，实现客户模式的切换，处理从客户模式的退出。KVM内核模块运行在这个模式下。

三种模式的层次关系我们可以用图简单描述一下：

![三种模式的关系](images/4.png)

![](./images/2019-06-04-15-01-11.png)

## 理解KVM与Qemu的关系

我们都知道开源虚拟机KVM，并且知道它总是跟Qemu结合出现，那这两者之间有什么关系呢？

首先，Qemu本身并不是KVM的一部分，而是一整套完整的虚拟化解决方案，它是纯软件实现的，包括处理器虚拟化、内存虚拟化以及各种虚拟设备的模拟，但因为是纯软件模拟，所以性能相对比较低。

而广义的KVM实际上包含两部分，一部分是基于Linux内核支持的KVM内核模块，另一部分就是经过简化和修改Qemu。

KVM内核模块模拟处理器和内存以支持虚拟机的运行，Qemu主要处理I/O以及为用户提供一个用户空间工具来进行虚拟机的管理。两者相互结合，相辅相成，构成了一个完整的虚拟化平台。



# 2. KVM工作原理

![config](images/3.png)

上图是一个执行过程图，首先启动一个**虚拟化管理软件qemu**，开始启动一个虚拟机，通过**ioctl**等**系统调用**向**内核**中**申请指定的资源**，搭建好虚拟环境，启动虚拟机内的OS，执行 **VMLAUCH** 指令，即**进入了guest代码**执行过程。

如果 Guest OS 发生**外部中断**或者**影子页表缺页**之类的事件，暂停 Guest OS 的执行，退出QEMU即**guest VM\-exit**，进行一些必要的处理，然后重新进入客户模式，执行guest代码；这个时候如果是io请求，则提交给用户态下的qemu处理，qemu处理后再次通过IOCTL反馈给KVM驱动。

KVM的大致工作原理：用户模式的Qemu利用接口libkvm通过**ioctl系统调用**进入内核模式。KVMDriver为虚拟机创建虚拟CPU和虚拟内存，然后执行VMLAUNCH指令进入**客户模式**，装载Guest OS并运行。Guest OS运行过程中如果发生中断或者影子缺页等异常，将暂停Guest OS的运行并保存当前上下文退出到内核模式来处理这些异常。内核模式处理这些异常时如果不需要I/O则处理完成后重新进入客户模式，如果需要I/O则进入到用户模式，由Qemu来处理I/O，处理完成后进入内核模式，再进入客户模式。

KVM的工作原理如图：

![工作原理](images/5.png)

# 3. CPU虚拟化

X86体系结构**CPU虚拟化**技术的称为 **Intel VT\-x** 技术，引入了VMX，提供了两种处理器的工作环境。 VMCS 结构实现两种环境之间的切换。 VM Entry 使虚拟机进去guest模式，VM Exit 使虚拟机退出guest模式。

VMM调度guest执行时，qemu 通过 ioctl [系统调用](http://oenhan.com/kernel-program-exec)进入内核模式，在 KVM Driver中获得当前物理 CPU的引用。之后将guest状态从VMCS中读出, 并装入物理CPU中。执行 VMLAUCH 指令使得物理处理器进入非根操作环境，运行guest OS代码。

当 guest OS 执行一些特权指令或者外部事件时，比如I/O访问，对控制寄存器的操作，MSR的读写等， 都会导致物理CPU发生 VMExit， 停止运行 Guest OS，将 Guest OS保存到VMCS中，Host 状态装入物理处理器中， 处理器进入根操作环境，KVM取得控制权，通过读取 VMCS 中 VM_EXIT_REASON 字段得到引起 VM Exit 的原因。 从而调用kvm_exit_handler 处理函数。 如果由于 I/O 获得信号到达，则退出到userspace模式的 Qemu 处理。处理完毕后，重新进入guest模式运行虚拟 CPU。

# 4. Mem虚拟化

OS对于物理内存主要有两点认识：1.物理地址从0开始；2.[内存地址](http://www.oenhan.com/kernel-program-exec)是连续的。

VMM接管了所有内存，但guest OS的对内存的使用就存在这两点冲突了，除此之外，一个guest对内存的操作很有可能影响到另外一个guest乃至host的运行。VMM的内存虚拟化就要解决这些问题。

在OS代码中，应用也是占用所有的逻辑地址，同时不影响其他应用的关键点在于有线性地址这个中间层；解决方法则是添加了一个中间层：**guest物理地址空间**；guest看到是从0开始的guest物理地址空间（类比从0开始的线性地址），而且是连续的，虽然有些地址没有映射；同时guest物理地址映射到不同的host逻辑地址，如此保证了VM之间的安全性要求。

这样MEM虚拟化就是GVA\->GPA\->HPA的寻址过程，传统软件方法有**影子页表**，硬件虚拟化提供了EPT支持。

# 参考

http://oenhan.com/kvm-src-1

https://www.linuxidc.com/Linux/2015-01/112328.htm