








## 中断注入

它是指**将中断写入VMCS对应的中断信息位**，来实现中断的注入，当中断完成后通过**读取中断的返回信息**来分析中断是否正确。

中断注入在KVM内部流程起始于一个函数`kvm_set_irq`

```cpp
/*
 * Return value:
 *  < 0   Interrupt was ignored (masked or not delivered for other reasons)
 *  = 0   Interrupt was coalesced (previous irq is still pending)
 *  > 0   Number of CPUs interrupt was delivered to
 */
int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
                bool line_status)
{
        struct kvm_kernel_irq_routing_entry irq_set[KVM_NR_IRQCHIPS];
        int ret = -1, i, idx;

        trace_kvm_set_irq(irq, level, irq_source_id);

        /* Not possible to detect if the guest uses the PIC or the
         * IOAPIC.  So set the bit in both. The guest will ignore
         * writes to the unused one.
         */
        idx = srcu_read_lock(&kvm->irq_srcu);
        // 获取同一个irq注册的所有中断路由项, 存于irq_set, 返回数量
        i = kvm_irq_map_gsi(kvm, irq_set, irq);
        srcu_read_unlock(&kvm->irq_srcu, idx);
        /* 依次调用同一个irq上的所有芯片的set方法 */
        while (i--) {
                int r;
                r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
                                   line_status);
                if (r < 0)
                        continue;

                ret = r + ((ret < 0) ? 0 : ret);
        }

        return ret;
}
```

各个参数的意思:

- **kvm**指定特定的虚拟机

- `irq_source_id`是**中断源ID**，一般有`KVM_USERSPACE_IRQ_SOURCE_ID`和`KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID`, 对于KVM设备我们都会申请一个中断资源ID，注册KVM IO设备时申请的

- **irq**是**全局的中断号**，这个是转化GSI之前的，比如时钟是0号，这里就是0，而不是32

- **level**指定**高低电平**，需要注意的是，针对**边沿触发**，需要**两个电平触发来模拟**，**先高电平再低电平**。

回到函数中，首先要收集的是**同一irq**上注册的**所有的设备信息**，这主要在于**irq共享**的情况，非共享的情况下最多就一个。**设备信息**抽象成一个`kvm_kernel_irq_routing_entry`，这里临时放到`irq_set`数组中。

然后对于数组中的**每个元素**，调用**其set方法**，目前大都是**APIC架构**，因此set方法基本都是`kvm_set_ioapic_irq`，在**传统pic**情况下，是`kvm_set_pic_irq`。

我们以`kvm_set_ioapic_irq`为例进行分析，该函数**没有实质性的操作**，就调用了`kvm_ioapic_set_irq`函数

```cpp
static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
                              struct kvm *kvm, int irq_source_id, int level,
                              bool line_status)
{
        // 获取该虚拟机的ioapic
        struct kvm_ioapic *ioapic = kvm->arch.vioapic;
        return kvm_ioapic_set_irq(ioapic, e->irqchip.pin, irq_source_id, level,
                                line_status);
}

int kvm_ioapic_set_irq(struct kvm_ioapic *ioapic, int irq, int irq_source_id,
                       int level, bool line_status)
{
        int ret, irq_level;

        BUG_ON(irq < 0 || irq >= IOAPIC_NUM_PINS);

        spin_lock(&ioapic->lock);
        /*判断请求高电平还是低电平*/
        irq_level = __kvm_irq_line_state(&ioapic->irq_states[irq],
                                         irq_source_id, level);
        ret = ioapic_set_irq(ioapic, irq, irq_level, line_status);

        spin_unlock(&ioapic->lock);

        return ret;
}

static int ioapic_set_irq(struct kvm_ioapic *ioapic, unsigned int irq,
                int irq_level, bool line_status)
{
        union kvm_ioapic_redirect_entry entry;
        // irq对应的位
        u32 mask = 1 << irq;
        u32 old_irr;
        int edge, ret;
        // 该irq对应的重定向表项
        entry = ioapic->redirtbl[irq];
        /*判断触发方式*/
        edge = (entry.fields.trig_mode == IOAPIC_EDGE_TRIG);
        /* 如果低电平, 表明是模拟边沿触发的第二次触发??? 清理irr对应位后直接返回*/
        // 说明电平触发只是在high电平触发
        if (!irq_level) {
                // 清理该irq位
                ioapic->irr &= ~mask;
                ret = 1;
                // 直接返回
                goto out;
        }

        /*
         * AMD SVM AVIC accelerate EOI write and do not trap,
         * in-kernel IOAPIC will not be able to receive the EOI.
         * In this case, we do lazy update of the pending EOI when
         * trying to set IOAPIC irq.
         */
        if (kvm_apicv_activated(ioapic->kvm))
                ioapic_lazy_update_eoi(ioapic, irq);

        /*
         * Return 0 for coalesced interrupts; for edge-triggered interrupts,
         * this only happens if a previous edge has not been delivered due
         * to masking.  For level interrupts, the remote_irr field tells
         * us if the interrupt is waiting for an EOI.
         *
         * RTC is special: it is edge-triggered, but userspace likes to know
         * if it has been already ack-ed via EOI because coalesced RTC
         * interrupts lead to time drift in Windows guests.  So we track
         * EOI manually for the RTC interrupt.
         */
        if (irq == RTC_GSI && line_status &&
                rtc_irq_check_coalesced(ioapic)) {
                ret = 0;
                goto out;
        }
        // 原来的irr寄存器
        old_irr = ioapic->irr;
        // irr相应位置位
        ioapic->irr |= mask;
        // 如果是边沿触发
        if (edge) {
                ioapic->irr_delivered &= ~mask;
                // 边沿触发且旧的irr寄存器与请求的irr相等
                if (old_irr == ioapic->irr) {
                        ret = 0;
                        goto out;
                }
        }
        // 1. 边沿触发, 旧的irr寄存器与请求的irr不等
        // 或者
        // 2. 电平触发
        ret = ioapic_service(ioapic, irq, line_status);

out:
        trace_kvm_ioapic_set_irq(entry.bits, irq, ret == 0);
        return ret;
}
```

到这里，中断已经到达**模拟的IO-APIC**了，IO-APIC最重要的就是它的**重定向表**，针对重定向表的操作主要在`ioapic_service`中，之前都是做一些准备工作，在进入`ioapic_service`函数之前，主要有**两个任务**：

1、**判断触发方式**，主要是区分**电平触发**和**边沿触发**。

2、设置**ioapic的irr寄存器**。

之前我们说过，**边沿触发**需要**两个水平触发**来**模拟**，**前后电平相反**。这里就要先做判断是对应哪一次。**只有首次触发**才会进行后续的操作，而**二次触发**相当于**reset操作**，就是把**ioapic的irr寄存器清除**。

边沿触发且旧irr与新的不等, 或者, 电平触发，就会对其进行更新，进入`ioapic_service`函数。

```cpp
static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)
{
        // 获取重定向表的相应项
        union kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];
        struct kvm_lapic_irq irqe;
        int ret;
        // 1. 该entry设置了mask
        // 或者
        // 2. 电平触发的
        // 直接返回
        if (entry->fields.mask ||
            (entry->fields.trig_mode == IOAPIC_LEVEL_TRIG &&
            entry->fields.remote_irr))
                return -1;

        irqe.dest_id = entry->fields.dest_id;
        // 向量号
        irqe.vector = entry->fields.vector;
        irqe.dest_mode = kvm_lapic_irq_dest_mode(!!entry->fields.dest_mode);
        irqe.trig_mode = entry->fields.trig_mode;
        irqe.delivery_mode = entry->fields.delivery_mode << 8;
        irqe.level = 1;
        irqe.shorthand = APIC_DEST_NOSHORT;
        irqe.msi_redir_hint = false;
        // 边沿触发
        if (irqe.trig_mode == IOAPIC_EDGE_TRIG)
                ioapic->irr_delivered |= 1 << irq;
        // RTC
        if (irq == RTC_GSI && line_status) {
                /*
                 * pending_eoi cannot ever become negative (see
                 * rtc_status_pending_eoi_check_valid) and the caller
                 * ensures that it is only called if it is >= zero, namely
                 * if rtc_irq_check_coalesced returns false).
                 */
                BUG_ON(ioapic->rtc_status.pending_eoi != 0);
                ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
                                               &ioapic->rtc_status.dest_map);
                ioapic->rtc_status.pending_eoi = (ret < 0 ? 0 : ret);
        } else
                ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);

        if (ret && irqe.trig_mode == IOAPIC_LEVEL_TRIG)
                entry->fields.remote_irr = 1;

        return ret;
}
```

该函数比较简单，就是**根据irq号**，获取**重定向表中的一项**，判断`kvm_ioapic_redirect_entry`**没有设置mask**，然后根据`kvm_ioapic_redirect_entry`，**构建**`kvm_lapic_irq`，这就类似于在**总线上的传递过程**。

之后调用`kvm_irq_delivery_to_apic`，该函数会**把消息传递给相应的VCPU** ，

```cpp
int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
                struct kvm_lapic_irq *irq, struct dest_map *dest_map)
{
        int i, r = -1;
        struct kvm_vcpu *vcpu, *lowest = NULL;
        unsigned long dest_vcpu_bitmap[BITS_TO_LONGS(KVM_MAX_VCPUS)];
        unsigned int dest_vcpus = 0;

        if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
                return r;

        if (irq->dest_mode == APIC_DEST_PHYSICAL &&
            irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
                printk(KERN_INFO "kvm: apic: phys broadcast and lowest prio\n");
                irq->delivery_mode = APIC_DM_FIXED;
        }

        memset(dest_vcpu_bitmap, 0, sizeof(dest_vcpu_bitmap));

        kvm_for_each_vcpu(i, vcpu, kvm) {
                if (!kvm_apic_present(vcpu))
                        continue;

                if (!kvm_apic_match_dest(vcpu, src, irq->shorthand,
                                        irq->dest_id, irq->dest_mode))
                        continue;

                if (!kvm_lowest_prio_delivery(irq)) {
                        if (r < 0)
                                r = 0;
                        r += kvm_apic_set_irq(vcpu, irq, dest_map);
                } else if (kvm_apic_sw_enabled(vcpu->arch.apic)) {
                        if (!kvm_vector_hashing_enabled()) {
                                if (!lowest)
                                        lowest = vcpu;
                                else if (kvm_apic_compare_prio(vcpu, lowest) < 0)
                                        lowest = vcpu;
                        } else {
                                __set_bit(i, dest_vcpu_bitmap);
                                dest_vcpus++;
                        }
                }
        }

        if (dest_vcpus != 0) {
                int idx = kvm_vector_to_index(irq->vector, dest_vcpus,
                                        dest_vcpu_bitmap, KVM_MAX_VCPUS);

                lowest = kvm_get_vcpu(kvm, idx);
        }

        if (lowest)
                r = kvm_apic_set_irq(lowest, irq, dest_map);

        return r;
}

int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
                     struct dest_map *dest_map)
{
        struct kvm_lapic *apic = vcpu->arch.apic;

        return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
                        irq->level, irq->trig_mode, dest_map);
}
```

具体需要调用`kvm_apic_set_irq`函数，继而调用`__apic_accept_irq`，该函数中会根据**不同的传递模式**处理消息,大部分情况都是`APIC_DM_FIXED`，在**该模式**下，**中断**被传递到**特定的CPU**，其中会调用`kvm_x86_ops->deliver_posted_interrupt`，实际上对应于`vmx.c`中的`vmx_deliver_posted_interrupt`

```cpp
/*
 * Add a pending IRQ into lapic.
 * Return 1 if successfully added and 0 if discarded.
 */
static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
                             int vector, int level, int trig_mode,
                             struct dest_map *dest_map)
{
        int result = 0;
        struct kvm_vcpu *vcpu = apic->vcpu;

        trace_kvm_apic_accept_irq(vcpu->vcpu_id, delivery_mode,
                                  trig_mode, vector);
        switch (delivery_mode) {
        case APIC_DM_LOWEST:
                vcpu->arch.apic_arb_prio++;
                /* fall through */
        case APIC_DM_FIXED:
                if (unlikely(trig_mode && !level))
                        break;

                /* FIXME add logic for vcpu on reset */
                if (unlikely(!apic_enabled(apic)))
                        break;

                result = 1;

                if (dest_map) {
                        __set_bit(vcpu->vcpu_id, dest_map->map);
                        dest_map->vectors[vcpu->vcpu_id] = vector;
                }

                if (apic_test_vector(vector, apic->regs + APIC_TMR) != !!trig_mode) {
                        if (trig_mode)
                                kvm_lapic_set_vector(vector,
                                                     apic->regs + APIC_TMR);
                        else
                                kvm_lapic_clear_vector(vector,
                                                       apic->regs + APIC_TMR);
                }

                if (kvm_x86_ops.deliver_posted_interrupt(vcpu, vector)) {
                        kvm_lapic_set_irr(vector, apic);
                        kvm_make_request(KVM_REQ_EVENT, vcpu);
                        kvm_vcpu_kick(vcpu);
                }
                break;

        case APIC_DM_REMRD:
                result = 1;
                vcpu->arch.pv.pv_unhalted = 1;
                kvm_make_request(KVM_REQ_EVENT, vcpu);
                kvm_vcpu_kick(vcpu);
                break;

        case APIC_DM_SMI:
                result = 1;
                kvm_make_request(KVM_REQ_SMI, vcpu);
                kvm_vcpu_kick(vcpu);
                break;

        case APIC_DM_NMI:
                result = 1;
                kvm_inject_nmi(vcpu);
                kvm_vcpu_kick(vcpu);
                break;

        case APIC_DM_INIT:
                if (!trig_mode || level) {
                        result = 1;
                        /* assumes that there are only KVM_APIC_INIT/SIPI */
                        apic->pending_events = (1UL << KVM_APIC_INIT);
                        kvm_make_request(KVM_REQ_EVENT, vcpu);
                        kvm_vcpu_kick(vcpu);
                }
                break;

        case APIC_DM_STARTUP:
                result = 1;
                apic->sipi_vector = vector;
                /* make sure sipi_vector is visible for the receiver */
                smp_wmb();
                set_bit(KVM_APIC_SIPI, &apic->pending_events);
                kvm_make_request(KVM_REQ_EVENT, vcpu);
                kvm_vcpu_kick(vcpu);
                break;

        case APIC_DM_EXTINT:
                /*
                 * Should only be called by kvm_apic_local_deliver() with LVT0,
                 * before NMI watchdog was enabled. Already handled by
                 * kvm_apic_accept_pic_intr().
                 */
                break;

        default:
                printk(KERN_ERR "TODO: unsupported delivery mode %x\n",
                       delivery_mode);
                break;
        }
        return result;
}
```

```cpp
/*
 * Send interrupt to vcpu via posted interrupt way.
 * 1. If target vcpu is running(non-root mode), send posted interrupt
 * notification to vcpu and hardware will sync PIR to vIRR atomically.
 * 2. If target vcpu isn't running(root mode), kick it to pick up the
 * interrupt from PIR in next vmentry.
 */
static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
{
        struct vcpu_vmx *vmx = to_vmx(vcpu);
        int r;

        r = vmx_deliver_nested_posted_interrupt(vcpu, vector);
        if (!r)
                return 0;

        if (!vcpu->arch.apicv_active)
                return -1;
        // 设置位图
        if (pi_test_and_set_pir(vector, &vmx->pi_desc))
                return 0;
        /*标记位图更新标志*/
        /* If a previous notification has sent the IPI, nothing to do.  */
        if (pi_test_and_set_on(&vmx->pi_desc))
                return 0;

        if (!kvm_vcpu_trigger_posted_interrupt(vcpu, false))
                kvm_vcpu_kick(vcpu);

        return 0;
}

static inline bool kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,
                                                     bool nested)
{
#ifdef CONFIG_SMP
        int pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;

        if (vcpu->mode == IN_GUEST_MODE) {
                /*
                 * The vector of interrupt to be delivered to vcpu had
                 * been set in PIR before this function.
                 *
                 * Following cases will be reached in this block, and
                 * we always send a notification event in all cases as
                 * explained below.
                 *
                 * Case 1: vcpu keeps in non-root mode. Sending a
                 * notification event posts the interrupt to vcpu.
                 *
                 * Case 2: vcpu exits to root mode and is still
                 * runnable. PIR will be synced to vIRR before the
                 * next vcpu entry. Sending a notification event in
                 * this case has no effect, as vcpu is not in root
                 * mode.
                 *
                 * Case 3: vcpu exits to root mode and is blocked.
                 * vcpu_block() has already synced PIR to vIRR and
                 * never blocks vcpu if vIRR is not cleared. Therefore,
                 * a blocked vcpu here does not wait for any requested
                 * interrupts in PIR, and sending a notification event
                 * which has no effect is safe here.
                 */

                apic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);
                return true;
        }
#endif
        return false;
}
```

这里主要是设置`vmx->pi_desc`中的位图即`struct pi_desc`中的**pir字段**，其是一个**32位的数组**，共**8项**。因此最大标记**256个中断**，**每个中断向量**对应一位。设置好后，请求`KVM_REQ_EVENT`事件，在**下次vm-entry**的时候会进行**中断注入**。


## 具体注入过程

在`vcpu_enter_guest` (x86.c)函数中,有这么一段代码

```cpp
        //检查是否有事件请求
        if (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win) {
                ++vcpu->stat.req_event;
                kvm_apic_accept_events(vcpu);
                if (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {
                        r = 1;
                        goto out;
                }
                // 注入阻塞的事件，中断，异常和nmi等
                /*注入中断在vcpu加载到真实cpu上后，相当于某些位已经被设置*/
                if (inject_pending_event(vcpu) != 0)
                        req_immediate_exit = true;
                else {
                        /* Enable SMI/NMI/IRQ window open exits if needed.
                         *
                         * SMIs have three cases:
                         * 1) They can be nested, and then there is nothing to
                         *    do here because RSM will cause a vmexit anyway.
                         * 2) There is an ISA-specific reason why SMI cannot be
                         *    injected, and the moment when this changes can be
                         *    intercepted.
                         * 3) Or the SMI can be pending because
                         *    inject_pending_event has completed the injection
                         *    of an IRQ or NMI from the previous vmexit, and
                         *    then we request an immediate exit to inject the
                         *    SMI.
                         */
                        if (vcpu->arch.smi_pending && !is_smm(vcpu))
                                if (!kvm_x86_ops.enable_smi_window(vcpu))
                                        req_immediate_exit = true;
                        /* 使能NMI/IRQ window，参见Intel64 System Programming Guide 25.3节（P366）
                         * 当使能了interrupt-window exiting或NMI-window exiting(由VMCS中相关字段控制)，
                         * 表示在刚进入虚拟机后，就会立刻因为有pending或注入的中断导致VM-exit
                         */
                        if (vcpu->arch.nmi_pending)
                                kvm_x86_ops.enable_nmi_window(vcpu);
                        // 收集中断
                        if (kvm_cpu_has_injectable_intr(vcpu) || req_int_win)
                                kvm_x86_ops.enable_irq_window(vcpu);
                        WARN_ON(vcpu->arch.exception.pending);
                }

                if (kvm_lapic_enabled(vcpu)) {
                        update_cr8_intercept(vcpu);
                        kvm_lapic_sync_to_vapic(vcpu);
                }
        }
```

即在**进入非根模式之前**会检查`KVM_REQ_EVENT`事件，如果**存在pending的事件**，则调用`kvm_apic_accept_events`**接收**，这里主要是处理**APIC初始化期间**和**IPI中断**的，暂且不关注。

之后会调用`inject_pending_event`，在这里会检查当前**是否有可注入的中断**，而具体检查过程时首先会通过`kvm_cpu_has_injectable_intr`函数，其中调用`kvm_apic_has_interrupt->apic_find_highest_irr->vmx_sync_pir_to_irr, vmx_sync_pir_to_irr`函数**对中断进行收集**，就是检查`vmx->pi_desc`中的**位图**，如果有，则会调用`kvm_apic_update_irr`把信息**更新到apic寄存器**里。然后调用`apic_search_irr`获取**IRR寄存器中的中断**，没找到的话会返回-1. 找到后调用`kvm_queue_interrupt`，**把中断记录到vcpu**中。

```cpp
static int inject_pending_event(struct kvm_vcpu *vcpu)
{
        int r;

        /* try to reinject previous events if any */

        if (vcpu->arch.exception.injected)
                kvm_x86_ops.queue_exception(vcpu);
        /*
         * Do not inject an NMI or interrupt if there is a pending
         * exception.  Exceptions and interrupts are recognized at
         * instruction boundaries, i.e. the start of an instruction.
         * Trap-like exceptions, e.g. #DB, have higher priority than
         * NMIs and interrupts, i.e. traps are recognized before an
         * NMI/interrupt that's pending on the same instruction.
         * Fault-like exceptions, e.g. #GP and #PF, are the lowest
         * priority, but are only generated (pended) during instruction
         * execution, i.e. a pending fault-like exception means the
         * fault occurred on the *previous* instruction and must be
         * serviced prior to recognizing any new events in order to
         * fully complete the previous instruction.
         */
        else if (!vcpu->arch.exception.pending) {
                if (vcpu->arch.nmi_injected)
                        kvm_x86_ops.set_nmi(vcpu);
                else if (vcpu->arch.interrupt.injected)
                        kvm_x86_ops.set_irq(vcpu);
        }

        /*
         * Call check_nested_events() even if we reinjected a previous event
         * in order for caller to determine if it should require immediate-exit
         * from L2 to L1 due to pending L1 events which require exit
         * from L2 to L1.
         */
        if (is_guest_mode(vcpu) && kvm_x86_ops.check_nested_events) {
                r = kvm_x86_ops.check_nested_events(vcpu);
                if (r != 0)
                        return r;
        }

        /* try to inject new event if pending */
        if (vcpu->arch.exception.pending) {
                trace_kvm_inj_exception(vcpu->arch.exception.nr,
                                        vcpu->arch.exception.has_error_code,
                                        vcpu->arch.exception.error_code);

                WARN_ON_ONCE(vcpu->arch.exception.injected);
                vcpu->arch.exception.pending = false;
                vcpu->arch.exception.injected = true;

                if (exception_type(vcpu->arch.exception.nr) == EXCPT_FAULT)
                        __kvm_set_rflags(vcpu, kvm_get_rflags(vcpu) |
                                             X86_EFLAGS_RF);

                if (vcpu->arch.exception.nr == DB_VECTOR) {
                        /*
                         * This code assumes that nSVM doesn't use
                         * check_nested_events(). If it does, the
                         * DR6/DR7 changes should happen before L1
                         * gets a #VMEXIT for an intercepted #DB in
                         * L2.  (Under VMX, on the other hand, the
                         * DR6/DR7 changes should not happen in the
                         * event of a VM-exit to L1 for an intercepted
                         * #DB in L2.)
                         */
                        kvm_deliver_exception_payload(vcpu);
                        if (vcpu->arch.dr7 & DR7_GD) {
                                vcpu->arch.dr7 &= ~DR7_GD;
                                kvm_update_dr7(vcpu);
                        }
                }

                kvm_x86_ops.queue_exception(vcpu);
        }

        /* Don't consider new event if we re-injected an event */
        if (kvm_event_needs_reinjection(vcpu))
                return 0;

        if (vcpu->arch.smi_pending && !is_smm(vcpu) &&
            kvm_x86_ops.smi_allowed(vcpu)) {
                vcpu->arch.smi_pending = false;
                ++vcpu->arch.smi_count;
                enter_smm(vcpu);
        } else if (vcpu->arch.nmi_pending && kvm_x86_ops.nmi_allowed(vcpu)) {
                --vcpu->arch.nmi_pending;
                vcpu->arch.nmi_injected = true;
                kvm_x86_ops.set_nmi(vcpu);
        } else if (kvm_cpu_has_injectable_intr(vcpu)) {
                /*
                 * Because interrupts can be injected asynchronously, we are
                 * calling check_nested_events again here to avoid a race condition.
                 * See https://lkml.org/lkml/2014/7/2/60 for discussion about this
                 * proposal and current concerns.  Perhaps we should be setting
                 * KVM_REQ_EVENT only on certain events and not unconditionally?
                 */
                if (is_guest_mode(vcpu) && kvm_x86_ops.check_nested_events) {
                        r = kvm_x86_ops.check_nested_events(vcpu);
                        if (r != 0)
                                return r;
                }
                if (kvm_x86_ops.interrupt_allowed(vcpu)) {
                        // 将中断记录到vcpu中
                        kvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu),
                                            false);
                        // 写入vmcs结构中
                        kvm_x86_ops.set_irq(vcpu);
                }
        }

        return 0;
}
```

```cpp
static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
    bool soft)
{
    vcpu->arch.interrupt.pending = true;
    vcpu->arch.interrupt.soft = soft;
    vcpu->arch.interrupt.nr = vector;
}
```

最后会调用`kvm_x86_ops->set_irq`，进行中断注入的最后一步，即写入到vmcs结构中。该函数指针指向`vmx_inject_irq`

```cpp
static void vmx_inject_irq(struct kvm_vcpu *vcpu)
{
        struct vcpu_vmx *vmx = to_vmx(vcpu);
        uint32_t intr;
        //中断号
        int irq = vcpu->arch.interrupt.nr;

        trace_kvm_inj_virq(irq);

        ++vcpu->stat.irq_injections;
        if (vmx->rmode.vm86_active) {
                int inc_eip = 0;
                if (vcpu->arch.interrupt.soft)
                        inc_eip = vcpu->arch.event_exit_inst_len;
                kvm_inject_realmode_interrupt(vcpu, irq, inc_eip);
                return;
        }
        // 设置有中断向量的有效性
        intr = irq | INTR_INFO_VALID_MASK;
        // 如果是软件中断
        if (vcpu->arch.interrupt.soft) {
                // 内部中断
                intr |= INTR_TYPE_SOFT_INTR;
                // 软件中断需要写入指令长度
                vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,
                             vmx->vcpu.arch.event_exit_inst_len);
        } else
                // 标记为外部中断
                intr |= INTR_TYPE_EXT_INTR;
        // 写入vmxs的VM_ENTRY_INTR_INFO_FIELD中
        vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);

        vmx_clear_hlt(vcpu);
}
```

最终会写入到vmcs的VM_ENTRY_INTR_INFO_FIELD中，这需要按照一定的格式。具体格式详见intel手册。0-7位是向量号，8-10位是中断类型（硬件中断或者软件中断），最高位是有效位，12位是NMI标志。

```cpp
// arch/x86/include/asm/vmx.h
/*
 * Interruption-information format
 */
#define INTR_INFO_VECTOR_MASK           0xff            /* 7:0 */
#define INTR_INFO_INTR_TYPE_MASK        0x700           /* 10:8 */
#define INTR_INFO_DELIVER_CODE_MASK     0x800           /* 11 */
#define INTR_INFO_UNBLOCK_NMI           0x1000          /* 12 */
#define INTR_INFO_VALID_MASK            0x80000000      /* 31 */
```

## 中断路由表的初始化


中断虚拟化流程

```cpp
kvm_set_irq
　　kvm_ioapic_set_irq
　　　　　ioapic_service
　　　　　　ioapic_deliver
　　　　　　　　kvm_irq_delivery_to_apic
　　　　　　　　　　kvm_apic_set_irq
　　　　　　　　　　　　__apic_accept_irq
　　　　　　　　　　　　　　vmx_deliver_posted_interrupt

具体注入阶段
vcpu_enter_guest
　　　kvm_apic_accept_events
　　　　　　inject_pending_event
　　　　　　　　kvm_queue_interrupt
　　　　　　　　　　vmx_inject_irq
　　　　　　　　　　　　vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);

中断路由表初始化

x86.c kvm_arch_vm_ioctl
　　kvm_setup_default_irq_routing irq_common.c
　　　　kvm_set_irq_routing irq_chip.c
　　　　　　setup_routing_entry irq_chip.c
　　　　　　　　kvm_set_routing_entry irq_chip.c
　　　　　　　　　　e->set = kvm_set_ioapic_irq; irq_common.c
```

# 参考

https://www.cnblogs.com/ck1020/p/7424922.html