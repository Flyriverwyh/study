
以时钟中断为例

一个操作系统要跑起来，必须有Time Tick，它就像是身体的脉搏。普通情况下，OS Time Tick由PIT(i8254)或APIC Timer设备提供—PIT定期(1ms in Linux)产生一个timer interrupt，作为global tick, APIC Timer产生一个local tick。在虚拟化情况下，必须为guest OS模拟一个PIT和APIC Timer。

模拟的PIT和APIC Timer不能像真正硬件那样物理计时，所以一般用HOST的某种系统服务或软件计时器来为这个模拟PIT提供模拟”时钟源”。

目前两种方案：1. 用户态模拟方案（QEMU）； 2. 内核态模拟方案（KVM）；




# 物理芯片介绍

## PIT主要为Intel 8254 PIT芯片

**PIT**(**Programmable Interval Timer**), **可编程间隔定时器**

**每个PC机**中都有一个**PIT**, 通过**IRQ**产生**周期性的时钟中断信号**来充当**系统定时器**.

i386使用通常是Intel 8254 PIT芯片, 它的I/O端口地址范围是`40h ~ 43h`.

8254 PIT有**3个计时通道**, 每个通道都有其不同的用途:

- **通道0**用来负责**更新系统时钟**. 它在**每个时钟滴答**会通过**IRQ0**向系统发出一次**时钟中断信号**.
- **通道1**通常用来控制**DMAC对RAM的刷新**
- **通道3**被连接到**PC机的扬声器**, 以产生**方波信号**.

## PIC主要为8259A PIC芯片

**PIC**(**Programmable Interrupt Controller**), **可编程中断控制器**

它具有`IR0 ~ IR7`共**8个中断管脚**连接到**外部设备**. **中断管脚**具有**优先级**, 其中IR0优先级最高, IR7最低.

PIC有三个重要的寄存器:

(1) IRR(Interrupt Request Register, 中断请求寄存器)共8位, 对应IR0到IR7这8个中断管脚. 某位置为1表明收到了对应管脚的中断但未提交到CPU.

(2) ISR(Interrupt Service Register, 中断请求寄存器): 共8位, 某位置为1表明对应管脚的中断已经提交到CPU处理, 但CPU还未处理完.

(3) IMR(Interrupt Mask Register, 中断屏蔽寄存器): 共8位, 某位置为1表明对应的中断管脚被屏蔽.

# 整体流程

整个主要流程:

![2020-04-18-17-59.png](./images/2020-04-18-17-59.png)

修正: 重定向表中有目标lapic的id

# QEMU

PIC创建:

```cpp
// kvm-all.c/kvm_init/kvm_irqchip_create
kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP)
```

PIT创建:

```cpp
// i8254.c
kvm_vm_ioctl(kvm_state, KVM_CREATE_PIT)
```

# PIC(8259)的创建

## 整体流程

```cpp
kvm_vm_ioctl()  // vm ioctl的入口
 ├─  kvm_arch_vm_ioctl()     
 |   ├─ irqchip_in_kernel(kvm)  // 如果irqchip在kvm中实现, 则直接返回, 不执行任何动作
 |   ├─ kvm_pic_init()   // pic创建, 8259
 |   ├─ kvm_ioapic_init()   // ept violation异常的处理
 |   ├─ kvm_setup_default_irq_routing()   // ept violation异常的处理
 |   └─ kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;  // ept violation异常的处理
 |   |   |─ exit_qualification = vmcs_readl(EXIT_QUALIFICATION);   // 读取exit_qualification字段
 |   |   |─ gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);   // 读取虚拟机的物理地址
 |   |   |─ error_code = XXXX;   // 拼凑error
 |   |   |─ kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);   // 处理page fault异常
 |   |   |   ├─ kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false); // 处理内存访问异常, mmio的见io部分
 |   |   |   |   └─ vcpu->arch.mmu->page_fault(vcpu, cr2_or_gpa, err, prefault);   // EPT下会调用kvm_tdp_page_fault
 |   |   |   |       └─ direct_page_fault(vcpu, gpa, error_code, prefault, max_level, true);   // EPT下会调用kvm_tdp_page_fault
 |   |   |   |           ├─ gfn = gpa >> PAGE_SHIFT;   // 虚拟机物理地址右移12位得到虚拟机物理页框号, 这是标准页面下的 GFN
 |   |   |   |           ├─ mmu_topup_memory_caches(vcpu);   // 分配缓存池
 |   |   |   |           ├─ fast_page_fault();   // 快速page fault处理
 |   |   |   |           ├─ try_async_pf();   // 根据gfn, 在memslots中查找, 得到pfn, 主机物理页框号, HPA
 |   |   |   |           |   ├─ slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);   // 得到该gfn(虚拟机页框号)对应的kvm_memory_slot
 |   |   |   |           |   └─ *pfn = __gfn_to_pfn_memslot();   // 得到该gfn(虚拟机页框号)对应的pfn(主机物理页框号), 即GPA到HPA的转换
 |   |   |   |           |        ├─ addr = __gfn_to_hva_many();   // 得到该gfn(虚拟机页框号)对应的qemu中分配的页面的HVA
 |   |   |   |           |        |   └─ return __gfn_to_hva_memslot();
 |   |   |   |           |        |       └─ return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE; // 这是gfn(虚拟机页框号)转换成主机虚拟地址(hva)
 |   |   |   |           |        └─ return hva_to_pfn(); // 得到这个主机虚拟地址(HVA)的主机物理页框号(HPA), 当然这只是一个PFN, 还需要PF完成真正页面的分配, 如果HVA对应的地址并不在内存中，还需要HOST自己处理缺页中断
 |   |   |   |           ├─ handle_abnormal_pfn();   // 处理反常的物理页
 |   |   |   |           └─ __direct_map();   // 完成EPT页表的构造，并在最后一级页表项中将gfn同pfn映射起来
 |   |   |   |               ├─ level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn);   // 获取到该gfn对应的level, 基于vcpu->kvm->mm(qemu的mm_struct)
 |   |   |   |               ├─ for_each_shadow_entry(vcpu, gpa, it);   // 遍历EPT页表
 |   |   |   |               ├─ it.level == level: break;   //  如果页表的level等于请求的level, 表明是该entry引起的violation(说明到了叶子节点), 跳出
 |   |   |   |               ├─ sp = kvm_mmu_get_page();   // 对于非叶子节点, 如果该entry页表项值为0, 表明下一级页表页不存在, 则分配当前entry的下一级(level-1, 指向的页表)页表的kvm_mmu_page(sp)
 |   |   |   |               ├─ link_shadow_page(vcpu, it.sptep, sp);   // 非叶子结点, 下一级页表页不存在时, 将分配的下一级页表页链接到当前entry, 这是HPA
 |   |   |   |           |        ├─ mmu_spte_set(sptep, spte);   // 将sp处理后成为spte, 将其添加到当前entry(sptep), HPA
 |   |   |   |           |        └─ mmu_page_add_parent_pte(vcpu, sp, sptep); // 当前页表页(sp)会被多个上级页表项引用, 将所有上级页表项的parent_spte添加到当前页表页的patent_ptes链表中
 |   |   |   |           |   └─ mmu_set_spte();   // 设置最后一级页表项(即表项指向真正的页面), sptep指向pfn, HPA
 |   |   |   └─ x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn, insn_len);   // 
```

## 代码分析

代码入口在

```cpp
        case KVM_CREATE_IRQCHIP: {
                mutex_lock(&kvm->lock);

                r = -EEXIST;
                // 如果irqchip在kvm中实现, 则不用创建
                if (irqchip_in_kernel(kvm))
                        goto create_irqchip_unlock;

                r = -EINVAL;
                if (kvm->created_vcpus)
                        goto create_irqchip_unlock;
                // 主要过程, 是8259
                r = kvm_pic_init(kvm);
                if (r)
                        goto create_irqchip_unlock;
                // ioapic
                r = kvm_ioapic_init(kvm);
                if (r) {
                        kvm_pic_destroy(kvm);
                        goto create_irqchip_unlock;
                }

                r = kvm_setup_default_irq_routing(kvm);
                if (r) {
                        kvm_ioapic_destroy(kvm);
                        kvm_pic_destroy(kvm);
                        goto create_irqchip_unlock;
                }
                /* Write kvm->irq_routing before enabling irqchip_in_kernel. */
                smp_wmb();
                kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
        create_irqchip_unlock:
                mutex_unlock(&kvm->lock);
                break;
        }
```


```cpp
// arch/x86/kvm/i8259.c
int kvm_pic_init(struct kvm *kvm)
{
        struct kvm_pic *s;
        int ret;
        // 分配 kvm_pic 结构体
        s = kzalloc(sizeof(struct kvm_pic), GFP_KERNEL_ACCOUNT);
        if (!s)
                return -ENOMEM;
        spin_lock_init(&s->lock);
        // 该pic所属的kvm虚拟机
        s->kvm = kvm;
        s->pics[0].elcr_mask = 0xf8;
        s->pics[1].elcr_mask = 0xde;
        s->pics[0].pics_state = s;
        s->pics[1].pics_state = s;

        /*
         * Initialize PIO device
         */
        kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
        kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
        kvm_iodevice_init(&s->dev_eclr, &picdev_eclr_ops);
        mutex_lock(&kvm->slots_lock);
        ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
                                      &s->dev_master);
        if (ret < 0)
                goto fail_unlock;

        ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
        if (ret < 0)
                goto fail_unreg_2;

        ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_eclr);
        if (ret < 0)
                goto fail_unreg_1;

        mutex_unlock(&kvm->slots_lock);

        kvm->arch.vpic = s;

        return 0;

fail_unreg_1:
        kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &s->dev_slave);

fail_unreg_2:
        kvm_io_bus_unregister_dev(kvm, KVM_PIO_BUS, &s->dev_master);

fail_unlock:
        mutex_unlock(&kvm->slots_lock);

        kfree(s);

        return ret;
}
```