
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. KVM模块初始化流程](#1-kvm模块初始化流程)
- [2. QEMU与KVM交互](#2-qemu与kvm交互)
- [3. /dev/kvm的ioctl接口kvm_dev_ioctl](#3-devkvm的ioctl接口kvm_dev_ioctl)
- [4. 虚拟机的创建](#4-虚拟机的创建)
  - [4.1. 基本原理](#41-基本原理)
  - [4.2. 基本流程](#42-基本流程)
  - [4.3. 代码分析](#43-代码分析)
    - [4.3.1. 虚拟机创建过程kvm_dev_ioctl_create_vm](#431-虚拟机创建过程kvm_dev_ioctl_create_vm)
      - [4.3.1.1. kvm_create_vm流程](#4311-kvm_create_vm流程)
      - [4.3.1.2. 返回kvm_vm的fd文件描述符](#4312-返回kvm_vm的fd文件描述符)
- [5. 虚拟机fd的ioctl接口kvm_vm_ioctl](#5-虚拟机fd的ioctl接口kvm_vm_ioctl)
- [6. vCPU的创建](#6-vcpu的创建)
  - [6.1. 基本原理](#61-基本原理)
  - [6.2. 基本流程](#62-基本流程)
  - [6.3. 代码分析](#63-代码分析)
    - [6.3.1. vCPU的创建过程kvm_vm_ioctl_create_vcpu](#631-vcpu的创建过程kvm_vm_ioctl_create_vcpu)
      - [6.3.1.1. kvm_arch_vcpu_create创建kvm_vcpu](#6311-kvm_arch_vcpu_create创建kvm_vcpu)
      - [6.3.1.2. kvm_arch_vcpu_setup初始化kvm_vcpu](#6312-kvm_arch_vcpu_setup初始化kvm_vcpu)
      - [6.3.1.3. vCPU检测](#6313-vcpu检测)
      - [6.3.1.4. 创建vcpu_fd](#6314-创建vcpu_fd)
      - [6.3.1.5. 释放内核锁和返回vcpu_fd](#6315-释放内核锁和返回vcpu_fd)
- [7. vCPU的运行](#7-vcpu的运行)
  - [7.1. 基本原理](#71-基本原理)
  - [7.2. 基本流程](#72-基本流程)
  - [7.3. 代码分析](#73-代码分析)

<!-- /code_chunk_output -->

# 1. KVM模块初始化流程

先有KVM相关模块的初始化. 从而生成`/dev/kvm`文件句柄给用户空间使用.

# 2. QEMU与KVM交互

KVM内核模块加载之后只提供了一个对外接口/dev/kvm，qemu通过/dev/kvm文件依次创建vm句柄和vcpu句柄来和kvm交互。

所以存在三种句柄: KVM访问句柄、虚拟机访问句柄和vCPU访问句柄

![](./images/2019-06-04-15-01-11.png)

整体流程是:

```
qemu_open("/dev/kvm", O_RDWR); //KVM访问句柄
kvm_ioctl(s, KVM_GET_API_VERSION, 0);
kvm_ioctl(s,KVM_CREATE_VM, 0); //创建vm访问句柄
kvm_arch_init ==>
kvm_vm_ioctl(s,KVM_SET_IDENTITY_MAP_ADDR, &identity_base）

           kvm_vm_ioctl(s,KVM_SET_TSS_ADDR, identity_base + 0x1000)

           kvm_vm_ioctl(s,KVM_SET_NR_MMU_PAGES, shadow_mem)
==>kvm_irqchip_create(s) ==> kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP)

kvm_init_vcpu(env);
==> kvm_vm_ioctl(s, KVM_CREATE_VCPU, env->cpu_index);创建vcpu句柄

	kvm_cpu_exec
	==>kvm_vcpu_ioctl(env, KVM_RUN, 0);
	
	//根据Exit的原因处理VM-Exit

       switch (run->exit_reason) {

       case KVM_EXIT_IO: //VM Exit Cause by IO operation

           kvm_handle_io();

```

# 3. /dev/kvm的ioctl接口kvm_dev_ioctl

在上面初始化中调用了`misc_register(&kvm_dev);`注册了`/dev/kvm`

`/dev/kvm`的ioctl的接口`kvm_dev_ioctl`主要提供了创建vm和校验版本号的功能

```cpp
static long kvm_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
	long r = -EINVAL;

	switch (ioctl) {
	case KVM_GET_API_VERSION:             //获取api版本
		if (arg)
			goto out;
		r = KVM_API_VERSION;
		break;
	case KVM_CREATE_VM:                    //创建VM，返回vmfd
		r = kvm_dev_ioctl_create_vm(arg);
		break;
	case KVM_CHECK_EXTENSION:
		r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
		break;
	case KVM_GET_VCPU_MMAP_SIZE:
		if (arg)
			goto out;
		r = PAGE_SIZE;     /* struct kvm_run */
#ifdef CONFIG_X86
		r += PAGE_SIZE;    /* pio data page */
#endif
#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
		r += PAGE_SIZE;    /* coalesced mmio ring page */
#endif
		break;

	default:
		return kvm_arch_dev_ioctl(filp, ioctl, arg); //默认创建设备
	}
out:
	return r;
}
```

# 4. 虚拟机的创建

## 4.1. 基本原理

基于KVM的虚拟机创建分为**虚拟机创建**和**虚拟CPU创建**两个步骤。

在下文的描述中，**虚拟机**对应的**文件描述符**为**vm\_fd**，**虚拟CPU**对应的**文件描述符**为**vcpu\_fd**。

**打开/dev/kvm** 文件并且获得**文件描述符 fd** 后，通过 `kvm_dev_ioctl` 指令写入`KVM_CREATE_VM`，即可创建一个 VM 虚拟机。

对`虚拟机(VM)`来说，**kvm结构体**是关键，**一个虚拟机**对应**一个kvm结构体**，虚拟机的创建过程实质为**kvm结构体的创建**和**初始化过程**。

## 4.2. 基本流程

整体流程如下:

```
用户态ioctl(fd,KVM_CREATE_VM,..)
    内核态kvm_dev_ioctl()
        kvm_dev_ioctl_create_vm()
            kvm_create_vm() //实现虚拟机创建的主要函数
                kvm_arch_alloc_vm() // 分配kvm结构体
                kvm_arch_init_vm() // 初始化kvm结构中的架构相关部分，比如中断
                hardware_enable_all() // 使能硬件，架构相关操作
                    hardware_enable_nolock
                        kvm_arch_hardware_enable()
                            kvm_shared_msr_cpu_online()     // kvm shared msr
                            kvm_x86_ops->hardware_enable()  // 会调用 vmxon 指令
                                crash_enable_local_vmclear()  // 清理位图
                                kvm_cpu_vmxon()  // vmxon打开VMX模式
                            tsc初始化规则
                kzalloc() // 分配memslots结构，并初始化为0
                kvm_init_memslots_id() // 初始化内存槽位(slot)的id信息
                kvm_eventfd_init() // 初始化事件通道
                kvm_init_mmu_notifier() // 初始化mmu操作的通知链
                list_add(&kvm->vm_list, &vm_list) // 将新创建的虚拟机的kvm结构，加入到全局链表vm_list中
```

## 4.3. 代码分析

kvm结构体见上节

### 4.3.1. 虚拟机创建过程kvm_dev_ioctl_create_vm

KVM 的**该部分代码**实现在**kvm\_dev** 的 **file\_operation 结构体**中，对应的代码在 **kvm\_main.c** 中调用 **kvm\_dev\_ioctl\_create\_vm**函数实现，其代码如下：

代码5\-7 KVM\_CREATE\_VM实现代码

```cpp
// virt/kvm/kvm_main.c
static long kvm_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
(1880)        case KVM_CREATE_VM:￼
(1881)             r = -EINVAL;￼
(1882)             if (arg)￼
(1883)                  goto out;￼
(1884)             r = kvm_dev_ioctl_create_vm();￼
(1885)             break;
```

**kvm\_dev\_ioctl\_create\_vm**函数(virt/kvm/kvm\_main.c)通过调用**kvm\_create\_vm**函数对**KVM结构体**进行创建。

```cpp
// virt/kvm/kvm_main.c
static int kvm_dev_ioctl_create_vm(unsigned long type)
{
	int r;
	struct kvm *kvm;

	kvm = kvm_create_vm(type);			// 创建VM
	if (IS_ERR(kvm))
		return PTR_ERR(kvm);
#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
	r = kvm_coalesced_mmio_init(kvm);
	if (r < 0) {
		kvm_put_kvm(kvm);
		return r;
	}
#endif
	r = anon_inode_getfd("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);	//挂载vmfd的ioctl接口
	if (r < 0)
		kvm_put_kvm(kvm);

	return r;
}
```

**KVM 结构体**如前文所述，保存了**虚拟机运行的上下文**及**其他相关状态**，在**使用之前**，需要进行**一定的初始化**工作。

#### 4.3.1.1. kvm_create_vm流程

虚拟机创建的主要函数

```cpp
// virt/kvm/kvm_main.c
static struct kvm *kvm_create_vm(unsigned long type)
{
	int r, i;
	/* 
     * 分配kvm结构体，一个虚拟机对应一个kvm结构，其中包括了虚拟机中的
     * 关键系统，比如内存、中断、VCPU、总线等信息，该结构体也是kvm的关键结
     * 构体之一
     */
	struct kvm *kvm = kvm_arch_alloc_vm();
	//  // 初始化kvm结构中的架构相关部分，比如中断
	r = kvm_arch_init_vm(kvm, type);
	// 硬件使能，最终调用架构相关的kvm_x86_ops->hardware_enable()接口
	r = hardware_enable_all();
	// 分配memslots结构，并初始化为0
	kvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);
    // 初始化内存槽位(slot)的id信息，便于后续索引
	kvm_init_memslots_id(kvm);
	if (init_srcu_struct(&kvm->srcu))
		goto out_err_nosrcu;
	// 初始化虚拟机的bus信息
	for (i = 0; i < KVM_NR_BUSES; i++) {
		kvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),
					GFP_KERNEL);
		if (!kvm->buses[i])
			goto out_err;
	}
	// 初始化mmu_lock
	spin_lock_init(&kvm->mmu_lock);
	// 设置虚拟机的mm(mm_struct)为当前进程的mm
	kvm->mm = current->mm;
	atomic_inc(&kvm->mm->mm_count);
	// 初始化事件通道
	kvm_eventfd_init(kvm);
	// 初始化mmu操作的通知链
	r = kvm_init_mmu_notifier(kvm);

	raw_spin_lock(&kvm_lock);
	// 将新创建的虚拟机的kvm结构，加入到全局链表vm_list中
	list_add(&kvm->vm_list, &vm_list);
	raw_spin_unlock(&kvm_lock);

	return kvm;
}
```

先**分配一个虚拟机struct kvm数据结构**

在 x86体系架构中，**KVM结构体**的**初始化**任务在**kvm\_arch\_init\_vm**函数(arch/x86/kvm/x86.c)中进行，进行了**分配内存**、**初始化设备列表**、设置**中断管理**和**初始化 tsc 的spin\_lock的功能**。

在**完成之后**，将执行**硬件初始化**工作，该部分硬件初始化工作通过调用**on\_each\_cpu宏**，将在**每个物理CPU**上执行**同样的操作**。该操作主要是尝试将**所有的CPU**切换入**vitualize模式**(使用`vmxon`指令)，并且设置好**时钟**等信息，这个过程通过 **kvm\_arch\_hardware\_enable** 函数完成。

该函数代码(arch/x86/kvm/x86.c)如下，主要执行了两个工作：

- 处理kvm shared msr信息
- **整理CPU的时钟信息**；
- 调用kvm\_x86\_ops的**硬件相关**的函数进行具体操作, 这其中调用了`vmxon`使每个cpu进入Virtualization模式.

代码5\-8 kvm\_arch\_hardware\_enable函数代码

```cpp
￼(5799)   int kvm_arch_hardware_enable(void *garbage)￼
(5800)   {￼
(5801)        struct kvm *kvm;￼
(5802)        struct kvm_vcpu *vcpu;￼
(5803)        int i;￼
(5804)￼
(5805)        kvm_shared_msr_cpu_online();￼
(5806)        list_for_each_entry(kvm, &vm_list, vm_list)￼
(5807)             kvm_for_each_vcpu(i, vcpu, kvm)￼
(5808)                  if (vcpu->cpu == smp_processor_id())￼
(5809)                       kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);￼
(5810)        return kvm_x86_ops->hardware_enable(garbage);￼
(5811)   }
```

接下来，将初始化 KVM 的 **memslot 结构体**、**Bus 总线结构体信息**、**scru读/写锁信息**、**eventfd事件通知信息**、**mmu内存管理结构体**信息。

#### 4.3.1.2. 返回kvm_vm的fd文件描述符

然后，调用 **anon\_inode\_getfd** 函数。

该函数设置了对**所有 KVM 的操作**都将给予**kvm\_vm这个共享文件**进行，该共享文件的操作封装在**kvm\_vm\_fops**结构体中，对**VM的操作**实际上就是对此文件的操作。因此，对其**ioctl调用**的是**kvm\_vm\_fops中的成员函数**。

代码5\-9 调用anon\_inode\_getfd创建kvm\-vm

```cpp
// 
￼(1840)        fd = anon_inode_getfd("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);￼
(1841)        if (fd < 0)￼
(1842)             kvm_put_kvm(kvm);￼
(1843)￼
(1844)        return fd;
```

通过anon\_inode\_getfd获得的**fd文件描述符**，就是供**用户态**使用的**vm\_fd**，用户态将通过该fd进行进一步的虚拟机操作，首先要做的事情是**初始化vCPU**。

# 5. 虚拟机fd的ioctl接口kvm_vm_ioctl

上面调用后会得到虚拟机的fd, 对应的ioctl是`kvm_vm_ioctl()`

```cpp
// virt/kvm/kvm_main.c
static long kvm_vm_ioctl(struct file *filp,
			   unsigned int ioctl, unsigned long arg)
```

# 6. vCPU的创建

## 6.1. 基本原理

创建vCPU实际上就是**创建vCPU的描述符**，在KVM中，**vCPU对应的数据结构体**为**kvm\_vcpu**。因此，创建vCPU的描述符，简单来说就是**分配相应大小的内存**，并且进行相应的**初始化**工作。

**kvm\_vcpu**中描述符包含的内容有很多，通常会包含各个**平台通用的内容**和**平台相关的内容**。

在**物理CPU上电**之后，需要进一步初始化才可以使用。在这个过程中，硬件会自动将CPU初始化成特定的状态。**kvm\_vcpu的初始化**也是一个**类似的过程**，将 **kvm\_vcpu 的各个数据结构体**设置成为可用的状态，通常需要包含如下内容。

- 分配**vCPU标识**，设置`cpu_id`属于**哪个KVM虚拟机**，并且分配对该vCPU的**唯一标识符**。
- 初始化**虚拟寄存器组**, 在`VT-x`下, 这些信息包含在**VMCS**中
- 初始化 `kvm_vcpu` 的**状态信息**，标识该**VCPU**当前所处的**状态**(睡眠、运行等)，主要供**调度器使用**。
- **寄存器/部件信息**，主要指**未包含在VMCS！！！** 中的**寄存器**或**CPU部件**，比如：**浮点寄存器！！！** 和 **虚拟的LAPIC！！！** 等。
- 用户**VMM**进行优化或存储额外信息的字段，如：存放该**VCPU私有数据的指针**。

接下来讲述KVM中**vCPU的创建过程**。

在获得了**fd\_vm**之后，通过**ioctl**调用**KVM\_CREATE\_VCPU指令**，可以对该fd\_vm对应的虚拟机**创建vCPU**，其入口函数地址在**kvm\_vm\_ioctl**函数(virt/kvm/kvm\_main.c)中，通过`switch`之后，程序流程将选择进入**kvm\_vm\_ioctl\_create\_vcpu**函数中进行处理，其代码如下。

## 6.2. 基本流程

```
kvm_vm_ioctl() // kvm ioctl vm指令入口
    kvm_vm_ioctl_create_vcpu() // 为虚拟机创建VCPU的ioctl调用的入口函数
        kvm_arch_vcpu_precreate() // stable tsc检查
        kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL); // 给kvm_vcpu分配内存, 该cache是kvm模块初始化时创建的
        page = alloc_page(); // 分配一页内存给vcpu->run(struct kvm_run)
        kvm_vcpu_init(vcpu, kvm, id); // vcpu结构体一些变量初始化
        kvm_arch_vcpu_create() // 初始化kvm_vcpu_arch结构体, 架构相关
            vcpu->arch.emulate_ctxt.ops = &emulate_ops; // 没有VMX时的软件模拟, 没人用...
            kvm_set_tsc_khz() // tsc的设置
            kvm_mmu_create() // kvm_vcpu_arch中mmu相关初始化, 内存虚拟化, 地址转换的重点
                alloc_mmu_pages() // 给vcpu->arch的gust mmu和root mmu分配页面
            kvm_create_lapic() // 初始化lapic
            alloc_page(); // 给kvm_vcpu_arch的pio分配内存页
            vcpu->arch.mce_banks = kzalloc(); // 以及其他一些代码实现初始化mce_banks
            vcpu->arch.user_fpu = kmem_cache_zalloc(); //给用户态fpu分配kmem cache
            vcpu->arch.guest_fpu = kmem_cache_zalloc(); // 虚拟机fpu的kmem cache
            kvm_pmu_init(vcpu); // 
            kvm_hv_vcpu_init(vcpu); // 
            kvm_x86_ops->vcpu_create(vcpu); //对于intel x86来说，最终调用 vmx_create_vcpu
                vmx->vpid = allocate_vpid(); // 分配vpid
                vmx->pml_pg = alloc_page(); // 
                alloc_loaded_vmcs(&vmx->vmc01); // loaded_vmcs的分配以及初始化
                    loaded_vmcs->vmcs = alloc_vmcs(); // 分配一个页面
                    loaded_vmcs_init(loaded_vmcs); // 初始化 loaded_vmcs
                        vmcs_clear(loaded_vmcs->vmcs); // 调用vmclear 
                    loaded_vmcs->msr_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL_ACCOUNT); // msr_bitmap 分配页面
                vmx_disable_intercept_for_msr();
                vmx_vcpu_load(); // 加载vcpu信息
                    vmx_vcpu_load_vmcs(); // 加载vmcs
                        already_loaded = vmx->loaded_vmcs->cpu == cpu; // 是否已经加载的判断
                            loaded_vmcs_clear(vmx->loaded_vmcs); // 没有加载时, 会调用vmclear命令(操作数为struct vmcs地址), 用于对该VMCS区域初始化, 包括将数据填充到VMCS区域和将VMCS状态(不可见字段)置为clear
                            list_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link, &per_cpu(loaded_vmcss_on_cpu, cpu)); // 没有加载时, 相应cpu上的loaded_vmcs链表
                        per_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs; // 赋值cpu的current_vmcs
                        vmcs_load(vmx->loaded_vmcs->vmcs); // vmptrld指令, 加载这个vmcs为current-VMCS
                            kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu); // 
                            vmcs_writel(HOST_TR_BASE, (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss); // 没加载时, 写VMCS的TSS
                            vmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   // 没加载时, 写GDT
                            vmx->loaded_vmcs->cpu = cpu; // 没加载时, 关联CPU
                        decache_tsc_multiplier(); 
                    vmx_vcpu_pi_load(); // 加载posted interrupt
                init_vmcs(vmx); // vmcs的guest-state在vmx_vcpu_reset()
                    vmcs_write64(); // 一堆写入vmcs, 使用vmwrite指令
                vmx_vcpu_put();
                    vmx_vcpu_pi_put();
                    vmx_prepare_switch_to_host(); // 
                put_cpu();
            kvm_vcpu_mtrr_init(vcpu); // 初始化mtrr链表 
            vcpu_load(vcpu);  // 加载vcpu信息<参数为kvm_vcpu>, struct vmx_vcpu(vcpu的一个运行环境)加载, 
                kvm_arch_vcpu_load(vcpu, cpu)
                    kvm_x86_ops->vcpu_load(vcpu, cpu); // 实际调用vmx.c的 vmx_vcpu_load(), 见上面
            kvm_vcpu_reset(vcpu, false); // 对vcpu结构进行初始化
                kvm_lapic_reset(vcpu, init_event);
                vcpu(kvm_vcpu)的很多变量的初始化
                kvm_clear_interrupt_queue(vcpu);
                kvm_clear_exception_queue(vcpu);
                kvm_make_request(KVM_REQ_EVENT, vcpu);
                kvmclock_reset(vcpu);
                kvm_x86_ops->vcpu_reset(vcpu, init_event);
            kvm_init_mmu(vcpu, false);
            vcpu_put(vcpu);
                kvm_arch_vcpu_put(vcpu);
                preempt_notifier_unregister(&vcpu->preempt_notifier);
                __this_cpu_write(kvm_running_vcpu, NULL);
        kvm_create_vcpu_debugfs() // 创建vcpu的debugfs
        kvm_get_kvm() // 增加kvm的引用计数
        create_vcpu_fd() // 为新创建的vcpu创建对应的fd，以便于后续通过该fd进行ioctl操作
        kvm_arch_vcpu_postcreate() // 架构相关的善后工作，比如再次调用vcpu_load，以及tsc相关处理
            vcpu_load();
            kvm_write_tsc();
```

## 6.3. 代码分析

kvm_vcpu结构体见上节

### 6.3.1. vCPU的创建过程kvm_vm_ioctl_create_vcpu

虚拟机创建VCPU的ioctl调用的入口函数，本质为创建vcpu结构并初始化，并将其填入kvm结构中。

代码5-10 kvm\_vm_ioctl\_create\_vcpu代码

```cpp
// virt/kvm/kvm_main.c
￼(1366)   static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)￼
(1367)   {￼
(1368)        int r;￼
(1369)        struct kvm_vcpu *vcpu, *v;￼
(1370)        // 创建vcpu结构，架构相关，对于intel x86来说，最终调用vmx_create_vcpu
(1371)        vcpu = kvm_arch_vcpu_create(kvm, id);￼
(1372)        if (IS_ERR(vcpu))￼
(1373)             return PTR_ERR(vcpu);￼
(1374)￼
(1375)        preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);￼
(1376)            /* 
     * 设置vcpu结构，主要调用kvm_x86_ops->vcpu_load，KVM虚拟机VCPU数据结构载入物理CPU，
     * 并进行虚拟机mmu相关设置，比如进行ept页表的相关初始工作或影子页表
     * 相关的设置。
     */￼
(1377)        r = kvm_arch_vcpu_setup(vcpu);￼
(1378)        if (r)￼
(1379)             return r;￼
(1380)￼
(1381)        mutex_lock(&kvm->lock);￼
(1382)        if (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {￼
(1383)             r = -EINVAL;￼
(1384)             goto vcpu_destroy;￼
(1385)        }￼
(1386)￼        // 检测分配的vcpu id是否已经存在
(1387)        kvm_for_each_vcpu(r, v, kvm)￼
(1388)             if (v->vcpu_id == id) {￼
(1389)                  r = -EEXIST;￼
(1390)                  goto vcpu_destroy;￼
(1391)             }￼
(1392)￼             /* kvm->vcpus[]数组包括该vm的所有vcpu，定义为KVM_MAX_VCPUS大小的数组。
     * 在kvm结构初始化时，其中所有成员都初始化为0，在vcpu还没有
     * 分配之前，如果不为0，那就是bug了。
     */
(1393)        BUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);￼
(1394)￼
(1395)        /* Now it's all set up, let userspace reach it */￼
		      // 增加kvm的引用计数
(1396)        kvm_get_kvm(kvm);￼
			 // 为新创建的vcpu创建对应的fd，以便于后续通过该fd进行ioctl操作
(1397)        r = create_vcpu_fd(vcpu);￼
(1398)        if (r < 0) {￼
(1399)             kvm_put_kvm(kvm);￼
(1400)             goto vcpu_destroy;￼
(1401)        }￼
(1402)￼        // 将新创建的vcpu填入kvm->vcpus[]数组中
(1403)        kvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;￼
(1404)        smp_wmb();￼ // 内存屏障，防止同时访问kvm结构时乱序
(1405)        atomic_inc(&kvm->online_vcpus);￼ // 增加online vcpu的数量
(1406)￼
(1407)   #ifdef CONFIG_KVM_APIC_ARCHITECTURE￼
(1408)        if (kvm->bsp_vcpu_id == id)￼
(1409)             kvm->bsp_vcpu = vcpu;￼
(1410)   #endif￼
(1411)        mutex_unlock(&kvm->lock);￼
(1412)        // 架构相关的善后工作，比如再次调用vcpu_load，以及tsc相关处理
(1413)        kvm_arch_vcpu_postcreate(vcpu);
(1414)        return r;￼
(1415)￼
(1416)   vcpu_destroy:￼
(1417)        mutex_unlock(&kvm->lock);￼
(1418)        kvm_arch_vcpu_destroy(vcpu);￼
(1419)        return r;￼
(1420)   }
```

#### 6.3.1.1. kvm_arch_vcpu_create创建kvm_vcpu

首先，在1371行，调用**kvm\_arch\_vcpu\_create**函数创建一个**kvm\_vcpu结构体！！！**。该创建内容**与架构相关**，因此，直接调用**kvm\_x86\_ops**中的`create_vcpu`方法执行。

(`kvm_arch_vcpu_create`在`arch/x86/kvm/x86.c`中, 调用`struct kvm\_x86\_ops vmx\_x86\_ops`(定义在arch/x86/kvm/vmx.c)中的`create\_vcpu`(即vmx\_create\_vcpu, 在arch/x86/kvm/vmx.c中))

**每个vCPU**的创建先alloc一个struct **vcpu\_vmx**里面包含了msr, vmcs, vcpu等, 这里不是主要对kvm\_vcpu做初始化

虽然代码不同，但是AMD平台和Intel平台实现的思路都是类似的：

- **先指定CPUID之后**，
- 接着**初始化MSR！！！** 和**VMCS！！！等寄存器**，(**每个VCPU都有**)
- 最后完成**I/O**和**内存部分寄存器的初始化**，为被**初次调度运行**做好准备。

`kvm_vm_ioctl()` --> `kvm_vm_ioctl_create_vcpu()` -->`kvm_arch_vcpu_create()`--> `kvm_x86_ops->vcpu_create()` --> `vmx_create_vcpu()`:

```cpp
/*
  * Intel x86架构中创建并初始化VCPU中架构相关部分
  */
static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
{
    int err;
    // 从slab中，分配vcpu_vmx结构体，其中包括VMX技术硬件相关信息。
    struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
    int cpu;

    if (!vmx)
        return ERR_PTR(-ENOMEM);
    // 分配vpid，vpid为VCPU的唯一标识。
    allocate_vpid(vmx);
    // 初始化vmx中的vcpu结构
    err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
    if (err)
        goto free_vcpu;
    // 分配Guest的msr寄存器保存区
    vmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);
    err = -ENOMEM;
    if (!vmx->guest_msrs) {
        goto uninit_vcpu;
    }

    vmx->loaded_vmcs = &vmx->vmcs01;
    /* 
     * 分配VMCS结构，该结构用于保存虚拟机和虚拟机监控器的系统编程接口状态。
     * 当执行VM exit和VM entry操作时，VT-x自动根据VMCS中的内容完成虚拟机和虚拟机监
     * 控器间的系统编程接口状态切换。
     */
    vmx->loaded_vmcs->vmcs = alloc_vmcs();
    if (!vmx->loaded_vmcs->vmcs)
        goto free_msrs;
    // 是否设置了vmm_exclusive
    if (!vmm_exclusive)
        // VMXON指令用于开启VMX模式
        kvm_cpu_vmxon(__pa(per_cpu(vmxarea, raw_smp_processor_id())));
    loaded_vmcs_init(vmx->loaded_vmcs);
    if (!vmm_exclusive)
        // VMXON指令用于关闭VMX模式
        kvm_cpu_vmxoff();
    // 当前cpu
    cpu = get_cpu();
    // KVM虚拟机VCPU数据结构载入物理CPU
    vmx_vcpu_load(&vmx->vcpu, cpu);
    vmx->vcpu.cpu = cpu;
    // 设置vmx相关信息
    err = vmx_vcpu_setup(vmx);
    vmx_vcpu_put(&vmx->vcpu);
    put_cpu();
    if (err)
        goto free_vmcs;
    if (vm_need_virtualize_apic_accesses(kvm)) {
        err = alloc_apic_access_page(kvm);
        if (err)
            goto free_vmcs;
    }
    // 是否支持EPT
    if (enable_ept) {
        if (!kvm->arch.ept_identity_map_addr)
            kvm->arch.ept_identity_map_addr =
                VMX_EPT_IDENTITY_PAGETABLE_ADDR;
        err = -ENOMEM;
        // 分配identity页表
        if (alloc_identity_pagetable(kvm) != 0)
            goto free_vmcs;
        // 初始化identity页表
        if (!init_rmode_identity_map(kvm))
            goto free_vmcs;
    }

    vmx->nested.current_vmptr = -1ull;
    vmx->nested.current_vmcs12 = NULL;

    return &vmx->vcpu;

free_vmcs:
    free_loaded_vmcs(vmx->loaded_vmcs);
free_msrs:
    kfree(vmx->guest_msrs);
uninit_vcpu:
    kvm_vcpu_uninit(&vmx->vcpu);
free_vcpu:
    free_vpid(vmx);
    kmem_cache_free(kvm_vcpu_cache, vmx);
    return ERR_PTR(err);
}
```

#### 6.3.1.2. kvm_arch_vcpu_setup初始化kvm_vcpu

其次，在1377行调用**kvm\_arch\_vcpu\_setup**函数对**kvm\_vcpu**中的数据结构进行**初始化**。

这里将先调用**kvm\_x86\_ops**中的**put\_vcpu函数**，实现将**vCPU的参数信息**加载入**CPU**中，并且执行**MMU初始化**和**CPU复位操作**。

kvm_vm_ioctl() --> kvm_vm_ioctl_create_vcpu() --> kvm_arch_vcpu_setup():

```cpp
int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
{
    int r;

    vcpu->arch.mtrr_state.have_fixed = 1;
    // KVM虚拟机VCPU数据结构载入物理CPU
    r = vcpu_load(vcpu);
    if (r)
        return r;
    // vcpu重置，包括相关寄存器、时钟、pmu等，最终调用vmx_vcpu_reset
    kvm_vcpu_reset(vcpu);
    /*
     * 进行虚拟机mmu相关设置，比如进行ept页表的相关初始工作或影子页表
     * 相关的设置。
     */
    r = kvm_mmu_setup(vcpu);
    vcpu_put(vcpu);

    return r;
}
```

kvm_vm_ioctl() --> kvm_vm_ioctl_create_vcpu() --> kvm_arch_vcpu_setup() --> kvm_mmu_setup() --> init_kvm_mmu():

```cpp
static int init_kvm_mmu(struct kvm_vcpu *vcpu)
{
    // NPT(Nested page table，AMD x86硬件提供的内存虚拟化技术，相当于Intel中的EPT技术)相关初始化
    if (mmu_is_nested(vcpu))
        return init_kvm_nested_mmu(vcpu);
    /* 
     * EPT(Extended page table，Intel x86硬件提供的内存虚拟化技术)相关初始化
     * 主要是设置一些函数指针，其中比较重要的如缺页异常处理函数
     */
    else if (tdp_enabled)
        return init_kvm_tdp_mmu(vcpu);
    // 影子页表(软件实现内存虚拟化技术)相关初始化
    else
        return init_kvm_softmmu(vcpu);
}
```

#### 6.3.1.3. vCPU检测

在 1381～1391 行中，进行了**两个检测**。

- 第一个是检测到如果**当前的 vCPU数量**已经达到了**系统设置的上限** **KVM\_MAX\_VCPU**，则将销毁刚才创建的实例。
- 第二个是如果**当前的vCPU**创建出来已经加入了**某一个已有的KVM主机**，则也将销毁该实例。

#### 6.3.1.4. 创建vcpu_fd

然后，在 1396～1405 行，会创建**当前 vCPU** 对应的文件描述符 **vcpu\_fd**，并且将**kvm\_vcpu**添加入**KVM**的**vCPU数组**中。

这里有一个特别之处，是使用了 atom\_read 和 atom\_inc 宏，这两个宏能够保证在进行 KVM 虚拟机的 vCPU添加时按照给定的顺序，不会因为执行中途的中断、进程切换等方式导致添加到不正确的kvm\_vcpu数组中。

#### 6.3.1.5. 释放内核锁和返回vcpu_fd

最后，释放掉所有的内核锁，完成本次vCPU的创建工作。

返回**vcpu\_fd**

# 7. vCPU的运行

## 7.1. 基本原理

在创建完**VM**和**vCPU**并且完成了**初始化工作**之后，就可以通过**调度程序调度执行**。

因为在Linux中，KVM虚拟机作为一个**系统线程**运行，因此，KVM虚拟机的调度程序实际上也就是Linux的调度程序，具体的调度将在后文qemu部分进行讨论，在当前，**KVM的调用**是从ioctl的**KVM\_RUN指令**字开始的。

```cpp
// virt/kvm/kvm_main.c
static long kvm_vcpu_ioctl(struct file *filp,
			   unsigned int ioctl, unsigned long arg)
{
    case KVM_RUN:
		r = -EINVAL;
		if (arg)
			goto out;
		if (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {
			/* The thread running this VCPU changed. */
			struct pid *oldpid = vcpu->pid;
			struct pid *newpid = get_task_pid(current, PIDTYPE_PID);

			rcu_assign_pointer(vcpu->pid, newpid);
			if (oldpid)
				synchronize_rcu();
			put_pid(oldpid);
		}
		r = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);
		trace_kvm_userspace_exit(vcpu->run->exit_reason, r);
		break;
}
```

**KVM\_RUN指令字**针对**fd\_vcpu描述符**操作，当**vCPU准备完成**之后，即可通过该指令让虚拟机运行起来。

**虚拟机运行**的主要任务则是进行**上下文切换**。上下文切换的内容较多，通常包括**通用寄存器**、**浮点寄存器**、**段寄存器**、**控制寄存器**、**MSR**等，在**KVM**中，还包括**APIC状态**、**TLB**等。

通常，进行**上下文切换的过程**可以归纳为如下步骤。

1）**KVM保存自己的上下文**。

2）KVM通过使用将**kvm\_vcpu结构体**中的**相关上下文**加载到**物理CPU**中。

3）KVM执行**kvm\_x86\_ops**中的**run\_vcpu函数**，调用具体的平台相关指令，进入**虚拟机运行环境**中。

由此可见，上下文切换次数过于频繁会带来不小的性能开销，因此，很有必要对这方面进行优化。和操作系统进行**进程切换的思路**一样，KVM使用**Lazy Save/Restore** 的方法进行优化。其基本思想是**尽量不要对寄存器进行恢复/保存操作**，直到必须要这么做的时候，才进行类似的操作。

## 7.2. 基本流程

**执行vCPU**的请求首先发送到**kvm\_vcpu\_ioctl函数**中，然后**加载vCPU参数**，调用**kvm\_arch\_vcpu\_ioctl\_run**函数进入**具体的vCPU运行**环节。(arch/x86/kvm/x86.c)

```
kvm_arch_vcpu_ioctl_run()                	// vcpu运行
 ├─vcpu_load(vcpu)                          // 加载vcpu信息, 见前面
 ├─kvm_sigset_activate(vcpu)                // 信号屏蔽, 防止干扰
 ├─kvm_load_guset_fpu()         			//  加载虚拟机的fpu
 |	├─kvm_save_current_fpu(vcpu->arch.user_fpu); // 将当前进程的fpu赋值给vcpu
 ├─vcpu_run()            					// 死循环进入vcpu_enter_guest
 |	├─for(;;)       // 循环受vcpu_enter_guest()返回值/系统信号控制, 只有异常/有外部信号才退出循环
 |	|   ├─vcpu_enter_guest()                	// 物理CPU进入guest状态
 |	|	|   ├─kvm_check_requests()  			// 处理 vcpu->requests, 每个有相应处理
 |	|	|   ├─kvm_mmu_reload(vcpu);  			// 加载mmu
 |	|	|   ├─kvm_x86_ops->prepare_guest_switch(vcpu); // 准备陷入到guest, 保存host当前状态
 |	|	|   ├─local_irq_disable();  			// 屏蔽中断响应，准备进入guest
 |	|	|   ├─vcpu->mode = IN_GUEST_MODE;  		// 将vcpu模式设为guest模式
 |	|	|   ├─guest_enter_irqoff();  			// 计算虚拟机系统时间和上下文切换
 |	|	|   ├─kvm_x86_ops->run(vcpu)  			// 开始运行guest, 会调用vmx_vcpu_run
 |	|	|   |   ├─__vmx_vcpu_run()  			// 调用汇编代码, vmlaunch
 |	|	|   |   ├─vmx_recover_nmi_blocking(vmx); // 恢复系统NMI中断
 |	|	|   |   ├─vmx_complete_interrupts(vmx); // 恢复中断
 |	|	|   ├─hw_breakpoint_restore()	// 恢复硬件断点
 |	|	|   ├─vcpu->mode = OUTSIDE_GUEST_MODE;	// vcpu mode
 |	|	|   ├─kvm_x86_ops->handle_exit(vcpu)	// vmexit的处理, 由vmx_handle_exit实现, 主要设置vcpu->run->exit_reason，让外部感知退出原因，并对应处理。
 ├─         								// 
```

## 7.3. 代码分析

1）通过调用**sigprocmask函数**，保证在**vCPU的初始化**过程中，不会因为来自**其他线程的信号干扰而中断**。

2）将**vCPU的状态**切换为**KVM\_MP\_STATE\_UNINITIALIZED**。

3）配置**APIC**和**mmio**的**中断信息**。

4）对要进入的虚拟机进行一些**关键指令的测试**，在测试中主要针对**内存读/写**情况进行测试。

5）将**vCPU**中保存的**上下文信息（寄存器状态等**）写入指定的位置。

6）接下来才开始实质性的工作，调用\_**vcpu\_run函数**进行后续处理。

\_vcpu\_run函数的代码如下。

代码5\-11 \_vcpu\_run函数

```cpp
// arch/x86/kvm/x86.c
(5232)   static int __vcpu_run(struct kvm_vcpu *vcpu)
(5233)   {
(5234)        int r;
(5235)        struct kvm *kvm = vcpu->kvm;
(5236)
(5237)        if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_SIPI_RECEIVED)) {
(5238)             pr_debug("vcpu %d received sipi with vector # %x\n",
(5239)                   vcpu->vcpu_id, vcpu->arch.sipi_vector);
(5240)             kvm_lapic_reset(vcpu);
(5241)             r = kvm_arch_vcpu_reset(vcpu);
(5242)             if (r)
(5243)                  return r;
(5244)             vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
(5245)        }
(5246)
(5247)        vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);
(5248)        vapic_enter(vcpu);
(5249)
(5250)        r = 1;
(5251)        while (r > 0) {
(5252)             if (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE)
(5253)                  r = vcpu_enter_guest(vcpu);
(5254)             else {
(5255)                  srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
(5256)                  kvm_vcpu_block(vcpu);
(5257)                  vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);
(5258)                  if (kvm_check_request(KVM_REQ_UNHALT, vcpu))
(5259)                  {
(5260)                       switch(vcpu->arch.mp_state) {
(5261)                       case KVM_MP_STATE_HALTED:
(5262)                            vcpu->arch.mp_state =
(5263)                                KVM_MP_STATE_RUNNABLE;
(5264)                       case KVM_MP_STATE_RUNNABLE:
(5265)                            break;
(5266)                       case KVM_MP_STATE_SIPI_RECEIVED:
(5267)                       default:
(5268)                            r = -EINTR;
(5269)                            break;
(5270)                       }
(5271)                  }
(5272)             }
(5273)
(5274)             if (r <= 0)
(5275)                  break;
(5276)
(5277)             clear_bit(KVM_REQ_PENDING_TIMER, &vcpu->requests);
(5278)             if (kvm_cpu_has_pending_timer(vcpu))
(5279)                  kvm_inject_pending_timer_irqs(vcpu);
(5280)
(5281)             if (dm_request_for_irq_injection(vcpu)) {
(5282)                  r = -EINTR;
(5283)                  vcpu->run->exit_reason = KVM_EXIT_INTR;
(5284)                  ++vcpu->stat.request_irq_exits;
(5285)             }
(5286)             if (signal_pending(current)) {
(5287)                  r = -EINTR;
(5288)                  vcpu->run->exit_reason = KVM_EXIT_INTR;
(5289)                  ++vcpu->stat.signal_exits;
(5290)             }
(5291)             if (need_resched()) {
(5292)                  srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
(5293)                  kvm_resched(vcpu);
(5294)                  vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);
(5295)             }
(5296)        }
(5297)
(5298)        srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
(5299)
(5300)        vapic_exit(vcpu);
(5301)
(5302)        return r;
(5303)   }
```

该函数较长，执行的内容也比较重要，重要的部分如下。

1）在5232～5240行中，将**虚拟APIC**和**vCPU**的状态**重置**。这个操作通过调用kvm\_lapic\_reset和kvm\_arch\_vcpu\_reset函数实现。

2）在5245～5270行中，正常情况下，将kvm\_vcp\-\>arch.mp\_state的取值作为**单机未运行的取值**：`KVM_MP_STATE_RUNNABLE`；如果 vCPU 在 VM 中处于**其他状态**，则会**出错**。在运行中有一个**循环**，直到确认了**vcpu\_enter\_guest**函数执行完毕，即**物理CPU进入了GUEST状态**并且执行完成后，才会执行下一步操作。

3）在5273～5278行中，将检查本次执行的一些结果。如果CPU当前有挂起的定时器或者其他中断，则会保存该中断的现场。

4）在5286～5289行中，如果对当前执行的vCPU需要调度，会引用Linux的进程调度子程序进行一次任务调度。

在上面的执行流程中，最重要的执行在 **vcpu\_enter\_guest** 函数中，该函数实现了进入客户机并且执行具体指令的操作。
