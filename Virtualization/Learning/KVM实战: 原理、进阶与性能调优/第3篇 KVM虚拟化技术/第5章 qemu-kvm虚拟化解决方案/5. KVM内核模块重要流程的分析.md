
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. KVM模块初始化流程](#1-kvm模块初始化流程)
- [2. QEMU与KVM交互](#2-qemu与kvm交互)
- [3. /dev/kvm的ioctl接口kvm_dev_ioctl](#3-devkvm的ioctl接口kvm_dev_ioctl)
- [4. 虚拟机的创建](#4-虚拟机的创建)
  - [4.1. 基本原理](#41-基本原理)
  - [4.2. 基本流程](#42-基本流程)
  - [4.3. 代码分析](#43-代码分析)
    - [4.3.1. 虚拟机创建过程kvm_dev_ioctl_create_vm](#431-虚拟机创建过程kvm_dev_ioctl_create_vm)
      - [4.3.1.1. kvm_create_vm流程](#4311-kvm_create_vm流程)
      - [4.3.1.2. 返回kvm_vm的fd文件描述符](#4312-返回kvm_vm的fd文件描述符)
- [5. 虚拟机fd的ioctl接口kvm_vm_ioctl](#5-虚拟机fd的ioctl接口kvm_vm_ioctl)
- [6. vCPU的创建](#6-vcpu的创建)
  - [6.1. 基本原理](#61-基本原理)
  - [6.2. 基本流程](#62-基本流程)
  - [6.3. 代码分析](#63-代码分析)
    - [6.3.1. vCPU的创建过程kvm_vm_ioctl_create_vcpu](#631-vcpu的创建过程kvm_vm_ioctl_create_vcpu)
      - [6.3.1.1. kvm_arch_vcpu_create创建kvm_vcpu](#6311-kvm_arch_vcpu_create创建kvm_vcpu)
      - [6.3.1.2. kvm_arch_vcpu_setup初始化kvm_vcpu](#6312-kvm_arch_vcpu_setup初始化kvm_vcpu)
      - [6.3.1.3. vCPU检测](#6313-vcpu检测)
      - [6.3.1.4. 创建vcpu_fd](#6314-创建vcpu_fd)
      - [6.3.1.5. 释放内核锁和返回vcpu_fd](#6315-释放内核锁和返回vcpu_fd)
- [7. KVM_SET_USER_MEMORY_REGION流程](#7-kvm_set_user_memory_region流程)
  - [7.1. 基本原理](#71-基本原理)
  - [7.2. 基本流程](#72-基本流程)
  - [7.3. 代码分析](#73-代码分析)
- [8. vCPU的运行](#8-vcpu的运行)

<!-- /code_chunk_output -->

# 1. KVM模块初始化流程

详细见: `Virtualization/KVM/2. 模块初始化以及创建虚拟机QEMU/1. KVM模块初始化.md`

KVM模块分为三个主要模块：kvm.ko、kvm\-intel.ko和kvm\-amd.ko，这三个模块在初始化阶段的流程如图5\-4所示。
￼
图5\-4 KVM模块初始化阶段:

![2019-07-05-21-29-03.png](./images/2019-07-05-21-29-03.png)

**KVM模块**可以**编译进内核**中，也可以作为**内核模块**在Linux系统启动完成**之后加载**。加载时，KVM 根据主机所用的体系架构是 Intel的 VMX技术还是AMD的SVM技术，会采用略微不同的加载流程。

Linux的**子模块入口**通常通过**module\_init**宏进行定义，由**内核进行调用**。KVM的初始化流程如图5\-5所示。
￼
图5\-5 KVM的初始化流程:

![2019-07-05-22-18-28.png](./images/2019-07-05-22-18-28.png)

KVM的初始化步骤分为以下三步。

1）在**平台相关的KVM模块**中通过**module\_init宏**正式进入KVM的初始化阶段，并且执行相关的**硬件初始化准备**。(arch/x86/kvm/vmx.c)

2）进入**kvm\_main.c**中的**kvm\_init**函数进行正式的初始化工作，期间进行了一系列子操作。(virt/kvm/kvm\_main.c)

注: vmx.c中定义了一个结构体`vmx_x86_ops`, 这是函数指针集合(各种操作的集合). 初始化很多动作以及iotcl调用也都会用到这些函数. 作为参数传给`kvm_init()`, 再传给`kvm_arch_init()`, 在传递到全局变量`kvm_x86_ops`. 可以见4的说明

* 通过**kvm\_arch\_init**函数初始化KVM内部的一些数据结构：**注册全局变量kvm\_x86\_ops**、**初始化MMU**等数据结构、**初始化Timer定时器**架构。(arch/x86/kvm/x86.c)
* 分配KVM内部操作所需要的**内存空间**。
* 调用kvm\_x86\_ops的**hardware\_setup**函数进行具体的硬件体系结构的初始化工作。
* 注册**sysfs**和**devfs**等API接口信息。
* 最后初始化**debugfs**的调试信息。

3）进行后续的硬件初始化准备操作。

# 2. QEMU与KVM交互

KVM内核模块加载之后只提供了一个对外接口/dev/kvm，qemu通过/dev/kvm文件依次创建vm句柄和vcpu句柄来和kvm交互。

所以存在三种句柄: KVM访问句柄、虚拟机访问句柄和vCPU访问句柄

![2019-12-11-15-33-58.png](./images/2019-12-11-15-33-58.png)

整体流程是:

```
qemu_open("/dev/kvm", O_RDWR); //KVM访问句柄
kvm_ioctl(s, KVM_GET_API_VERSION, 0);
kvm_ioctl(s,KVM_CREATE_VM, 0); //创建vm访问句柄
kvm_arch_init ==>
kvm_vm_ioctl(s,KVM_SET_IDENTITY_MAP_ADDR, &identity_base）

           kvm_vm_ioctl(s,KVM_SET_TSS_ADDR, identity_base + 0x1000)

           kvm_vm_ioctl(s,KVM_SET_NR_MMU_PAGES, shadow_mem)
==>kvm_irqchip_create(s) ==> kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP)

kvm_init_vcpu(env);
==> kvm_vm_ioctl(s, KVM_CREATE_VCPU, env->cpu_index);创建vcpu句柄

	kvm_cpu_exec
	==>kvm_vcpu_ioctl(env, KVM_RUN, 0);
	
	//根据Exit的原因处理VM-Exit

       switch (run->exit_reason) {

       case KVM_EXIT_IO: //VM Exit Cause by IO operation

           kvm_handle_io();

```

# 3. /dev/kvm的ioctl接口kvm_dev_ioctl

在上面初始化中调用了`misc_register(&kvm_dev);`注册了`/dev/kvm`

`/dev/kvm`的ioctl的接口`kvm_dev_ioctl`主要提供了创建vm和校验版本号的功能

```cpp
static long kvm_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
	long r = -EINVAL;

	switch (ioctl) {
	case KVM_GET_API_VERSION:             //获取api版本
		if (arg)
			goto out;
		r = KVM_API_VERSION;
		break;
	case KVM_CREATE_VM:                    //创建VM，返回vmfd
		r = kvm_dev_ioctl_create_vm(arg);
		break;
	case KVM_CHECK_EXTENSION:
		r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
		break;
	case KVM_GET_VCPU_MMAP_SIZE:
		if (arg)
			goto out;
		r = PAGE_SIZE;     /* struct kvm_run */
#ifdef CONFIG_X86
		r += PAGE_SIZE;    /* pio data page */
#endif
#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
		r += PAGE_SIZE;    /* coalesced mmio ring page */
#endif
		break;

	default:
		return kvm_arch_dev_ioctl(filp, ioctl, arg); //默认创建设备
	}
out:
	return r;
}
```

# 4. 虚拟机的创建

## 4.1. 基本原理

基于KVM的虚拟机创建分为**虚拟机创建**和**虚拟CPU创建**两个步骤。

在下文的描述中，**虚拟机**对应的**文件描述符**为**vm\_fd**，**虚拟CPU**对应的**文件描述符**为**vcpu\_fd**。

**打开/dev/kvm** 文件并且获得**文件描述符 fd** 后，通过 `kvm_dev_ioctl` 指令写入`KVM_CREATE_VM`，即可创建一个 VM 虚拟机。

对`虚拟机(VM)`来说，**kvm结构体**是关键，**一个虚拟机**对应**一个kvm结构体**，虚拟机的创建过程实质为**kvm结构体的创建**和**初始化过程**。

## 4.2. 基本流程

整体流程如下:

```
用户态ioctl(fd,KVM_CREATE_VM,..)
    内核态kvm_dev_ioctl()
        kvm_dev_ioctl_create_vm()
            kvm_create_vm() //实现虚拟机创建的主要函数
                kvm_arch_alloc_vm() // 分配kvm结构体
                kvm_arch_init_vm() // 初始化kvm结构中的架构相关部分，比如中断
                hardware_enable_all() // 使能硬件，架构相关操作
                    hardware_enable_nolock
                        kvm_arch_hardware_enable()
                            kvm_shared_msr_cpu_online()     // kvm shared msr
                            kvm_x86_ops->hardware_enable()  // 会调用vmxon指令
                                crash_enable_local_vmclear()  // 清理位图
                                kvm_cpu_vmxon()  // vmxon打开VMX模式
                            tsc初始化规则
                kzalloc() // 分配memslots结构，并初始化为0
                kvm_init_memslots_id() // 初始化内存槽位(slot)的id信息
                kvm_eventfd_init() // 初始化事件通道
                kvm_init_mmu_notifier() // 初始化mmu操作的通知链
                list_add(&kvm->vm_list, &vm_list) // 将新创建的虚拟机的kvm结构，加入到全局链表vm_list中
```

## 4.3. 代码分析

kvm结构体见上节

### 4.3.1. 虚拟机创建过程kvm_dev_ioctl_create_vm

KVM 的**该部分代码**实现在**kvm\_dev** 的 **file\_operation 结构体**中，对应的代码在 **kvm\_main.c** 中调用 **kvm\_dev\_ioctl\_create\_vm**函数实现，其代码如下：

代码5\-7 KVM\_CREATE\_VM实现代码

```cpp
// virt/kvm/kvm_main.c
static long kvm_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
(1880)        case KVM_CREATE_VM:￼
(1881)             r = -EINVAL;￼
(1882)             if (arg)￼
(1883)                  goto out;￼
(1884)             r = kvm_dev_ioctl_create_vm();￼
(1885)             break;
```

**kvm\_dev\_ioctl\_create\_vm**函数(virt/kvm/kvm\_main.c)通过调用**kvm\_create\_vm**函数对**KVM结构体**进行创建。

```cpp
// virt/kvm/kvm_main.c
static int kvm_dev_ioctl_create_vm(unsigned long type)
{
	int r;
	struct kvm *kvm;

	kvm = kvm_create_vm(type);			// 创建VM
	if (IS_ERR(kvm))
		return PTR_ERR(kvm);
#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
	r = kvm_coalesced_mmio_init(kvm);
	if (r < 0) {
		kvm_put_kvm(kvm);
		return r;
	}
#endif
	r = anon_inode_getfd("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);	//挂载vmfd的ioctl接口
	if (r < 0)
		kvm_put_kvm(kvm);

	return r;
}
```

**KVM 结构体**如前文所述，保存了**虚拟机运行的上下文**及**其他相关状态**，在**使用之前**，需要进行**一定的初始化**工作。

#### 4.3.1.1. kvm_create_vm流程

虚拟机创建的主要函数

```cpp
// virt/kvm/kvm_main.c
static struct kvm *kvm_create_vm(unsigned long type)
{
	int r, i;
	/* 
     * 分配kvm结构体，一个虚拟机对应一个kvm结构，其中包括了虚拟机中的
     * 关键系统，比如内存、中断、VCPU、总线等信息，该结构体也是kvm的关键结
     * 构体之一
     */
	struct kvm *kvm = kvm_arch_alloc_vm();
	//  // 初始化kvm结构中的架构相关部分，比如中断
	r = kvm_arch_init_vm(kvm, type);
	// 硬件使能，最终调用架构相关的kvm_x86_ops->hardware_enable()接口
	r = hardware_enable_all();
	// 分配memslots结构，并初始化为0
	kvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);
    // 初始化内存槽位(slot)的id信息，便于后续索引
	kvm_init_memslots_id(kvm);
	if (init_srcu_struct(&kvm->srcu))
		goto out_err_nosrcu;
	// 初始化虚拟机的bus信息
	for (i = 0; i < KVM_NR_BUSES; i++) {
		kvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),
					GFP_KERNEL);
		if (!kvm->buses[i])
			goto out_err;
	}
	// 初始化mmu_lock
	spin_lock_init(&kvm->mmu_lock);
	// 设置虚拟机的mm(mm_struct)为当前进程的mm
	kvm->mm = current->mm;
	atomic_inc(&kvm->mm->mm_count);
	// 初始化事件通道
	kvm_eventfd_init(kvm);
	// 初始化mmu操作的通知链
	r = kvm_init_mmu_notifier(kvm);

	raw_spin_lock(&kvm_lock);
	// 将新创建的虚拟机的kvm结构，加入到全局链表vm_list中
	list_add(&kvm->vm_list, &vm_list);
	raw_spin_unlock(&kvm_lock);

	return kvm;
}
```

先**分配一个虚拟机struct kvm数据结构**

在 x86体系架构中，**KVM结构体**的**初始化**任务在**kvm\_arch\_init\_vm**函数(arch/x86/kvm/x86.c)中进行，进行了**分配内存**、**初始化设备列表**、设置**中断管理**和**初始化 tsc 的spin\_lock的功能**。

在**完成之后**，将执行**硬件初始化**工作，该部分硬件初始化工作通过调用**on\_each\_cpu宏**，将在**每个物理CPU**上执行**同样的操作**。该操作主要是尝试将**所有的CPU**切换入**vitualize模式**(使用`vmxon`指令)，并且设置好**时钟**等信息，这个过程通过 **kvm\_arch\_hardware\_enable** 函数完成。

该函数代码(arch/x86/kvm/x86.c)如下，主要执行了两个工作：

- 处理kvm shared msr信息
- **整理CPU的时钟信息**；
- 调用kvm\_x86\_ops的**硬件相关**的函数进行具体操作, 这其中调用了`vmxon`使每个cpu进入Virtualization模式.

代码5\-8 kvm\_arch\_hardware\_enable函数代码

```cpp
￼(5799)   int kvm_arch_hardware_enable(void *garbage)￼
(5800)   {￼
(5801)        struct kvm *kvm;￼
(5802)        struct kvm_vcpu *vcpu;￼
(5803)        int i;￼
(5804)￼
(5805)        kvm_shared_msr_cpu_online();￼
(5806)        list_for_each_entry(kvm, &vm_list, vm_list)￼
(5807)             kvm_for_each_vcpu(i, vcpu, kvm)￼
(5808)                  if (vcpu->cpu == smp_processor_id())￼
(5809)                       kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);￼
(5810)        return kvm_x86_ops->hardware_enable(garbage);￼
(5811)   }
```

接下来，将初始化 KVM 的 **memslot 结构体**、**Bus 总线结构体信息**、**scru读/写锁信息**、**eventfd事件通知信息**、**mmu内存管理结构体**信息。

#### 4.3.1.2. 返回kvm_vm的fd文件描述符

然后，调用 **anon\_inode\_getfd** 函数。

该函数设置了对**所有 KVM 的操作**都将给予**kvm\_vm这个共享文件**进行，该共享文件的操作封装在**kvm\_vm\_fops**结构体中，对**VM的操作**实际上就是对此文件的操作。因此，对其**ioctl调用**的是**kvm\_vm\_fops中的成员函数**。

代码5\-9 调用anon\_inode\_getfd创建kvm\-vm

```cpp
// 
￼(1840)        fd = anon_inode_getfd("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);￼
(1841)        if (fd < 0)￼
(1842)             kvm_put_kvm(kvm);￼
(1843)￼
(1844)        return fd;
```

通过anon\_inode\_getfd获得的**fd文件描述符**，就是供**用户态**使用的**vm\_fd**，用户态将通过该fd进行进一步的虚拟机操作，首先要做的事情是**初始化vCPU**。

# 5. 虚拟机fd的ioctl接口kvm_vm_ioctl

上面调用后会得到虚拟机的fd, 对应的ioctl是`kvm_vm_ioctl()`

```cpp
// virt/kvm/kvm_main.c
static long kvm_vm_ioctl(struct file *filp,
			   unsigned int ioctl, unsigned long arg)
```

# 6. vCPU的创建

## 6.1. 基本原理

创建vCPU实际上就是**创建vCPU的描述符**，在KVM中，**vCPU对应的数据结构体**为**kvm\_vcpu**。因此，创建vCPU的描述符，简单来说就是**分配相应大小的内存**，并且进行相应的**初始化**工作。

**kvm\_vcpu**中描述符包含的内容有很多，通常会包含各个**平台通用的内容**和**平台相关的内容**。

在**物理CPU上电**之后，需要进一步初始化才可以使用。在这个过程中，硬件会自动将CPU初始化成特定的状态。**kvm\_vcpu的初始化**也是一个**类似的过程**，将 **kvm\_vcpu 的各个数据结构体**设置成为可用的状态，通常需要包含如下内容。

- 分配**vCPU标识**，设置`cpu_id`属于**哪个KVM虚拟机**，并且分配对该vCPU的**唯一标识符**。
- 初始化**虚拟寄存器组**, 在`VT-x`下, 这些信息包含在**VMCS**中
- 初始化 `kvm_vcpu` 的**状态信息**，标识该**VCPU**当前所处的**状态**(睡眠、运行等)，主要供**调度器使用**。
- **寄存器/部件信息**，主要指**未包含在VMCS！！！** 中的**寄存器**或**CPU部件**，比如：**浮点寄存器！！！** 和 **虚拟的LAPIC！！！** 等。
- 用户**VMM**进行优化或存储额外信息的字段，如：存放该**VCPU私有数据的指针**。

接下来讲述KVM中**vCPU的创建过程**。

在获得了**fd\_vm**之后，通过**ioctl**调用**KVM\_CREATE\_VCPU指令**，可以对该fd\_vm对应的虚拟机**创建vCPU**，其入口函数地址在**kvm\_vm\_ioctl**函数(virt/kvm/kvm\_main.c)中，通过`switch`之后，程序流程将选择进入**kvm\_vm\_ioctl\_create\_vcpu**函数中进行处理，其代码如下。

## 6.2. 基本流程

```
kvm_vm_ioctl() // kvm ioctl vm指令入口
    kvm_vm_ioctl_create_vcpu() // 为虚拟机创建VCPU的ioctl调用的入口函数
        kvm_arch_vcpu_precreate() // stable tsc检查
        kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL); // 给kvm_vcpu分配内存, 该cache是kvm模块初始化时创建的
        page = alloc_page(); // 分配一页内存给vcpu->run(kvm_run)
        kvm_vcpu_init(vcpu, kvm, id); // vcpu结构体一些变量初始化
        kvm_arch_vcpu_create() // 初始化kvm_vcpu_arch结构体, 架构相关
            vcpu->arch.emulate_ctxt.ops = &emulate_ops; // 没有VMX时的软件模拟, 没人用...
            kvm_set_tsc_khz() // tsc的设置
            kvm_mmu_create() // kvm_vcpu_arch中mmu相关初始化, 内存虚拟化, 地址转换的重点
            kvm_create_lapic() // 初始化lapic
            alloc_page(); // 给kvm_vcpu_arch的pio分配内存页
            vcpu->arch.mce_banks = kzalloc(); // 以及其他一些代码实现初始化mce_banks
            vcpu->arch.user_fpu = kmem_cache_zalloc(); //给用户态fpu分配kmem cache
            vcpu->arch.guest_fpu = kmem_cache_zalloc(); // 
            kvm_x86_ops->vcpu_create(vcpu); //对于intel x86来说，最终调用vmx_create_vcpu
            kvm_vcpu_mtrr_init(vcpu); // 初始化mtrr链表 
            vcpu_load(vcpu);  // 实际调用vmx.c的vmx_vcpu_load()
                vmx_vcpu_load_vmcs();
                vmx_vcpu_pi_load();
                
            kvm_vcpu_reset(vcpu, false);
            kvm_init_mmu(vcpu, false);
            vcpu_put(vcpu);
        kvm_create_vcpu_debugfs() //
        kvm_get_kvm() // 增加kvm的引用计数
        kvm_arch_vcpu_setup() // 设置VCPU结构
        create_vcpu_fd() // 为新创建的vcpu创建对应的fd，以便于后续通过该fd进行ioctl操作
        kvm_arch_vcpu_postcreate() // 架构相关的善后工作，比如再次调用vcpu_load，以及tsc相关处理
```

## 6.3. 代码分析

kvm_vcpu结构体见上节

### 6.3.1. vCPU的创建过程kvm_vm_ioctl_create_vcpu

虚拟机创建VCPU的ioctl调用的入口函数，本质为创建vcpu结构并初始化，并将其填入kvm结构中。

代码5-10 kvm\_vm_ioctl\_create\_vcpu代码

```cpp
// virt/kvm/kvm_main.c
￼(1366)   static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)￼
(1367)   {￼
(1368)        int r;￼
(1369)        struct kvm_vcpu *vcpu, *v;￼
(1370)        // 创建vcpu结构，架构相关，对于intel x86来说，最终调用vmx_create_vcpu
(1371)        vcpu = kvm_arch_vcpu_create(kvm, id);￼
(1372)        if (IS_ERR(vcpu))￼
(1373)             return PTR_ERR(vcpu);￼
(1374)￼
(1375)        preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);￼
(1376)            /* 
     * 设置vcpu结构，主要调用kvm_x86_ops->vcpu_load，KVM虚拟机VCPU数据结构载入物理CPU，
     * 并进行虚拟机mmu相关设置，比如进行ept页表的相关初始工作或影子页表
     * 相关的设置。
     */￼
(1377)        r = kvm_arch_vcpu_setup(vcpu);￼
(1378)        if (r)￼
(1379)             return r;￼
(1380)￼
(1381)        mutex_lock(&kvm->lock);￼
(1382)        if (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {￼
(1383)             r = -EINVAL;￼
(1384)             goto vcpu_destroy;￼
(1385)        }￼
(1386)￼        // 检测分配的vcpu id是否已经存在
(1387)        kvm_for_each_vcpu(r, v, kvm)￼
(1388)             if (v->vcpu_id == id) {￼
(1389)                  r = -EEXIST;￼
(1390)                  goto vcpu_destroy;￼
(1391)             }￼
(1392)￼             /* kvm->vcpus[]数组包括该vm的所有vcpu，定义为KVM_MAX_VCPUS大小的数组。
     * 在kvm结构初始化时，其中所有成员都初始化为0，在vcpu还没有
     * 分配之前，如果不为0，那就是bug了。
     */
(1393)        BUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);￼
(1394)￼
(1395)        /* Now it's all set up, let userspace reach it */￼
		      // 增加kvm的引用计数
(1396)        kvm_get_kvm(kvm);￼
			 // 为新创建的vcpu创建对应的fd，以便于后续通过该fd进行ioctl操作
(1397)        r = create_vcpu_fd(vcpu);￼
(1398)        if (r < 0) {￼
(1399)             kvm_put_kvm(kvm);￼
(1400)             goto vcpu_destroy;￼
(1401)        }￼
(1402)￼        // 将新创建的vcpu填入kvm->vcpus[]数组中
(1403)        kvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;￼
(1404)        smp_wmb();￼ // 内存屏障，防止同时访问kvm结构时乱序
(1405)        atomic_inc(&kvm->online_vcpus);￼ // 增加online vcpu的数量
(1406)￼
(1407)   #ifdef CONFIG_KVM_APIC_ARCHITECTURE￼
(1408)        if (kvm->bsp_vcpu_id == id)￼
(1409)             kvm->bsp_vcpu = vcpu;￼
(1410)   #endif￼
(1411)        mutex_unlock(&kvm->lock);￼
(1412)        // 架构相关的善后工作，比如再次调用vcpu_load，以及tsc相关处理
(1413)        kvm_arch_vcpu_postcreate(vcpu);
(1414)        return r;￼
(1415)￼
(1416)   vcpu_destroy:￼
(1417)        mutex_unlock(&kvm->lock);￼
(1418)        kvm_arch_vcpu_destroy(vcpu);￼
(1419)        return r;￼
(1420)   }
```

#### 6.3.1.1. kvm_arch_vcpu_create创建kvm_vcpu

首先，在1371行，调用**kvm\_arch\_vcpu\_create**函数创建一个**kvm\_vcpu结构体！！！**。该创建内容**与架构相关**，因此，直接调用**kvm\_x86\_ops**中的`create_vcpu`方法执行。

(`kvm_arch_vcpu_create`在`arch/x86/kvm/x86.c`中, 调用`struct kvm\_x86\_ops vmx\_x86\_ops`(定义在arch/x86/kvm/vmx.c)中的`create\_vcpu`(即vmx\_create\_vcpu, 在arch/x86/kvm/vmx.c中))

**每个vCPU**的创建先alloc一个struct **vcpu\_vmx**里面包含了msr, vmcs, vcpu等, 这里不是主要对kvm\_vcpu做初始化

虽然代码不同，但是AMD平台和Intel平台实现的思路都是类似的：

- **先指定CPUID之后**，
- 接着**初始化MSR！！！** 和**VMCS！！！等寄存器**，(**每个VCPU都有**)
- 最后完成**I/O**和**内存部分寄存器的初始化**，为被**初次调度运行**做好准备。

`kvm_vm_ioctl()` --> `kvm_vm_ioctl_create_vcpu()` -->`kvm_arch_vcpu_create()`--> `kvm_x86_ops->vcpu_create()` --> `vmx_create_vcpu()`:

```cpp
/*
  * Intel x86架构中创建并初始化VCPU中架构相关部分
  */
static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
{
    int err;
    // 从slab中，分配vcpu_vmx结构体，其中包括VMX技术硬件相关信息。
    struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
    int cpu;

    if (!vmx)
        return ERR_PTR(-ENOMEM);
    // 分配vpid，vpid为VCPU的唯一标识。
    allocate_vpid(vmx);
    // 初始化vmx中的vcpu结构
    err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
    if (err)
        goto free_vcpu;
    // 分配Guest的msr寄存器保存区
    vmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);
    err = -ENOMEM;
    if (!vmx->guest_msrs) {
        goto uninit_vcpu;
    }

    vmx->loaded_vmcs = &vmx->vmcs01;
    /* 
     * 分配VMCS结构，该结构用于保存虚拟机和虚拟机监控器的系统编程接口状态。
     * 当执行VM exit和VM entry操作时，VT-x自动根据VMCS中的内容完成虚拟机和虚拟机监
     * 控器间的系统编程接口状态切换。
     */
    vmx->loaded_vmcs->vmcs = alloc_vmcs();
    if (!vmx->loaded_vmcs->vmcs)
        goto free_msrs;
    // 是否设置了vmm_exclusive
    if (!vmm_exclusive)
        // VMXON指令用于开启VMX模式
        kvm_cpu_vmxon(__pa(per_cpu(vmxarea, raw_smp_processor_id())));
    loaded_vmcs_init(vmx->loaded_vmcs);
    if (!vmm_exclusive)
        // VMXON指令用于关闭VMX模式
        kvm_cpu_vmxoff();
    // 当前cpu
    cpu = get_cpu();
    // KVM虚拟机VCPU数据结构载入物理CPU
    vmx_vcpu_load(&vmx->vcpu, cpu);
    vmx->vcpu.cpu = cpu;
    // 设置vmx相关信息
    err = vmx_vcpu_setup(vmx);
    vmx_vcpu_put(&vmx->vcpu);
    put_cpu();
    if (err)
        goto free_vmcs;
    if (vm_need_virtualize_apic_accesses(kvm)) {
        err = alloc_apic_access_page(kvm);
        if (err)
            goto free_vmcs;
    }
    // 是否支持EPT
    if (enable_ept) {
        if (!kvm->arch.ept_identity_map_addr)
            kvm->arch.ept_identity_map_addr =
                VMX_EPT_IDENTITY_PAGETABLE_ADDR;
        err = -ENOMEM;
        // 分配identity页表
        if (alloc_identity_pagetable(kvm) != 0)
            goto free_vmcs;
        // 初始化identity页表
        if (!init_rmode_identity_map(kvm))
            goto free_vmcs;
    }

    vmx->nested.current_vmptr = -1ull;
    vmx->nested.current_vmcs12 = NULL;

    return &vmx->vcpu;

free_vmcs:
    free_loaded_vmcs(vmx->loaded_vmcs);
free_msrs:
    kfree(vmx->guest_msrs);
uninit_vcpu:
    kvm_vcpu_uninit(&vmx->vcpu);
free_vcpu:
    free_vpid(vmx);
    kmem_cache_free(kvm_vcpu_cache, vmx);
    return ERR_PTR(err);
}
```

#### 6.3.1.2. kvm_arch_vcpu_setup初始化kvm_vcpu

其次，在1377行调用**kvm\_arch\_vcpu\_setup**函数对**kvm\_vcpu**中的数据结构进行**初始化**。

这里将先调用**kvm\_x86\_ops**中的**put\_vcpu函数**，实现将**vCPU的参数信息**加载入**CPU**中，并且执行**MMU初始化**和**CPU复位操作**。

kvm_vm_ioctl() --> kvm_vm_ioctl_create_vcpu() --> kvm_arch_vcpu_setup():

```cpp
int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
{
    int r;

    vcpu->arch.mtrr_state.have_fixed = 1;
    // KVM虚拟机VCPU数据结构载入物理CPU
    r = vcpu_load(vcpu);
    if (r)
        return r;
    // vcpu重置，包括相关寄存器、时钟、pmu等，最终调用vmx_vcpu_reset
    kvm_vcpu_reset(vcpu);
    /*
     * 进行虚拟机mmu相关设置，比如进行ept页表的相关初始工作或影子页表
     * 相关的设置。
     */
    r = kvm_mmu_setup(vcpu);
    vcpu_put(vcpu);

    return r;
}
```

kvm_vm_ioctl() --> kvm_vm_ioctl_create_vcpu() --> kvm_arch_vcpu_setup() --> kvm_mmu_setup() --> init_kvm_mmu():

```cpp
static int init_kvm_mmu(struct kvm_vcpu *vcpu)
{
    // NPT(Nested page table，AMD x86硬件提供的内存虚拟化技术，相当于Intel中的EPT技术)相关初始化
    if (mmu_is_nested(vcpu))
        return init_kvm_nested_mmu(vcpu);
    /* 
     * EPT(Extended page table，Intel x86硬件提供的内存虚拟化技术)相关初始化
     * 主要是设置一些函数指针，其中比较重要的如缺页异常处理函数
     */
    else if (tdp_enabled)
        return init_kvm_tdp_mmu(vcpu);
    // 影子页表(软件实现内存虚拟化技术)相关初始化
    else
        return init_kvm_softmmu(vcpu);
}
```

#### 6.3.1.3. vCPU检测

在 1381～1391 行中，进行了**两个检测**。

- 第一个是检测到如果**当前的 vCPU数量**已经达到了**系统设置的上限** **KVM\_MAX\_VCPU**，则将销毁刚才创建的实例。
- 第二个是如果**当前的vCPU**创建出来已经加入了**某一个已有的KVM主机**，则也将销毁该实例。

#### 6.3.1.4. 创建vcpu_fd

然后，在 1396～1405 行，会创建**当前 vCPU** 对应的文件描述符 **vcpu\_fd**，并且将**kvm\_vcpu**添加入**KVM**的**vCPU数组**中。

这里有一个特别之处，是使用了 atom\_read 和 atom\_inc 宏，这两个宏能够保证在进行 KVM 虚拟机的 vCPU添加时按照给定的顺序，不会因为执行中途的中断、进程切换等方式导致添加到不正确的kvm\_vcpu数组中。

#### 6.3.1.5. 释放内核锁和返回vcpu_fd

最后，释放掉所有的内核锁，完成本次vCPU的创建工作。

返回**vcpu\_fd**

# 7. KVM_SET_USER_MEMORY_REGION流程

## 7.1. 基本原理

如之前分析，kvm虚拟机实际运行于qemu-kvm的进程上下文中，因此，需要建立虚拟机的物理内存空间(GPA)与qemu-kvm进程的虚拟地址空间(HVA)的映射关系。

虚拟机的**物理地址空间**实际也是**不连续**的，分成不同的内存区域(slot)，因为物理地址空间中通常还包括BIOS、MMIO、显存、ISA保留等部分。

qemu-kvm通过ioctl vm指令KVM_SET_USER_MEMORY_REGION来为虚拟机设置内存。主要建立guest物理地址空间中的内存区域与qemu-kvm虚拟地址空间中的内存区域的映射，从而建立其从GVA到HVA的对应关系，该对应关系主要通过kvm_mem_slot结构体保存，所以实质为设置kvm_mem_slot结构体。

本文简介ioctl vm指令KVM_SET_USER_MEMORY_REGION在内核中的执行流程，qemu-kvm用户态部分暂不包括。

## 7.2. 基本流程

ioctl vm指令KVM_SET_USER_MEMORY_REGION在内核主要执行流程如下：

```
kvm_vm_ioctl()
    kvm_vm_ioctl_set_memory_region()
        kvm_set_memory_region()
            __kvm_set_memory_region()
                kvm_iommu_unmap_pages() // 原来的slot需要删除，所以需要unmap掉相应的内存区域
                install_new_memslots() //将new分配的memslot写入kvm->memslots[]数组中
                kvm_free_physmem_slot() // 释放旧内存区域相应的物理内存(HPA)
```

## 7.3. 代码分析




# 8. vCPU的运行

在创建完**VM**和**vCPU**并且完成了**初始化工作**之后，就可以通过**调度程序调度执行**。因为在Linux中，KVM虚拟机作为一个**系统线程**运行，因此，KVM虚拟机的调度程序实际上也就是Linux的调度程序，具体的调度将在后文qemu部分进行讨论，在当前，**KVM的调用**是从ioctl的**KVM\_RUN指令**字开始的。

```cpp
// virt/kvm/kvm_main.c
static long kvm_vcpu_ioctl(struct file *filp,
			   unsigned int ioctl, unsigned long arg)
{
    case KVM_RUN:
		r = -EINVAL;
		if (arg)
			goto out;
		if (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {
			/* The thread running this VCPU changed. */
			struct pid *oldpid = vcpu->pid;
			struct pid *newpid = get_task_pid(current, PIDTYPE_PID);

			rcu_assign_pointer(vcpu->pid, newpid);
			if (oldpid)
				synchronize_rcu();
			put_pid(oldpid);
		}
		r = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);
		trace_kvm_userspace_exit(vcpu->run->exit_reason, r);
		break;
}
```

**KVM\_RUN指令字**针对**fd\_vcpu描述符**操作，当**vCPU准备完成**之后，即可通过该指令让虚拟机运行起来。

**虚拟机运行**的主要任务则是进行**上下文切换**。上下文切换的内容较多，通常包括**通用寄存器**、**浮点寄存器**、**段寄存器**、**控制寄存器**、**MSR**等，在**KVM**中，还包括**APIC状态**、**TLB**等。

通常，进行**上下文切换的过程**可以归纳为如下步骤。

1）**KVM保存自己的上下文**。

2）KVM通过使用将**kvm\_vcpu结构体**中的相关上下文加载到**物理CPU**中。

3）KVM执行**kvm\_x86\_ops**中的**run\_vcpu函数**，调用具体的平台相关指令，进入**虚拟机运行环境**中。

由此可见，上下文切换次数过于频繁会带来不小的性能开销，因此，很有必要对这方面进行优化。和操作系统进行**进程切换的思路**一样，KVM使用**Lazy Save/Restore** 的方法进行优化。其基本思想是**尽量不要对寄存器进行恢复/保存操作**，直到必须要这么做的时候，才进行类似的操作。

**执行vCPU**的请求首先发送到**kvm\_vcpu\_ioctl函数**中，然后**加载vCPU参数**，调用**kvm\_arch\_vcpu\_ioctl\_run**函数进入**具体的vCPU运行**环节。(arch/x86/kvm/x86.c)

```
kvm_arch_vcpu_ioctl_run()                	// vcpu运行
 ├─kvm_sigset_activate(vcpu)                // 
 ├─kvm_load_guset_fpu()         			//  
 ├─vcpu_run()            					// 死循环进入vcpu_enter_guest
 |	├─vcpu_enter_guest()                	// 准备guestos的运行上下文
 |	|	├─kvm_x86_ops->run(vcpu)  			// 这里会调用vmx_vcpu_run
 |	|	├─kvm_x86_ops->handle_exit(vcpu)	// vmexit的处理
 ├─         								// 
```

1）通过调用**sigprocmask函数**，保证在**vCPU的初始化**过程中，不会因为来自**其他线程的信号干扰而中断**。

2）将**vCPU的状态**切换为**KVM\_MP\_STATE\_UNINITIALIZED**。

3）配置**APIC**和**mmio**的**中断信息**。

4）对要进入的虚拟机进行一些**关键指令的测试**，在测试中主要针对**内存读/写**情况进行测试。

5）将**vCPU**中保存的**上下文信息（寄存器状态等**）写入指定的位置。

6）接下来才开始实质性的工作，调用\_**vcpu\_run函数**进行后续处理。

\_vcpu\_run函数的代码如下。

代码5\-11 \_vcpu\_run函数

```cpp
// arch/x86/kvm/x86.c
(5232)   static int __vcpu_run(struct kvm_vcpu *vcpu)￼
(5233)   {￼
(5234)        int r;￼
(5235)        struct kvm *kvm = vcpu->kvm;￼
(5236)￼
(5237)        if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_SIPI_RECEIVED)) {￼
(5238)             pr_debug("vcpu %d received sipi with vector # %x\n",￼
(5239)                   vcpu->vcpu_id, vcpu->arch.sipi_vector);￼
(5240)             kvm_lapic_reset(vcpu);￼
(5241)             r = kvm_arch_vcpu_reset(vcpu);￼
(5242)             if (r)￼
(5243)                  return r;￼
(5244)             vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;￼
(5245)        }￼
(5246)￼
(5247)        vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);￼
(5248)        vapic_enter(vcpu);￼
(5249)￼
(5250)        r = 1;￼
(5251)        while (r > 0) {￼
(5252)             if (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE)￼
(5253)                  r = vcpu_enter_guest(vcpu);￼
(5254)             else {￼
(5255)                  srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);￼
(5256)                  kvm_vcpu_block(vcpu);￼
(5257)                  vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);￼
(5258)                  if (kvm_check_request(KVM_REQ_UNHALT, vcpu))￼
(5259)                  {￼
(5260)                       switch(vcpu->arch.mp_state) {￼
(5261)                       case KVM_MP_STATE_HALTED:￼
(5262)                            vcpu->arch.mp_state =￼
(5263)                                KVM_MP_STATE_RUNNABLE;￼
(5264)                       case KVM_MP_STATE_RUNNABLE:￼
(5265)                            break;￼
(5266)                       case KVM_MP_STATE_SIPI_RECEIVED:￼
(5267)                       default:￼
(5268)                            r = -EINTR;￼
(5269)                            break;￼
(5270)                       }￼
(5271)                  }￼
(5272)             }￼
(5273)￼
(5274)             if (r <= 0)￼
(5275)                  break;￼
(5276)￼
(5277)             clear_bit(KVM_REQ_PENDING_TIMER, &vcpu->requests);￼
(5278)             if (kvm_cpu_has_pending_timer(vcpu))￼
(5279)                  kvm_inject_pending_timer_irqs(vcpu);￼
(5280)￼
(5281)             if (dm_request_for_irq_injection(vcpu)) {￼
(5282)                  r = -EINTR;￼
(5283)                  vcpu->run->exit_reason = KVM_EXIT_INTR;￼
(5284)                  ++vcpu->stat.request_irq_exits;￼
(5285)             }￼
(5286)             if (signal_pending(current)) {￼
(5287)                  r = -EINTR;￼
(5288)                  vcpu->run->exit_reason = KVM_EXIT_INTR;￼
(5289)                  ++vcpu->stat.signal_exits;￼
(5290)             }￼
(5291)             if (need_resched()) {￼
(5292)                  srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);￼
(5293)                  kvm_resched(vcpu);￼
(5294)                  vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);￼
(5295)             }￼
(5296)        }￼
(5297)￼
(5298)        srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);￼
(5299)￼
(5300)        vapic_exit(vcpu);￼
(5301)￼
(5302)        return r;￼
(5303)   }
```

该函数较长，执行的内容也比较重要，重要的部分如下。

1）在5232～5240行中，将**虚拟APIC**和**vCPU**的状态**重置**。这个操作通过调用kvm\_lapic\_reset和kvm\_arch\_vcpu\_reset函数实现。

2）在5245～5270行中，正常情况下，将kvm\_vcp\-\>arch.mp\_state的取值作为**单机未运行的取值**：KVM\_MP\_STATE\_RUNNABLE；如果 vCPU 在 VM 中处于其他状态，则会出错。在运行中有一个**循环**，直到确认了**vcpu\_enter\_guest**函数执行完毕，即物理CPU进入了GUEST状态并且执行完成后，才会执行下一步操作。

3）在5273～5278行中，将检查本次执行的一些结果。如果CPU当前有挂起的定时器或者其他中断，则会保存该中断的现场。

4）在5286～5289行中，如果对当前执行的vCPU需要调度，会引用Linux的进程调度子程序进行一次任务调度。

在上面的执行流程中，最重要的执行在 **vcpu\_enter\_guest** 函数中，该函数实现了进入客户机并且执行具体指令的操作。