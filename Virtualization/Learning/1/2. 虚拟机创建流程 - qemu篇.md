
在kvm+qemu架构下，qemu负责模拟虚拟机所有的硬件设备，并与kvm交互。

qemu是云计算中虚拟化的最终执行者，通过openstack，libvirt等封装的各种设备配置都需要qemu模拟并运行。

本文会通过解析qemu在虚拟机创建过程中的流程来向大家介绍一下qemu的大致工作流程及其工作原理。

# 从libvirt到qemu

在上一篇中我们分析了libvirt中创建虚拟机的流程，在最后阶段libvirt组装了qemu command，并通过fork调用拉起qemu进程，在宿主机上可以看到这样一个进程：

```bash
/usr/bin/qemu-system-x86_64 
-name guest=instance-000439eb,debug-threads=on 
-S 
-machine pc-i440fx-2.5,accel=kvm,usb=off 
-cpu IvyBridge,+ds,+acpi,+ss,+ht,+tm,+pbe,+dtes64,+monitor,+ds_cpl,+vmx,+smx,+est,+tm2,+xtpr,+pdcm,+pcid,+dca,+osxsave,+pdpe1gb 
-m size=1048576k,slots=64,maxmem=268435456k 
-realtime mlock=off 
-smp 1,maxcpus=64,sockets=64,cores=1,threads=1 
-numa node,nodeid=0,cpus=0-63,mem=1024 
-uuid 2178112f-1e08-4a0b-b495-3bcc8faf3d59 
-smbios type=1,manufacturer=OpenStack Foundation,product=OpenStack Nova,version=2013.2-netease.910,serial=44454c4c-3900-1057-8032-b6c04f373232,uuid=2178112f-1e08-4a0b-b495-3bcc8faf3d59 

-drive file=rbd:vms/2178112f-1e08-4a0b-b495-3bcc8faf3d59_disk:auth_supported=none:mon_host=10.180.0.47\:6789\;10.180.0.48\:6789\;10.180.0.49\:6789,format=raw,if=none,id=drive-virtio-disk0,cache=none 
-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 

-drive file=rbd:vms/2178112f-1e08-4a0b-b495-3bcc8faf3d59_disk.config:auth_supported=none:mon_host=10.180.0.47\:6789\;10.180.0.48\:6789\;10.180.0.49\:6789,format=raw,if=none,id=drive-virtio-disk25,readonly=on,cache=none 
-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk25,id=virtio-disk25 

-netdev tap,fd=179,id=hostnet0,vhost=on,vhostfd=183 
-device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:38:e9:53,bus=pci.0,addr=0x3 
-chardev file,id=charserial0,path=/data/nova/instances/2178112f-1e08-4a0b-b495-3bcc8faf3d59/console.log 
-device isa-serial,chardev=charserial0,id=serial0 
-chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 
-chardev socket,id=charchannel0,path=/var/lib/libvirt/qemu/org.qemu.guest_agent.0.instance-000439eb.sock,server,nowait 
-device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 
-device usb-tablet,id=input0 
-vnc 10.180.0.47:64,password -k en-us 
-device cirrus-vga,id=video0,bus=pci.0,addr=0x2 
-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x7 
-msg timestamp=on
-no-user-config 
-nodefaults 
-chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-408-instance-000439eb/monitor.sock,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control 
-rtc base=utc,driftfix=slew 
-global kvm-pit.lost_tick_policy=discard 
-no-shutdown 
-boot strict=on 
-device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 
-device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x4 
```

可以看到，qemu进程的可执行文件是qemu-system-x86_64，该文件是x86_64架构的模拟器。

# qemu+kvm虚拟化原理

在qemu kvm架构下，qemu负责各种设备的模拟，kvm则负责保障虚拟机中的代码可以正常执行。

具体来说，kvm暴露一个设备文件接口/dev/kvm给用户态的qemu进程。而qemu进程通过系统调用ioctl操作kvm接口，完成一些需要真实硬件参与的虚拟机操作。

## vcpu运行

现在使用的x86架构的虚拟化技术利用了intel的VT-x技术。vt-x的基本思想是区分cpu的工作模式，root和非root模式。每一种模式又分为0-3四个特权级。在虚拟机中cpu运行在非root模式下，当执行敏感指令时，cpu会自动从非root模式切换到root模式，称为vm-exit。对应的，VMM也会发起从root模式到非root模式的切换，称为vm-entry。VT-x还引入了VMCS的概念，用户保存cpu在各种模式下的运行状态，方便cpu在多种模式下的切换。VMCS在系统中存储在一块最大不超过4kB大小的内存中，内容包括VMCS版本号，VMCS中止标识以及VMCS数据域。在数据域中包括如下各种信息：

* 客户机状态域 在虚拟机内运行时，即非root模式下CPU的状态。vm-exit发生时，cpu当前的状态会存储到客户机状态域。vm-entry发生时，从客户机状态域恢复cpu状态。
* 宿主机状态域 在VMM运行时，即root模式下CPU的状态。vm-exit发生时，cpu从这里恢复cpu运行状态。
* vm-entry控制域 控制vm-entry的过程
* vm-execution控制域 控制非根模式下的行为
* vm-exit控制域 控制vm-exit的过程
* vm-exit信息域 提供vm-exit的原因和其他信息，只读域。

## 内存访问

在虚拟化场景下，虚拟机内部如果需要访问一段内存，需要经过两步映射才能找到真正的物理地址： Guest虚拟机地址(GVA)->Guest物理地址（GPA）->宿主机虚拟地址（HVA）->宿主机物理地址（HPA）

## 影子页表

在hypervisor中维护一张内存影子页表，根据GVA-GPA-HVA-HPA的映射关系直接计算GVA-HPA的映射关系，并将对应的映射关系写入影子页表。这样可以解决虚拟机内存访问的问题，但是依赖软件实现的影子页表也带来了很多问题。像各种页表之间的同步问题，页表本身的内存开销等。

## EPT
EPT页表利用硬件实现了从GPA到HPA的映射，每个虚拟机只需要维护一个EPT页表即可。减少了开销，提高了性能。