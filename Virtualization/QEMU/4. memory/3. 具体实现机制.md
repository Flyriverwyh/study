
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 整体概述](#1-整体概述)
- [2. 回调函数的注册](#2-回调函数的注册)
- [3. 全局的系统AS和MR的初始化: 不分配GPA/HVA](#3-全局的系统as和mr的初始化-不分配gpahva)
  - [3.1. address_space_init(): 整个AS初始化](#31-address_space_init-整个as初始化)
- [4. 实际内存的分配GPA/HVA](#4-实际内存的分配gpahva)
  - [4.1. 分配内存(pc.ram, pc.rom): memory_region_allocate_system_memory](#41-分配内存pcram-pcrom-memory_region_allocate_system_memory)
    - [4.1.1. memory_region_init_ram: 创建ramblock并从host上分配虚拟内存](#411-memory_region_init_ram-创建ramblock并从host上分配虚拟内存)
    - [4.1.2. vmstate_register_ram_global(): ramblock与mr关联](#412-vmstate_register_ram_global-ramblock与mr关联)
  - [4.2. 将整体mr(pc.ram)划分为两个子MR(subregion): 不再申请内存, 即不关联host虚拟内存(HVA)](#42-将整体mrpcram划分为两个子mrsubregion-不再申请内存-即不关联host虚拟内存hva)
    - [4.2.1. 计算above_4g_mem_size和below_4g_mem_size](#421-计算above_4g_mem_size和below_4g_mem_size)
    - [4.2.2. 从pc.ram中划分并加入system_memory的subregion](#422-从pcram中划分并加入system_memory的subregion)
      - [4.2.2.1. memory_region_init_alias(): 初始化alias的mr, 不申请内存](#4221-memory_region_init_alias-初始化alias的mr-不申请内存)
      - [4.2.2.2. memory_region_add_subregion(): subregion注册到全局的system_memory(不是pc.ram)](#4222-memory_region_add_subregion-subregion注册到全局的system_memory不是pcram)
  - [4.3. 提交修改: memory_region_transaction_commit()](#43-提交修改-memory_region_transaction_commit)
    - [4.3.1. address_space_update_topology(): 设置拓扑flatview, 会与KVM交互](#431-address_space_update_topology-设置拓扑flatview-会与kvm交互)
      - [4.3.1.1. generate_memory_topology(mr): 根据给定的mr生成新的Flatview](#4311-generate_memory_topologymr-根据给定的mr生成新的flatview)
      - [4.3.1.2. address_space_set_flatview(): 设置as的FlatView, 会调用KVM](#4312-address_space_set_flatview-设置as的flatview-会调用kvm)
- [5. 新FlatRange的更新](#5-新flatrange的更新)
- [6. 参考](#6-参考)

<!-- /code_chunk_output -->

# 1. 整体概述

qemu部分的内存申请流程上可以分为三小部分，分成三小部分主要是我在看代码的时候觉得这三部分耦合性不是很大，相对而言比较独立。

众所周知，qemu起始于vl.c中的main函数，那么这三部分也按照在main函数中的调用顺序分别介绍。

# 2. 回调函数的注册

涉及函数：`configure_accelerator() -->kvm_init()-->memory_listener_register ()`

这里所说的accelerator在这里就是kvm，初始化函数自然调用了`kvm_init`，该函数主要完成对**kvm的初始化**，包括一些常规检查如CPU个数、kvm版本等，还会通过ioctl和内核交互**创建kvm结构**，这些并非本文重点，不在赘述。在`kvm_init`函数的结尾调用了`memory_listener_register`

```cpp
memory_listener_register(&kvm_memory_listener, &address_space_memory);
memory_listener_register(&kvm_io_listener, &address_space_io);
```

通过`memory_listener_register`函数，针对**地址空间！！！** 注册了**lisenter**,lisenter本身是**一组函数表**，当**地址空间发生变化**的时候，会调用到listener中的相应函数，从而保持**内核**和**用户空间**的**内存信息的一致性**。

**虚拟机**包含有**两个地址空间**`address_space_memory`和`address_space_io`，很容易理解，正常系统也包含**系统地址空间**和**IO地址空间**。

`memory_listener_register`函数不复杂，咱们看下

```cpp
void memory_listener_register(MemoryListener *listener, AddressSpace *filter)
{
    MemoryListener *other = NULL;
    AddressSpace *as;

    listener->address_space_filter = filter;
    /*如果listener为空或者当前listener的优先级大于最后一个listener的优先级，则可以直接插入*/
    if (QTAILQ_EMPTY(&memory_listeners)
        || listener->priority >= QTAILQ_LAST(&memory_listeners,
                                             memory_listeners)->priority) {
        QTAILQ_INSERT_TAIL(&memory_listeners, listener, link);
    } else {
        /*listener按照优先级升序排列*/
        QTAILQ_FOREACH(other, &memory_listeners, link) {
            if (listener->priority < other->priority) {
                break;
            }
        }
        /*插入listener*/
        QTAILQ_INSERT_BEFORE(other, listener, link);
    }
    /*全局address_spaces-->as*/
    /*对于每个address_spaces，设置listener*/
    QTAILQ_FOREACH(as, &address_spaces, address_spaces_link) {
        listener_add_address_space(listener, as);
    }
}
```

系统中可以存在**多个listener**，listener之间有着明确的**优先级关系**，通过**链表**进行组织，**链表头**是**全局**的`memory_listeners`。

```cpp
static QTAILQ_HEAD(, MemoryListener) memory_listeners
    = QTAILQ_HEAD_INITIALIZER(memory_listeners);
```

函数中，如果`memory_listeners`为空或者**当前listener**的**优先级**大于最后一个listener的优先级，即直接把当前listener插入。否则需要**挨个遍历链表**，找到**合适的位置**。具体按照优先级升序查找。

在函数最后还针对**每个address_space**，调用`listener_add_address_space`函数，该函数对其对应的`address_space`管理的**flatrange**向**KVM注册**。当然，实际上**此时address_space**尚未经过初始化，所以这里的循环其实是**空循环**。

# 3. 全局的系统AS和MR的初始化: 不分配GPA/HVA

涉及函数：`cpu_exec_init_all() -> memory_map_init()` 

在第一节中已经**注册了listener**，但是**addressspace尚未初始化**，本节就介绍下其初始化流程。

从上节的`configure_accelerator()`函数往下走，会执行`cpu_exec_init_all() -> memory_map_init()`函数，该函数主要**初始化**了**IO地址空间**和**系统地址空间**。

原本AS是树状结构, 通过该方式将其展开成平面结构，最终得到的FlatView相当于一个内存条, 最终生成的结构体间关系如下图所示

![2020-03-11-09-44-30.png](./images/2020-03-11-09-44-30.png)

```cpp
static void memory_map_init(void)
{
    /*为system_memory分配内存*/
    system_memory = g_malloc(sizeof(*system_memory));

    memory_region_init(system_memory, NULL, "system", UINT64_MAX);
    /*初始化全局的address_space_memory*/
    address_space_init(&address_space_memory, system_memory, "memory");

    system_io = g_malloc(sizeof(*system_io));
    memory_region_init_io(system_io, NULL, &unassigned_io_ops, NULL, "io",
                          65536);
    address_space_init(&address_space_io, system_io, "I/O");
}
```

首先MemoryRegion有两种,一种为**纯容器**(逻辑上的概念),一种为**物理内存**(物理内存又有几种: mmio内存. ram, rom, ioport),

地址空间即CPU所能看到的空间，包括Io空间和memory空间，IO空间使用in、out指令访问，有回掉函数。Memory空间使用mov等指令访问。

所以在函数起始，就对`system_memory`分配了内存，然后调用了`memory_region_init`函数对其进行初始化，其中**size**设置为**整个地址空间**：如果是**64位**就是`2^64`.

## 3.1. address_space_init(): 整个AS初始化

接着调用了`address_space_init`函数对`address_space_memory`进行了初始化。

```cpp
void address_space_init(AddressSpace *as, MemoryRegion *root, const char *name)
{
    memory_region_ref(root);
    /* 指定address_space_memory的root为system_memory */
    as->root = root;
    as->current_map = NULL;
    as->ioeventfd_nb = 0;
    as->ioeventfds = NULL;
    /* 初始化AddressSpace的listeners链表 */
    QTAILQ_INIT(&as->listeners);
    /* 把address_space_memory插入全局链表 */
    QTAILQ_INSERT_TAIL(&address_spaces, as, address_spaces_link);
    /* AddressSpace的名称 */
    as->name = g_strdup(name ? name : "anonymous");
    // 初始化对应的 flatview, 设置拓扑
    address_space_update_topology(as);
    address_space_update_ioeventfds(as);
}
```

函数主要做了以下几个工作，设置**addressSpaceh和MR的关联**，设置**其名称**, 把`address_space_memory`加入到**全局的address_spaces链表**中，并通过`address_space_update_topology(as)`**初始化对应的FlatView**, 在这个里面会调用`MEMORY_LISTENER_UPDATE_REGION()`**提交本次修改**，这个后面做论述。

`address_space_update_topology()`调用下面会分析.

回到`memory_map_init()`函数中，接下来按照同样的模式对IO区域`system_io`和IO地址空间`address_space_io`做了初始化。

针对系统内核、系统IO的MR和AS的初始化， 但是并不分配GPA/HVA, MR和AS都是全局的

# 4. 实际内存的分配GPA/HVA

前面**注册listener**也好，或是初始化addressspace也好，实际上均**没有对应的物理内存**。

顺着main函数往下走，会调用到`machine—>init`，实际上对应于`pc_init1`函数，在该函数中有`pc_memory_init()`函数对实际的内存做了分配。

我们直接从`pc_memory_init()`函数开始

```cpp
void pc_memory_init(PCMachineState *pcms,
                    MemoryRegion *system_memory,
                    MemoryRegion *rom_memory,
                    MemoryRegion **ram_memory)
{
    int linux_boot, i;
    MemoryRegion *ram, *option_rom_mr;
    MemoryRegion *ram_below_4g, *ram_above_4g;
    FWCfgState *fw_cfg;
    MachineState *machine = MACHINE(pcms);
    MachineClass *mc = MACHINE_GET_CLASS(machine);
    PCMachineClass *pcmc = PC_MACHINE_GET_CLASS(pcms);
    X86MachineState *x86ms = X86_MACHINE(pcms);

    assert(machine->ram_size == x86ms->below_4g_mem_size +
                                x86ms->above_4g_mem_size);

    linux_boot = (machine->kernel_filename != NULL);

    /* Allocate RAM.  We allocate it as a single memory region and use
     * aliases to address portions of it, mostly for backwards compatibility
     * with older qemus that used qemu_ram_alloc().
     */
    /* 分配新的整体MR */
    ram = g_malloc(sizeof(*ram));
    /* 根据虚拟机内存大小初始化MR, 为其分配具体的内存, 将mr->name设进block */
    memory_region_allocate_system_memory(ram, NULL, "pc.ram",
                                         machine->ram_size);
    *ram_memory = ram;
    ram_below_4g = g_malloc(sizeof(*ram_below_4g));
    // 对前面的整体mr(pc.ram)进行划分
    memory_region_init_alias(ram_below_4g, NULL, "ram-below-4g", ram,
                             0, x86ms->below_4g_mem_size);
    // 将mr(ram_below_4g)加入全局mr(system_memory)的子mr
    memory_region_add_subregion(system_memory, 0, ram_below_4g);
    e820_add_entry(0, x86ms->below_4g_mem_size, E820_RAM);
    if (x86ms->above_4g_mem_size > 0) {
        ram_above_4g = g_malloc(sizeof(*ram_above_4g));
        memory_region_init_alias(ram_above_4g, NULL, "ram-above-4g", ram,
                                 x86ms->below_4g_mem_size,
                                 x86ms->above_4g_mem_size);
        memory_region_add_subregion(system_memory, 0x100000000ULL,
                                    ram_above_4g);
        e820_add_entry(0x100000000ULL, x86ms->above_4g_mem_size, E820_RAM);
    }

    if (!pcmc->has_reserved_memory &&
        (machine->ram_slots ||
         (machine->maxram_size > machine->ram_size))) {

        error_report("\"-memory 'slots|maxmem'\" is not supported by: %s",
                     mc->name);
        exit(EXIT_FAILURE);
    }
    /* always allocate the device memory information */
    machine->device_memory = g_malloc0(sizeof(*machine->device_memory));

    /* initialize device memory address space */
    if (pcmc->has_reserved_memory &&
        (machine->ram_size < machine->maxram_size)) {
        ram_addr_t device_mem_size = machine->maxram_size - machine->ram_size;

        if (machine->ram_slots > ACPI_MAX_RAM_SLOTS) {
            error_report("unsupported amount of memory slots: %"PRIu64,
                         machine->ram_slots);
            exit(EXIT_FAILURE);
        }

        if (QEMU_ALIGN_UP(machine->maxram_size,
                          TARGET_PAGE_SIZE) != machine->maxram_size) {
            error_report("maximum memory size must by aligned to multiple of "
                         "%d bytes", TARGET_PAGE_SIZE);
            exit(EXIT_FAILURE);
        }

        machine->device_memory->base =
            ROUND_UP(0x100000000ULL + x86ms->above_4g_mem_size, 1 * GiB);

        if (pcmc->enforce_aligned_dimm) {
            /* size device region assuming 1G page max alignment per slot */
            device_mem_size += (1 * GiB) * machine->ram_slots;
        }

        if ((machine->device_memory->base + device_mem_size) <
            device_mem_size) {
            error_report("unsupported amount of maximum memory: " RAM_ADDR_FMT,
                         machine->maxram_size);
            exit(EXIT_FAILURE);
        }

        memory_region_init(&machine->device_memory->mr, OBJECT(pcms),
                           "device-memory", device_mem_size);
        memory_region_add_subregion(system_memory, machine->device_memory->base,
                                    &machine->device_memory->mr);
    }
    /* Initialize PC system firmware */
    pc_system_firmware_init(pcms, rom_memory);

    option_rom_mr = g_malloc(sizeof(*option_rom_mr));
    memory_region_init_ram(option_rom_mr, NULL, "pc.rom", PC_ROM_SIZE,
                           &error_fatal);
    if (pcmc->pci_enabled) {
        memory_region_set_readonly(option_rom_mr, true);
    }
    memory_region_add_subregion_overlap(rom_memory,
                                        PC_ROM_MIN_VGA,
                                        option_rom_mr,
                                        1);

    fw_cfg = fw_cfg_arch_create(machine,
                                x86ms->boot_cpus, x86ms->apic_id_limit);

    rom_set_fw(fw_cfg);

    if (pcmc->has_reserved_memory && machine->device_memory->base) {
        uint64_t *val = g_malloc(sizeof(*val));
        PCMachineClass *pcmc = PC_MACHINE_GET_CLASS(pcms);
        uint64_t res_mem_end = machine->device_memory->base;

        if (!pcmc->broken_reserved_end) {
            res_mem_end += memory_region_size(&machine->device_memory->mr);
        }
        *val = cpu_to_le64(ROUND_UP(res_mem_end, 1 * GiB));
        fw_cfg_add_file(fw_cfg, "etc/reserved-memory-end", val, sizeof(*val));
    }

    if (linux_boot) {
        x86_load_linux(x86ms, fw_cfg, pcmc->acpi_data_size,
                       pcmc->pvh_enabled, pcmc->linuxboot_dma_enabled);
    }

    for (i = 0; i < nb_option_roms; i++) {
        rom_add_option(option_rom[i].name, option_rom[i].bootindex);
    }
    x86ms->fw_cfg = fw_cfg;

    /* Init default IOAPIC address space */
    x86ms->ioapic_as = &address_space_memory;

    /* Init ACPI memory hotplug IO base address */
    pcms->memhp_io_base = ACPI_MEMORY_HOTPLUG_BASE;
}
```

从总体上来讲，该函数主要完成了三个工作：

1. 为pc.ram和pc.rom都生成新的MR, 并**分配内存**（一整个memory region）

2. 然后根据`below_4g_mem_size`、`above_4g_mem_size`分别对**pc.ram进行划分**，形成**子MR**，并注册**子MR**到**root MR system_memory 的subregions链表**中。

3. 最后需要调用`memory_region_transaction_commit()`函数提交修改.

## 4.1. 分配内存(pc.ram, pc.rom): memory_region_allocate_system_memory

为虚拟机内存"pc.ram"和"pc.rom"都分别生成了新的MR, 然后分配内存, 这里看"pc.ram"的内存分配`memory_region_allocate_system_memory(ram, NULL, "pc.ram", machine->ram_size);`, 对于非大页最终会调用到函数`memory_region_init_ram_shared_nomigrate()`

```cpp
memory_region_allocate_system_memory()
    allocate_system_memory_nonuma()
    | └- memory_region_init_ram_shared_nomigrate()
    └- vmstate_register_ram_global();
```

### 4.1.1. memory_region_init_ram: 创建ramblock并从host上分配虚拟内存

```cpp
void memory_region_init_ram_shared_nomigrate(MemoryRegion *mr,
                                             Object *owner,
                                             const char *name,
                                             uint64_t size,
                                             bool share,
                                             Error **errp)
{
    Error *err = NULL;
    // 这个mr就是为系统内存重新分配的, 这里只是初始化
    memory_region_init(mr, owner, name, size);
    // 是ram
    mr->ram = true;
    // 不是纯容器
    mr->terminates = true;
    mr->destructor = memory_region_destructor_ram;
    // 分配了具体的内存, 主机虚拟内存, 返回RAMBlock
    mr->ram_block = qemu_ram_alloc(size, share, mr, &err);
    mr->dirty_log_mask = tcg_enabled() ? (1 << DIRTY_MEMORY_CODE) : 0;
    if (err) {
        mr->size = int128_zero();
        object_unparent(OBJECT(mr));
        error_propagate(errp, err);
    }
}
```

该函数实现很简单，首先调用`memory_region_init`对**MR**做**初始化**。

然后进行简单的设置，最重要的还是最后的`qemu_ram_alloc`, 最终调用`qemu_ram_alloc_internal(size, size, NULL, NULL, false, share, mr, errp)`

```cpp
// exec.c
static
RAMBlock *qemu_ram_alloc_internal(ram_addr_t size, ram_addr_t max_size,
                                  void (*resized)(const char*,
                                                  uint64_t length,
                                                  void *host),
                                  void *host, bool resizeable, bool share,
                                  MemoryRegion *mr, Error **errp)
{
    RAMBlock *new_block;
    Error *local_err = NULL;
    // 根据主机页面对齐
    size = HOST_PAGE_ALIGN(size);
    max_size = HOST_PAGE_ALIGN(max_size);
    // RAMBlock 生成
    new_block = g_malloc0(sizeof(*new_block));
    // 指向pc.ram/pc.rom
    new_block->mr = mr;
    // NULL
    new_block->resized = resized;
    // 已使用长度, 等于
    new_block->used_length = size;
    new_block->max_length = max_size;
    assert(max_size >= size);
    // 映射文件描述符
    new_block->fd = -1;
    // 页面大小
    new_block->page_size = qemu_real_host_page_size;
    // 这里是NULL
    new_block->host = host;
    if (host) {
        new_block->flags |= RAM_PREALLOC;
    }
    if (resizeable) {
        new_block->flags |= RAM_RESIZEABLE;
    }
    // 添加该ramblock, 里面从host上分配内存
    ram_block_add(new_block, &local_err, share);
    if (local_err) {
        g_free(new_block);
        error_propagate(errp, local_err);
        return NULL;
    }
    return new_block;
}

static void ram_block_add(RAMBlock *new_block, Error **errp, bool shared)
{
    RAMBlock *block;
    RAMBlock *last_block = NULL;
    ram_addr_t old_ram_size, new_ram_size;
    Error *err = NULL;

    old_ram_size = last_ram_page();

    qemu_mutex_lock_ramlist();
    // 在虚拟机物理地址空间的偏移
    new_block->offset = find_ram_offset(new_block->max_length);
    // 还没分配, 所以会进入
    if (!new_block->host) {
        if (xen_enabled()) {
            xen_ram_alloc(new_block->offset, new_block->max_length,
                          new_block->mr, &err);
            if (err) {
                error_propagate(errp, err);
                qemu_mutex_unlock_ramlist();
                return;
            }
        } else {
            // 在host上分配虚拟内存, 其实就是alloc()函数
            new_block->host = phys_mem_alloc(new_block->max_length,
                                             &new_block->mr->align, shared);
            if (!new_block->host) {
                error_setg_errno(errp, errno,
                                 "cannot set up guest memory '%s'",
                                 memory_region_name(new_block->mr));
                qemu_mutex_unlock_ramlist();
                return;
            }
            // 与相邻的block合并
            memory_try_enable_merging(new_block->host, new_block->max_length);
        }
    }

    new_ram_size = MAX(old_ram_size,
              (new_block->offset + new_block->max_length) >> TARGET_PAGE_BITS);
    if (new_ram_size > old_ram_size) {
        dirty_memory_extend(old_ram_size, new_ram_size);
    }
    /* Keep the list sorted from biggest to smallest block.  Unlike QTAILQ,
     * QLIST (which has an RCU-friendly variant) does not have insertion at
     * tail, so save the last element in last_block.
     */
    RAMBLOCK_FOREACH(block) {
        last_block = block;
        if (block->max_length < new_block->max_length) {
            break;
        }
    }
    if (block) {
        QLIST_INSERT_BEFORE_RCU(block, new_block, next);
    } else if (last_block) {
        QLIST_INSERT_AFTER_RCU(last_block, new_block, next);
    } else { /* list is empty */
        QLIST_INSERT_HEAD_RCU(&ram_list.blocks, new_block, next);
    }
    ram_list.mru_block = NULL;

    /* Write list before version */
    smp_wmb();
    ram_list.version++;
    qemu_mutex_unlock_ramlist();

    cpu_physical_memory_set_dirty_range(new_block->offset,
                                        new_block->used_length,
                                        DIRTY_CLIENTS_ALL);

    if (new_block->host) {
        qemu_ram_setup_dump(new_block->host, new_block->max_length);
        qemu_madvise(new_block->host, new_block->max_length, QEMU_MADV_HUGEPAGE);
        /* MADV_DONTFORK is also needed by KVM in absence of synchronous MMU */
        qemu_madvise(new_block->host, new_block->max_length, QEMU_MADV_DONTFORK);
        ram_block_notify_add(new_block->host, new_block->max_length);
    }
}
```

咱们重点看下这个函数，该函数围绕**RAMBlock结构**，在函数起始对**size进行页对齐**，然后**申请一个RAMBlock结构**，之后对其字段进行一些设置如**指定MR** 和offset，**offset**理论上需要调用`find_ram_offset`在已有的**RAMBlock**中找到一个可用的。但是此时实际上**还没有其他block**，所以这里应该**直接返回0**的。

![2020-02-29-18-57-01.png](./images/2020-02-29-18-57-01.png)

上图是qemu模拟的普通内存分布模型, 内存的线性也是分块被使用的，每个块称为RAMBlock，由ram_list统领，RAMBlock.offset则是区块的线性地址，即相对于开始的偏移位，RAMBlock.length(size)则是区块的大小. `find_ram_offset`函数则是在**线性区间内**找到**没有使用的一段空间**，可以完全容纳**新申请的ramblock length大小**，代码就是进行了所有区块的遍历(全局链表`ram_list`)，找到满足新申请length的最小区间，把ramblock安插进去即可，返回的offset即是新分配区间的开始地址。

本部分最重要的莫过于**设置其host**了，其对应的**宿主机虚拟地址空间！！！** 的**虚拟地址**。在支持**大页**的情况下调用`file_ram_alloc()`函数进行分配，否则调用`phys_mem_alloc()`函数进行分配。二者分配其实都是利用**mmap**分配的，但是在支持大页的情况下需要传递参数`mem_path`,所以需要两个函数。

在**非大页**的情况下，分配好内存尝试**和相邻的block合并**下。最后把block插入到全局的链表`ram_list`中，只是block保持**从大到小**的顺序。

### 4.1.2. vmstate_register_ram_global(): ramblock与mr关联

```cpp
void vmstate_register_ram_global(MemoryRegion *mr)
{
    vmstate_register_ram(mr, NULL);
}

void vmstate_register_ram(MemoryRegion *mr, DeviceState *dev)
{
    qemu_ram_set_idstr(mr->ram_block,
                       memory_region_name(mr), dev);
    qemu_ram_set_migratable(mr->ram_block);
}
```

`vmstate_register_ram_global`这个函数则是负责将前面提到的ramlist中的**ramblock**和**memory region**的初始地址对应一下，将`mr->name`填充到ramblock的**idstr**里面，就是让二者有确定的对应关系，如此**mr**就有了**物理内存**使用。

## 4.2. 将整体mr(pc.ram)划分为两个子MR(subregion): 不再申请内存, 即不关联host虚拟内存(HVA)

在创建好了ram并且分配好了空间之后，创建了**两个mr alias**，`ram_below_4g`以及`ram_above_4g`，这两个mr分别指向**ram的低4g**以及**高4g空间**，这两个alias是挂在**根system\_memory mr下面**的, 即高低端内存（也不一定是32bit机器）

### 4.2.1. 计算above_4g_mem_size和below_4g_mem_size

对于 `x86_64`, 需要给system io留0.5G内存, 检查虚拟机内存是否大于4G(ram_size是否大于3.5G), 计算如下

```cpp
PCMAchineState *pcms;
X86MachineState *x86ms = X86_MACHINE(pcms);

pc_init1()
    if (machine->ram_size >= 0xe0000000) {
        lowmem = gigabyte_align ? 0xc0000000 : 0xe0000000;
    } else {
        lowmem = 0xe0000000;
    }

    if (machine->ram_size >= lowmem) {
        above_4g_mem_size = machine->ram_size - lowmem;
        below_4g_mem_size = lowmem;
    } else {
        above_4g_mem_size = 0;
        below_4g_mem_size = machine->ram_size;
    }
```

其中 0xc0000000 为 3G, 0xe0000000 为 3.5G; 总之, 无论是否32位机器

- 内存大于3.5G, `below_4g_mem_size = 3G`, `above = size - 3G`;
- 内存小于3.5G, `below_4g_mem_size = size`, `above = 0`;


### 4.2.2. 从pc.ram中划分并加入system_memory的subregion

```cpp
    // 创建新的 mr
    MemoryRegion *ram_below_4g = g_malloc(sizeof(*ram_below_4g));
    // 对前面的整体mr(pc.ram)进行划分
    memory_region_init_alias(ram_below_4g, NULL, "ram-below-4g", ram,
                             0, x86ms->below_4g_mem_size);
    // 将mr(ram_below_4g)加入全局mr(system_memory)的子mr
    memory_region_add_subregion(system_memory, 0, ram_below_4g);
    e820_add_entry(0, x86ms->below_4g_mem_size, E820_RAM);
    if (x86ms->above_4g_mem_size > 0) {
        MemoryRegion *ram_above_4g = g_malloc(sizeof(*ram_above_4g));
        memory_region_init_alias(ram_above_4g, NULL, "ram-above-4g", ram,
                                 x86ms->below_4g_mem_size,
                                 x86ms->above_4g_mem_size);
        memory_region_add_subregion(system_memory, 0x100000000ULL,
                                    ram_above_4g);
        e820_add_entry(0x100000000ULL, x86ms->above_4g_mem_size, E820_RAM);
    }
```

#### 4.2.2.1. memory_region_init_alias(): 初始化alias的mr, 不申请内存

`memory_region_init_alias()`函数便对**两个子MR进行初始化**，这里就**不再进行内存的申请**，主要是设置`alias`和`alias_offset`字段，代码如下。

```cpp
void memory_region_init_alias(MemoryRegion *mr,
                              Object *owner,
                              const char *name,
                              MemoryRegion *orig,
                              hwaddr offset,
                              uint64_t size)
{
    memory_region_init(mr, owner, name, size);
    // 这里就是 pc.ram 的MR
    mr->alias = orig;
    // 这里就是相对于 pc.ram 的偏移
    mr->alias_offset = offset;
}
```

12月16日

#### 4.2.2.2. memory_region_add_subregion(): subregion注册到全局的system_memory(不是pc.ram)

在初始化完毕，需要调用`memory_region_add_subregion()`函数把**子MR**注册到**全局的system_memory**。该函数实现是比较简单的，但是需要重点介绍下，因为这里执行了关键的更新操作。

```cpp
memory_region_add_subregion(system_memory, 0, ram_below_4g);
memory_region_add_subregion(system_memory, 0x100000000ULL, ram_above_4g);

void memory_region_add_subregion(MemoryRegion *mr,
                                 hwaddr offset,
                                 MemoryRegion *subregion)
{
    // 优先级
    subregion->priority = 0;
    memory_region_add_subregion_common(mr, offset, subregion);
}
```

函数核心交给`memory_region_add_subregion_common()`函数实现

```cpp
static void memory_region_add_subregion_common(MemoryRegion *mr,
                                               hwaddr offset,
                                               MemoryRegion *subregion)
{
    assert(!subregion->container);
    // container为system_memory
    subregion->container = mr;
    // addr 是在AddressSpace中的地址, GPA, 
    subregion->addr = offset;
    memory_region_update_container_subregions(subregion);
}
```

该函数同样比较简单，设置`subregion->container`和`system_memory`的关联，设置**其addr字段**为offset，即在**全局AS中的偏移**。

```cpp
static void memory_region_update_container_subregions(MemoryRegion *subregion)
{
    MemoryRegion *mr = subregion->container;
    MemoryRegion *other;

    memory_region_transaction_begin();

    memory_region_ref(subregion);
    QTAILQ_FOREACH(other, &mr->subregions, subregions_link) {
        if (subregion->priority >= other->priority) {
            QTAILQ_INSERT_BEFORE(other, subregion, subregions_link);
            goto done;
        }
    }
    QTAILQ_INSERT_TAIL(&mr->subregions, subregion, subregions_link);
done:
    memory_region_update_pending |= mr->enabled && subregion->enabled;
    memory_region_transaction_commit();
}
```

然后就按照**优先级顺序**把**subregion**插入到`system_memory`的**subregions链表**中，这些都比较简单.

对上述x86_64的`memory_region_init_alias()`画图描述：

![2020-03-01-00-23-18.png](./images/2020-03-01-00-23-18.png)

上述x86_64 ram的`memory_region_add_subregion_common()`

![2020-03-01-00-23-42.png](./images/2020-03-01-00-23-42.png)

目前我们添加了区域，**地址空间已经发生变化**，自然要把变化和KVM进行同步，这一工作由`memory_region_transaction_commit()`实现。

## 4.3. 提交修改: memory_region_transaction_commit()

```cpp
void memory_region_transaction_commit(void)
{
    AddressSpace *as;

    assert(memory_region_transaction_depth);
    --memory_region_transaction_depth;
    if (!memory_region_transaction_depth && memory_region_update_pending) {
        memory_region_update_pending = false;
        MEMORY_LISTENER_CALL_GLOBAL(begin, Forward);
        /*更新各个addressspace 拓扑结构*/
        QTAILQ_FOREACH(as, &address_spaces, address_spaces_link) {
            address_space_update_topology(as);
        }

        MEMORY_LISTENER_CALL_GLOBAL(commit, Forward);
    }
}
```

可以看到，这里listener的作用就凸显出来了。

对于**每个address_space！！！**，调用`address_space_update_topology()`执行更新。

### 4.3.1. address_space_update_topology(): 设置拓扑flatview, 会与KVM交互

这个在`address_space_init()`时候也会调用, 当`address_space_memory`调用时, mr就是`system_memory`

```cpp
static void address_space_update_topology(AddressSpace *as)
{
    // 获取as的mr, 如果是alias, 使用原始mr
    MemoryRegion *physmr = memory_region_get_flatview_root(as->root);

    flatviews_init();
    if (!g_hash_table_lookup(flat_views, physmr)) {
        // 核心函数, 基于root MR生成 flatview
        generate_memory_topology(physmr);
    }
    address_space_set_flatview(as);
}
```

里面涉及两个重要的函数`generate_memory_topology`和`address_space_update_topology_pass`。

前者对于**一个给定的MR**，生成其**对应的FlatView**，而后者则根据**oldview**和**newview**对当前视图进行更新。

#### 4.3.1.1. generate_memory_topology(mr): 根据给定的mr生成新的Flatview

```cpp
/* Render a memory topology into a list of disjoint absolute ranges. */
static FlatView *generate_memory_topology(MemoryRegion *mr)
{
    int i;
    FlatView *view;

    /* 根据AddressSpace对应的MR生成一个新的FlatView */
    view = flatview_new(mr);

    if (mr) {
        /*
          * 将mr所管理的内存进行平坦展开为FlatView,通过view返回
          * addrrange_make(int128_zero(), int128_2_64()) --> 指定一个GUEST内存空间，起始GPA等于0，大小等于2^64，可以想象成GUEST的整个物理地址空间
          */
        render_memory_region(view, mr, int128_zero(),
                             addrrange_make(int128_zero(), int128_2_64()),
                             false, false);
    }
    /* 简化FlatView，将View中的FlatRange能合并的都合并 */
    flatview_simplify(view);
    // dispatch 负责根据 GPA 找到 HVA
    view->dispatch = address_space_dispatch_new(view);
    for (i = 0; i < view->nr; i++) {
        MemoryRegionSection mrs =
            section_from_flat_range(&view->ranges[i], view);
        flatview_add_to_dispatch(view, &mrs);
    }
    address_space_dispatch_compact(view->dispatch);
    // 更新flatview的哈希表 flat_views
    g_hash_table_replace(flat_views, mr, view);

    return view;
}

static FlatView *flatview_new(MemoryRegion *mr_root)
{
    FlatView *view;
    // 新分配一个FlatView并初始化
    view = g_new0(FlatView, 1);
    // 引用计数
    view->ref = 1;
    // 该FlatView的root指向对应的MR
    view->root = mr_root;
    memory_region_ref(mr_root);
    trace_flatview_new(view, mr_root);
    return view;
}
```

调用`render_memory_region()`:

- 函数的参数为 `FlatView *view`, 代表线性空间视图,维护**MemoryRegion**和 **线性地址**的关系(用于**gpa→hva**的转换)

- `MemoryRegion *mr` 则为要渲染的MemoryRegion
- base 标示该MemoryRegion对应最根层MemoryRegion的偏移(最根层的偏移一般为0 ,现在已知的就 system_memory 和 system_io两个地址空间, 分别代表内存空间和io空间), 这里为0, GPA
- clip 表示**父空间的地址范围**, 子空间的地址范围是不允许落在父空间的范围外的, 如第一次render时clip代表了整个虚拟机物理地址空间

```cpp
(AddrRange){0, 2^64};
```

```cpp
/* Render a memory region into the global view.  Ranges in @view obscure
 * ranges in @mr.
 */
static void render_memory_region(FlatView *view,
                                 MemoryRegion *mr,
                                 Int128 base,
                                 AddrRange clip,
                                 bool readonly,
                                 bool nonvolatile)
{
    MemoryRegion *subregion;
    unsigned i;
    // 在region中的偏移
    hwaddr offset_in_region;
    // 待展开的内存的长度
    Int128 remain;
    // 本次展开的长度或跳过的长度
    Int128 now;
    FlatRange fr;
    AddrRange tmp;

    if (!mr->enabled) {
        return;
    }
    // base
    // mr的addr什么时候初始化的?
    // base改为MR的起始地址, addr实际上是subregion在父mr中的偏移, base = base + mr->addr
    int128_addto(&base, int128_make64(mr->addr));
    //它的读写模式就是mr的读写模式
    readonly |= mr->readonly;
    nonvolatile |= mr->nonvolatile;
    // 返回base以及大小即start&size, {0, 2^64}
    // 当前mr的地址空间范围
    tmp = addrrange_make(base, mr->size);
    //计算结果true因此不执行这里。继续向下执行
    // 判断目标MR与clip是否有交叉, 即MR应该落在clip的范围内
    if (!addrrange_intersects(tmp, clip)) {
        return;
    }
    // 缩小clip到交叉的部分
    //结果为start=0，size=0xffffffffffffffffui64
    clip = addrrange_intersection(tmp, clip);
    //对别名空间进行渲染
    // 子区域才有alias
    if (mr->alias) {
        int128_subfrom(&base, int128_make64(mr->alias->addr));
        int128_subfrom(&base, int128_make64(mr->alias_offset));
        //以上两行的计算得出别名空间的大小, 然后渲染别名空间
        render_memory_region(view, mr->alias, base, clip,
                             readonly, nonvolatile);
        return;
    }
    // //渲染子region
    /* Render subregions in priority order. */
    QTAILQ_FOREACH(subregion, &mr->subregions, subregions_link) {
        render_memory_region(view, subregion, base, clip,
                             readonly, nonvolatile);
    }
    //teminates为false表示该容器为纯容器只需要渲染不需要生成flatrange
    if (!mr->terminates) {
        return;
    }
    //计算Mr在线性空间的大小和偏移, 返回结果为0
    // clip.start表示地址区间的起始, base为本次映射的基址, 差值就是offset
    offset_in_region = int128_get64(int128_sub(clip.start, base));
    // 最初base为0
    // 开始映射时, clip表示映射的区间范围, base作为一个移动指导每个FR的映射, remain表示clip还没映射的大小
    base = clip.start;
    // 为0xffffffffffffffffui64
    remain = clip.size;
    //Flatrange成员的初始化
    fr.mr = mr;
    fr.dirty_log_mask = memory_region_get_dirty_log_mask(mr);
    fr.romd_mode = mr->romd_mode;
    fr.readonly = readonly;
    fr.nonvolatile = nonvolatile;
    // 遍历FlatView中的FlatRanges, flatview的nr什么时候初始化？
    /* Render the region itself into any gaps left by the current view. */
    for (i = 0; i < view->nr && int128_nz(remain); ++i) {
        // 将flatrange结束地址小于mr开始地址的FlatRange排除掉
        if (int128_ge(base, addrrange_end(view->ranges[i].addr))) {
            continue;
        }
        // FlatRange开始地址大于mr开始地址
        /*如果base < = view->ranges[i].addr.start*/
        if (int128_lt(base, view->ranges[i].addr.start)) {
            /*now 表示已经存在的FR 或者本次填充的FR的长度*/
            //找出两个中小的赋值给now, min(remain, flatrange.start - base)
            now = int128_min(remain,
                             int128_sub(view->ranges[i].addr.start, base));
            // flatrange在mr中的偏移, 第一次是0
            /*fr.offset_in_region表示在整个地址空间中的偏移*/
            fr.offset_in_region = offset_in_region;
            //完成fr的addrfange的赋值, base到min(remin, start)
            fr.addr = addrrange_make(base, now);
            // 将flatrange插入flatview
            flatview_insert(view, i, &fr);
            ++i;
            // base = base + now
            int128_addto(&base, now);
            // offset_in_region = offset_in_region + now
            offset_in_region += int128_get64(now);
            // remain = remain - now
            int128_subfrom(&remain, now);
        }
        // now = min(base + remain, flatrange结束地址) - base
        now = int128_sub(int128_min(int128_add(base, remain),
                                    addrrange_end(view->ranges[i].addr)),
                         base);
        // base = base + now
        int128_addto(&base, now);
        // offset_in_region = offset_in_region + now
        offset_in_region += int128_get64(now);
        // remain = remain - now
        int128_subfrom(&remain, now);
    }
    // remain != 0
    /*如果还有剩下的clip没有映射，则下面不会在发生冲突，直接一次性的映射完成*/
    if (int128_nz(remain)) {
        fr.offset_in_region = offset_in_region;
        // 返回AddressRange{base, remain}
        fr.addr = addrrange_make(base, remain);
        flatview_insert(view, i, &fr);
    }
}
```

在此必须清楚MR和clip的意义，即该函数实现的是把**MR的区域**填充**clip空间**。填充的方式就是**生成一个个FlatRange**，并交由FlatView管理。基本管理逻辑理论上如图所示

![2020-02-28-23-26-43.png](./images/2020-02-28-23-26-43.png)

棕色部分意味着已经存在的映射，这种情况下只需要把R1和R2映射进来即可。最终FlatView中的FlatRange按照在物理地址空间的布局，依次排列。但是按照实际情况来讲，实际上传递进来的MR是整个地址空间system_space,所以像图中那样比较复杂的格局应该基本不会出现。不过咱们还是根据代码来看。首先获取了当前MR的区间范围，以base为起点。我们目的是要把MR的区间映射进clip中，所以如果两者没有交叉，那么无法完成映射。接着设置clip为二者地址区间重叠的部分，以图中所示，clip就成了clip1所标的范围。如果当前MR是某个subregion,则需要对其原始的MR进行展开，因为具体的信息都保存在原始的MR中。但是全局MR system_memory作为参数传递进来，那么这里mr->alias为NULL，所以到了下面对system_memory的每个subregion均进行展开。就这样再次进入render_memory_region函数的时候，MR为某个subregion，clip也为subregion对应的区域和原始clip的交集，由于其mr->alias指向原始MR，进入if判断，对原始MR的对应区间进行展开，再次调用render_memory_region。这一次就要进行真正的展开操作了，即生成对应的FlatRange。

顺着函数往下走，涉及几个变量这里先介绍下，offset_in_region为对应MR在全局地址空间中的偏移，base为一个移动指针，指向当前映射的小区间，指导每个FR的映射。now当前已经映射的FR的长度，有两种可能，第一可能是当前映射的FR，第二可能是已经映射的FR。remain表示当前clip中剩下的未映射的部分（不考虑已经存在的FR），有了这些再看下面的代码就不吃力了。

核心的工作起始于一个for循环，循环的条件是 view->nr && int128_nz(remain)，表示当前还有未遍历的FR并且remain还有剩余。循环中如果MR base的值大于或者等于当前FR的end,则继续往后遍历FR，否则进入下面，如果base小于当前FR的start，则表明base到start之间的区间还没有映射，则为其进行映射，now表示要映射的长度，取remain和int128_sub(view->ranges[i].addr.start, base)之间的最小值，后者表示下一个FR的start和base之间的差值，当然按照clip为准。接下来就没难度了，设置FR的offset_in_region和addr,然后调用flatview_insert插入到FlatView的FlatRange数组中。不过由于FR按照地址顺序排列，如果插入位置靠前，则需要移动较多的项，不知道为何不用链表实现。下面就很自然了移动base，增加offset_in_region，减少remain等操作。出了if,此时base已经和FR的start对齐，所以还需要略过当前FR。就这么一直映射下去。

出了for循环，如果remain不为0，则表明还有没有映射的，但是现在已经没有已经存在的FR了，所以不会发生冲突，直接把remain直接映射成一个FR即可。

按照这个思路，吧所有的subregion都映射下去，最终把FlatView返回。

qemu要把**MemoryRegion**映射到cpu看到的**线性地址上**(**AddressSpace**), 但是有些情况**两个设备**的内存映射到**相同的线性地址**, 比如mmio. 举个例子,对于一个**内存条是4g**, 它的线性地址为**0-4g**, 但是**vga设备**的内存地址通过**mmio映射**到**线性地址**的`768k–(768+256k)`, 这种情况应该使vga的线性地址覆盖掉ram的`768k–(768+256k)`地址,这样写vga地址才能成功.

**FlatRang**则表示一个MemoryRegion映射到线性空间的一段地址.

qemu内存建模是通过**优先级**来实现**内存地址的覆盖**, 优先级高的MemoryRegion 先被转换为 **FlatRang**, 后面渲染的时候**低优先级**的FlatRang**不覆盖高优先级**的FlatRang, **低优先级**的**FlatRang**如果被**高优先级**的FlatRang**截断**则**只保存**不被高优先级遮盖的部分到flatview, 这是`render_memory_region` 的677行到704行的主要工作

MemoryRegion 的terminates 变量如果为false表示它是一个**纯容器**, 纯容器自身**没有线性地址**(只规定一个偏移和大小来限制子区域线性地址的取值), 通过子region来生成线性地址, 所以纯容器所规定的线性地址空间可能留下空洞,会被其他低优先级的MemoryRegion所填充MemoryRegion的addr 变量表示相对于父容器的偏移, 和大小,FlatRange的offset_in_region 变量表示该FlatRange相对所在MemoryRegion的位置(因为一个MemoryRegion可能被高优先级的MemoryRegion截断成多段,所以offset_in_region不一定为0)

fr.addr 标示该FlatRange在线性地址空间的偏移

FlatRange在FlatView中是按照地址升序来排列的, 另外不存在地址重合的FlatRange.

后面把结束地址小于MemoryRegion开始地址的FlatRange都排除掉,因为他们的地址和我们要生成的地址有冲突

剩下的地址分为如下四种情况

![2020-03-11-11-18-13.png](./images/2020-03-11-11-18-13.png)

base 表示要生成的地址空间的开始 remin表示要生成地址空间的结尾

start表示下一个要比较的地址空间的开始, end表示下一个要比较地址空间的结尾

for循环中处理的就是图中的1, 2, 4.

先把base到 min(remin, start) 这段地址生成一个FlatRange(不和其他地址重合)

将剩余部分地址裁剪出来, 其实还有一部分FlatRange可能要生成新的FlatRange(end到remin)

如果是alias还要展开自己，这个说法是不对的。事实上，alias mr和它指向的mr的“gpa”(base+addr)并不一定一致，如果都展开，映射就重复了。

下面是info mtree 打印出来的虚拟机address-space及 memory-region(被alias mr指向的mr)

```
address-space: memory
  0000000000000000-ffffffffffffffff (prio 0, RW): system
    0000000000000000-00000000bfffffff (prio 0, RW): alias ram-below-4g @pc.ram 0000000000000000-00000000bfffffff
    0000000000000000-ffffffffffffffff (prio -1, RW): pci
      00000000000a0000-00000000000bffff (prio 1, RW): cirrus-lowmem-container
        00000000000a0000-00000000000a7fff (prio 1, RW): alias vga.bank0 @vga.vram 0000000000000000-0000000000007fff
        00000000000a0000-00000000000bffff (prio 0, RW): cirrus-low-memory
        00000000000a8000-00000000000affff (prio 1, RW): alias vga.bank1 @vga.vram 0000000000008000-000000000000ffff
      00000000000c0000-00000000000dffff (prio 1, RW): pc.rom
      00000000000e0000-00000000000fffff (prio 1, R-): alias isa-bios @pc.bios 0000000000020000-000000000003ffff
      00000000fc000000-00000000fdffffff (prio 1, RW): cirrus-pci-bar0
        00000000fc000000-00000000fc7fffff (prio 1, RW): vga.vram
        00000000fc000000-00000000fc7fffff (prio 0, RW): cirrus-linear-io
        00000000fd000000-00000000fd3fffff (prio 0, RW): cirrus-bitblt-mmio
      00000000febf0000-00000000febf0fff (prio 1, RW): cirrus-mmio
      00000000febf1000-00000000febf1fff (prio 1, RW): virtio-scsi-pci-msix
        00000000febf1000-00000000febf103f (prio 0, RW): msix-table
        00000000febf1800-00000000febf1807 (prio 0, RW): msix-pba
      00000000febf2000-00000000febf2fff (prio 1, RW): virtio-serial-pci-msix
        00000000febf2000-00000000febf201f (prio 0, RW): msix-table
        00000000febf2800-00000000febf2807 (prio 0, RW): msix-pba
      00000000fffc0000-00000000ffffffff (prio 0, R-): pc.bios
    00000000000a0000-00000000000bffff (prio 1, RW): alias smram-region @pci 00000000000a0000-00000000000bffff
    00000000000c0000-00000000000c3fff (prio 1, RW): alias pam-ram @pc.ram 00000000000c0000-00000000000c3fff [disabled]
    00000000000c0000-00000000000c3fff (prio 1, RW): alias pam-pci @pc.ram 00000000000c0000-00000000000c3fff [disabled]
    00000000000c0000-00000000000c3fff (prio 1, R-): alias pam-rom @pc.ram 00000000000c0000-00000000000c3fff
    00000000000c0000-00000000000c3fff (prio 1, RW): alias pam-pci @pci 00000000000c0000-00000000000c3fff [disabled]
    00000000000c4000-00000000000c7fff (prio 1, RW): alias pam-ram @pc.ram 00000000000c4000-00000000000c7fff [disabled]
    00000000000c4000-00000000000c7fff (prio 1, RW): alias pam-pci @pc.ram 00000000000c4000-00000000000c7fff [disabled]
    00000000000c4000-00000000000c7fff (prio 1, R-): alias pam-rom @pc.ram 00000000000c4000-00000000000c7fff
    00000000000c4000-00000000000c7fff (prio 1, RW): alias pam-pci @pci 00000000000c4000-00000000000c7fff [disabled]
    00000000000c8000-00000000000cbfff (prio 1, RW): alias pam-ram @pc.ram 00000000000c8000-00000000000cbfff [disabled]
    00000000000c8000-00000000000cbfff (prio 1, RW): alias pam-pci @pc.ram 00000000000c8000-00000000000cbfff [disabled]
    00000000000c8000-00000000000cbfff (prio 1, R-): alias pam-rom @pc.ram 00000000000c8000-00000000000cbfff
    00000000000c8000-00000000000cbfff (prio 1, RW): alias pam-pci @pci 00000000000c8000-00000000000cbfff [disabled]
    00000000000c9000-00000000000cbfff (prio 1000, RW): alias kvmvapic-rom @pc.ram 00000000000c9000-00000000000cbfff
    00000000000cc000-00000000000cffff (prio 1, RW): alias pam-ram @pc.ram 00000000000cc000-00000000000cffff [disabled]
    00000000000cc000-00000000000cffff (prio 1, RW): alias pam-pci @pc.ram 00000000000cc000-00000000000cffff [disabled]
    00000000000cc000-00000000000cffff (prio 1, R-): alias pam-rom @pc.ram 00000000000cc000-00000000000cffff
    00000000000cc000-00000000000cffff (prio 1, RW): alias pam-pci @pci 00000000000cc000-00000000000cffff [disabled]
    00000000000d0000-00000000000d3fff (prio 1, RW): alias pam-ram @pc.ram 00000000000d0000-00000000000d3fff [disabled]
    00000000000d0000-00000000000d3fff (prio 1, RW): alias pam-pci @pc.ram 00000000000d0000-00000000000d3fff [disabled]
    00000000000d0000-00000000000d3fff (prio 1, R-): alias pam-rom @pc.ram 00000000000d0000-00000000000d3fff
    00000000000d0000-00000000000d3fff (prio 1, RW): alias pam-pci @pci 00000000000d0000-00000000000d3fff [disabled]
    00000000000d4000-00000000000d7fff (prio 1, RW): alias pam-ram @pc.ram 00000000000d4000-00000000000d7fff [disabled]
    00000000000d4000-00000000000d7fff (prio 1, RW): alias pam-pci @pc.ram 00000000000d4000-00000000000d7fff [disabled]
    00000000000d4000-00000000000d7fff (prio 1, R-): alias pam-rom @pc.ram 00000000000d4000-00000000000d7fff
    00000000000d4000-00000000000d7fff (prio 1, RW): alias pam-pci @pci 00000000000d4000-00000000000d7fff [disabled]
    00000000000d8000-00000000000dbfff (prio 1, RW): alias pam-ram @pc.ram 00000000000d8000-00000000000dbfff [disabled]
    00000000000d8000-00000000000dbfff (prio 1, RW): alias pam-pci @pc.ram 00000000000d8000-00000000000dbfff [disabled]
    00000000000d8000-00000000000dbfff (prio 1, R-): alias pam-rom @pc.ram 00000000000d8000-00000000000dbfff
    00000000000d8000-00000000000dbfff (prio 1, RW): alias pam-pci @pci 00000000000d8000-00000000000dbfff [disabled]
    00000000000dc000-00000000000dffff (prio 1, RW): alias pam-ram @pc.ram 00000000000dc000-00000000000dffff [disabled]
    00000000000dc000-00000000000dffff (prio 1, RW): alias pam-pci @pc.ram 00000000000dc000-00000000000dffff [disabled]
    00000000000dc000-00000000000dffff (prio 1, R-): alias pam-rom @pc.ram 00000000000dc000-00000000000dffff
    00000000000dc000-00000000000dffff (prio 1, RW): alias pam-pci @pci 00000000000dc000-00000000000dffff [disabled]
    00000000000e0000-00000000000e3fff (prio 1, RW): alias pam-ram @pc.ram 00000000000e0000-00000000000e3fff [disabled]
    00000000000e0000-00000000000e3fff (prio 1, RW): alias pam-pci @pc.ram 00000000000e0000-00000000000e3fff [disabled]
    00000000000e0000-00000000000e3fff (prio 1, R-): alias pam-rom @pc.ram 00000000000e0000-00000000000e3fff
    00000000000e0000-00000000000e3fff (prio 1, RW): alias pam-pci @pci 00000000000e0000-00000000000e3fff [disabled]
    00000000000e4000-00000000000e7fff (prio 1, RW): alias pam-ram @pc.ram 00000000000e4000-00000000000e7fff [disabled]
    00000000000e4000-00000000000e7fff (prio 1, RW): alias pam-pci @pc.ram 00000000000e4000-00000000000e7fff [disabled]
    00000000000e4000-00000000000e7fff (prio 1, R-): alias pam-rom @pc.ram 00000000000e4000-00000000000e7fff
    00000000000e4000-00000000000e7fff (prio 1, RW): alias pam-pci @pci 00000000000e4000-00000000000e7fff [disabled]
    00000000000e8000-00000000000ebfff (prio 1, RW): alias pam-ram @pc.ram 00000000000e8000-00000000000ebfff [disabled]
    00000000000e8000-00000000000ebfff (prio 1, RW): alias pam-pci @pc.ram 00000000000e8000-00000000000ebfff [disabled]
    00000000000e8000-00000000000ebfff (prio 1, R-): alias pam-rom @pc.ram 00000000000e8000-00000000000ebfff
    00000000000e8000-00000000000ebfff (prio 1, RW): alias pam-pci @pci 00000000000e8000-00000000000ebfff [disabled]
    00000000000ec000-00000000000effff (prio 1, RW): alias pam-ram @pc.ram 00000000000ec000-00000000000effff
    00000000000ec000-00000000000effff (prio 1, RW): alias pam-pci @pc.ram 00000000000ec000-00000000000effff [disabled]
    00000000000ec000-00000000000effff (prio 1, R-): alias pam-rom @pc.ram 00000000000ec000-00000000000effff [disabled]
    00000000000ec000-00000000000effff (prio 1, RW): alias pam-pci @pci 00000000000ec000-00000000000effff [disabled]
    00000000000f0000-00000000000fffff (prio 1, RW): alias pam-ram @pc.ram 00000000000f0000-00000000000fffff [disabled]
    00000000000f0000-00000000000fffff (prio 1, RW): alias pam-pci @pc.ram 00000000000f0000-00000000000fffff [disabled]
    00000000000f0000-00000000000fffff (prio 1, R-): alias pam-rom @pc.ram 00000000000f0000-00000000000fffff
    00000000000f0000-00000000000fffff (prio 1, RW): alias pam-pci @pci 00000000000f0000-00000000000fffff [disabled]
    00000000fec00000-00000000fec00fff (prio 0, RW): kvm-ioapic
    00000000fee00000-00000000feefffff (prio 4096, RW): kvm-apic-msi
    0000000100000000-000000013fffffff (prio 0, RW): alias ram-above-4g @pc.ram 00000000c0000000-00000000ffffffff

address-space: I/O
  0000000000000000-000000000000ffff (prio 0, RW): io
    0000000000000000-0000000000000007 (prio 0, RW): dma-chan
    0000000000000008-000000000000000f (prio 0, RW): dma-cont
    0000000000000020-0000000000000021 (prio 0, RW): kvm-pic
    0000000000000040-0000000000000043 (prio 0, RW): kvm-pit
    0000000000000060-0000000000000060 (prio 0, RW): i8042-data
    0000000000000061-0000000000000061 (prio 0, RW): pcspk
    0000000000000064-0000000000000064 (prio 0, RW): i8042-cmd
    0000000000000070-0000000000000071 (prio 0, RW): rtc
    000000000000007e-000000000000007f (prio 0, RW): kvmvapic
    0000000000000080-0000000000000080 (prio 0, RW): ioport80
    0000000000000081-0000000000000083 (prio 0, RW): dma-page
    0000000000000087-0000000000000087 (prio 0, RW): dma-page
    0000000000000089-000000000000008b (prio 0, RW): dma-page
    000000000000008f-000000000000008f (prio 0, RW): dma-page
    0000000000000092-0000000000000092 (prio 0, RW): port92
    00000000000000a0-00000000000000a1 (prio 0, RW): kvm-pic
    00000000000000b2-00000000000000b3 (prio 0, RW): apm-io
    00000000000000c0-00000000000000cf (prio 0, RW): dma-chan
    00000000000000d0-00000000000000df (prio 0, RW): dma-cont
    00000000000000f0-00000000000000f0 (prio 0, RW): ioportF0
    0000000000000170-0000000000000177 (prio 0, RW): ide
    00000000000001f0-00000000000001f7 (prio 0, RW): ide
    0000000000000376-0000000000000376 (prio 0, RW): ide
    00000000000003b0-00000000000003df (prio 0, RW): cirrus-io
    00000000000003f1-00000000000003f5 (prio 0, RW): fdc
    00000000000003f6-00000000000003f6 (prio 0, RW): ide
    00000000000003f7-00000000000003f7 (prio 0, RW): fdc
    00000000000003f8-00000000000003ff (prio 0, RW): serial
    00000000000004d0-00000000000004d0 (prio 0, RW): kvm-elcr
    00000000000004d1-00000000000004d1 (prio 0, RW): kvm-elcr
    0000000000000510-0000000000000511 (prio 0, RW): fwcfg
    0000000000000514-000000000000051b (prio 0, RW): fwcfg.dma
    0000000000000600-000000000000063f (prio 0, RW): piix4-pm
      0000000000000600-0000000000000603 (prio 0, RW): acpi-evt
      0000000000000604-0000000000000605 (prio 0, RW): acpi-cnt
      0000000000000608-000000000000060b (prio 0, RW): acpi-tmr
    0000000000000700-000000000000073f (prio 0, RW): pm-smbus
    0000000000000cf8-0000000000000cfb (prio 0, RW): pci-conf-idx
    0000000000000cf9-0000000000000cf9 (prio 1, RW): piix3-reset-control
    0000000000000cfc-0000000000000cff (prio 0, RW): pci-conf-data
    0000000000005658-0000000000005658 (prio 0, RW): vmport
    000000000000ae00-000000000000ae13 (prio 0, RW): acpi-pci-hotplug
    000000000000af00-000000000000af1f (prio 0, RW): acpi-cpu-hotplug
    000000000000afe0-000000000000afe3 (prio 0, RW): acpi-gpe0
    000000000000c000-000000000000c0ff (prio 1, RW): pv_channel
    000000000000c100-000000000000c13f (prio 1, RW): virtio-pci
    000000000000c140-000000000000c15f (prio 1, RW): uhci
    000000000000c160-000000000000c17f (prio 1, RW): virtio-pci
    000000000000c180-000000000000c19f (prio 1, RW): virtio-pci
    000000000000c1a0-000000000000c1af (prio 1, RW): piix-bmdma-container
      000000000000c1a0-000000000000c1a3 (prio 0, RW): piix-bmdma
      000000000000c1a4-000000000000c1a7 (prio 0, RW): bmdma
      000000000000c1a8-000000000000c1ab (prio 0, RW): piix-bmdma
      000000000000c1ac-000000000000c1af (prio 0, RW): bmdma

address-space: i440FX
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff [disabled]

address-space: PIIX3
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff [disabled]

address-space: piix3-ide
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff

address-space: PIIX4_PM
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff [disabled]

address-space: piix3-usb-uhci
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff

address-space: virtio-scsi-pci
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff

address-space: virtio-pci-cfg-as
  0000000000000000-00000000007fffff (prio 0, RW): alias virtio-pci-cfg @virtio-pci 0000000000000000-00000000007fffff

address-space: virtio-serial-pci
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff

address-space: virtio-pci-cfg-as
  0000000000000000-00000000007fffff (prio 0, RW): alias virtio-pci-cfg @virtio-pci 0000000000000000-00000000007fffff

address-space: cirrus-vga
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff [disabled]

address-space: virtio-balloon-pci
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff

address-space: virtio-pci-cfg-as
  0000000000000000-00000000007fffff (prio 0, RW): alias virtio-pci-cfg @virtio-pci 0000000000000000-00000000007fffff

address-space: pv_channel
  0000000000000000-ffffffffffffffff (prio 0, RW): alias bus master @system 0000000000000000-ffffffffffffffff [disabled]

address-space: KVM-SMRAM
  0000000000000000-ffffffffffffffff (prio 0, RW): mem-container-smram
    0000000000000000-00000000ffffffff (prio 10, RW): smram
      00000000000a0000-00000000000bffff (prio 0, RW): alias smram-low @pc.ram 00000000000a0000-00000000000bffff
    0000000000000000-ffffffffffffffff (prio 0, RW): alias mem-smram @system 0000000000000000-ffffffffffffffff

memory-region: pc.ram
  0000000000000000-00000000ffffffff (prio 0, RW): pc.ram

memory-region: vga.vram
  0000000000000000-00000000007fffff (prio 1, RW): vga.vram

memory-region: pc.bios
  00000000fffc0000-00000000ffffffff (prio 0, R-): pc.bios

memory-region: pci
  0000000000000000-ffffffffffffffff (prio -1, RW): pci
    00000000000a0000-00000000000bffff (prio 1, RW): cirrus-lowmem-container
      00000000000a0000-00000000000a7fff (prio 1, RW): alias vga.bank0 @vga.vram 0000000000000000-0000000000007fff
      00000000000a0000-00000000000bffff (prio 0, RW): cirrus-low-memory
      00000000000a8000-00000000000affff (prio 1, RW): alias vga.bank1 @vga.vram 0000000000008000-000000000000ffff
    00000000000c0000-00000000000dffff (prio 1, RW): pc.rom
    00000000000e0000-00000000000fffff (prio 1, R-): alias isa-bios @pc.bios 0000000000020000-000000000003ffff
    00000000fc000000-00000000fdffffff (prio 1, RW): cirrus-pci-bar0
      00000000fc000000-00000000fc7fffff (prio 1, RW): vga.vram
      00000000fc000000-00000000fc7fffff (prio 0, RW): cirrus-linear-io
      00000000fd000000-00000000fd3fffff (prio 0, RW): cirrus-bitblt-mmio
    00000000febf0000-00000000febf0fff (prio 1, RW): cirrus-mmio
    00000000febf1000-00000000febf1fff (prio 1, RW): virtio-scsi-pci-msix
      00000000febf1000-00000000febf103f (prio 0, RW): msix-table
      00000000febf1800-00000000febf1807 (prio 0, RW): msix-pba
    00000000febf2000-00000000febf2fff (prio 1, RW): virtio-serial-pci-msix
      00000000febf2000-00000000febf201f (prio 0, RW): msix-table
      00000000febf2800-00000000febf2807 (prio 0, RW): msix-pba
    00000000fffc0000-00000000ffffffff (prio 0, R-): pc.bios

memory-region: system
  0000000000000000-ffffffffffffffff (prio 0, RW): system
    0000000000000000-00000000bfffffff (prio 0, RW): alias ram-below-4g @pc.ram 0000000000000000-00000000bfffffff
    0000000000000000-ffffffffffffffff (prio -1, RW): pci
      00000000000a0000-00000000000bffff (prio 1, RW): cirrus-lowmem-container
        00000000000a0000-00000000000a7fff (prio 1, RW): alias vga.bank0 @vga.vram 0000000000000000-0000000000007fff
        00000000000a0000-00000000000bffff (prio 0, RW): cirrus-low-memory
        00000000000a8000-00000000000affff (prio 1, RW): alias vga.bank1 @vga.vram 0000000000008000-000000000000ffff
      00000000000c0000-00000000000dffff (prio 1, RW): pc.rom
      00000000000e0000-00000000000fffff (prio 1, R-): alias isa-bios @pc.bios 0000000000020000-000000000003ffff
      00000000fc000000-00000000fdffffff (prio 1, RW): cirrus-pci-bar0
        00000000fc000000-00000000fc7fffff (prio 1, RW): vga.vram
        00000000fc000000-00000000fc7fffff (prio 0, RW): cirrus-linear-io
        00000000fd000000-00000000fd3fffff (prio 0, RW): cirrus-bitblt-mmio
      00000000febf0000-00000000febf0fff (prio 1, RW): cirrus-mmio
      00000000febf1000-00000000febf1fff (prio 1, RW): virtio-scsi-pci-msix
        00000000febf1000-00000000febf103f (prio 0, RW): msix-table
        00000000febf1800-00000000febf1807 (prio 0, RW): msix-pba
      00000000febf2000-00000000febf2fff (prio 1, RW): virtio-serial-pci-msix
        00000000febf2000-00000000febf201f (prio 0, RW): msix-table
        00000000febf2800-00000000febf2807 (prio 0, RW): msix-pba
      00000000fffc0000-00000000ffffffff (prio 0, R-): pc.bios
    00000000000a0000-00000000000bffff (prio 1, RW): alias smram-region @pci 00000000000a0000-00000000000bffff
    00000000000c0000-00000000000c3fff (prio 1, RW): alias pam-ram @pc.ram 00000000000c0000-00000000000c3fff [disabled]
    00000000000c0000-00000000000c3fff (prio 1, RW): alias pam-pci @pc.ram 00000000000c0000-00000000000c3fff [disabled]
    00000000000c0000-00000000000c3fff (prio 1, R-): alias pam-rom @pc.ram 00000000000c0000-00000000000c3fff
    00000000000c0000-00000000000c3fff (prio 1, RW): alias pam-pci @pci 00000000000c0000-00000000000c3fff [disabled]
    00000000000c4000-00000000000c7fff (prio 1, RW): alias pam-ram @pc.ram 00000000000c4000-00000000000c7fff [disabled]
    00000000000c4000-00000000000c7fff (prio 1, RW): alias pam-pci @pc.ram 00000000000c4000-00000000000c7fff [disabled]
    00000000000c4000-00000000000c7fff (prio 1, R-): alias pam-rom @pc.ram 00000000000c4000-00000000000c7fff
    00000000000c4000-00000000000c7fff (prio 1, RW): alias pam-pci @pci 00000000000c4000-00000000000c7fff [disabled]
    00000000000c8000-00000000000cbfff (prio 1, RW): alias pam-ram @pc.ram 00000000000c8000-00000000000cbfff [disabled]
    00000000000c8000-00000000000cbfff (prio 1, RW): alias pam-pci @pc.ram 00000000000c8000-00000000000cbfff [disabled]
    00000000000c8000-00000000000cbfff (prio 1, R-): alias pam-rom @pc.ram 00000000000c8000-00000000000cbfff
    00000000000c8000-00000000000cbfff (prio 1, RW): alias pam-pci @pci 00000000000c8000-00000000000cbfff [disabled]
    00000000000c9000-00000000000cbfff (prio 1000, RW): alias kvmvapic-rom @pc.ram 00000000000c9000-00000000000cbfff
    00000000000cc000-00000000000cffff (prio 1, RW): alias pam-ram @pc.ram 00000000000cc000-00000000000cffff [disabled]
    00000000000cc000-00000000000cffff (prio 1, RW): alias pam-pci @pc.ram 00000000000cc000-00000000000cffff [disabled]
    00000000000cc000-00000000000cffff (prio 1, R-): alias pam-rom @pc.ram 00000000000cc000-00000000000cffff
    00000000000cc000-00000000000cffff (prio 1, RW): alias pam-pci @pci 00000000000cc000-00000000000cffff [disabled]
    00000000000d0000-00000000000d3fff (prio 1, RW): alias pam-ram @pc.ram 00000000000d0000-00000000000d3fff [disabled]
    00000000000d0000-00000000000d3fff (prio 1, RW): alias pam-pci @pc.ram 00000000000d0000-00000000000d3fff [disabled]
    00000000000d0000-00000000000d3fff (prio 1, R-): alias pam-rom @pc.ram 00000000000d0000-00000000000d3fff
    00000000000d0000-00000000000d3fff (prio 1, RW): alias pam-pci @pci 00000000000d0000-00000000000d3fff [disabled]
    00000000000d4000-00000000000d7fff (prio 1, RW): alias pam-ram @pc.ram 00000000000d4000-00000000000d7fff [disabled]
    00000000000d4000-00000000000d7fff (prio 1, RW): alias pam-pci @pc.ram 00000000000d4000-00000000000d7fff [disabled]
    00000000000d4000-00000000000d7fff (prio 1, R-): alias pam-rom @pc.ram 00000000000d4000-00000000000d7fff
    00000000000d4000-00000000000d7fff (prio 1, RW): alias pam-pci @pci 00000000000d4000-00000000000d7fff [disabled]
    00000000000d8000-00000000000dbfff (prio 1, RW): alias pam-ram @pc.ram 00000000000d8000-00000000000dbfff [disabled]
    00000000000d8000-00000000000dbfff (prio 1, RW): alias pam-pci @pc.ram 00000000000d8000-00000000000dbfff [disabled]
    00000000000d8000-00000000000dbfff (prio 1, R-): alias pam-rom @pc.ram 00000000000d8000-00000000000dbfff
    00000000000d8000-00000000000dbfff (prio 1, RW): alias pam-pci @pci 00000000000d8000-00000000000dbfff [disabled]
    00000000000dc000-00000000000dffff (prio 1, RW): alias pam-ram @pc.ram 00000000000dc000-00000000000dffff [disabled]
    00000000000dc000-00000000000dffff (prio 1, RW): alias pam-pci @pc.ram 00000000000dc000-00000000000dffff [disabled]
    00000000000dc000-00000000000dffff (prio 1, R-): alias pam-rom @pc.ram 00000000000dc000-00000000000dffff
    00000000000dc000-00000000000dffff (prio 1, RW): alias pam-pci @pci 00000000000dc000-00000000000dffff [disabled]
    00000000000e0000-00000000000e3fff (prio 1, RW): alias pam-ram @pc.ram 00000000000e0000-00000000000e3fff [disabled]
    00000000000e0000-00000000000e3fff (prio 1, RW): alias pam-pci @pc.ram 00000000000e0000-00000000000e3fff [disabled]
    00000000000e0000-00000000000e3fff (prio 1, R-): alias pam-rom @pc.ram 00000000000e0000-00000000000e3fff
    00000000000e0000-00000000000e3fff (prio 1, RW): alias pam-pci @pci 00000000000e0000-00000000000e3fff [disabled]
    00000000000e4000-00000000000e7fff (prio 1, RW): alias pam-ram @pc.ram 00000000000e4000-00000000000e7fff [disabled]
    00000000000e4000-00000000000e7fff (prio 1, RW): alias pam-pci @pc.ram 00000000000e4000-00000000000e7fff [disabled]
    00000000000e4000-00000000000e7fff (prio 1, R-): alias pam-rom @pc.ram 00000000000e4000-00000000000e7fff
    00000000000e4000-00000000000e7fff (prio 1, RW): alias pam-pci @pci 00000000000e4000-00000000000e7fff [disabled]
    00000000000e8000-00000000000ebfff (prio 1, RW): alias pam-ram @pc.ram 00000000000e8000-00000000000ebfff [disabled]
    00000000000e8000-00000000000ebfff (prio 1, RW): alias pam-pci @pc.ram 00000000000e8000-00000000000ebfff [disabled]
    00000000000e8000-00000000000ebfff (prio 1, R-): alias pam-rom @pc.ram 00000000000e8000-00000000000ebfff
    00000000000e8000-00000000000ebfff (prio 1, RW): alias pam-pci @pci 00000000000e8000-00000000000ebfff [disabled]
    00000000000ec000-00000000000effff (prio 1, RW): alias pam-ram @pc.ram 00000000000ec000-00000000000effff
    00000000000ec000-00000000000effff (prio 1, RW): alias pam-pci @pc.ram 00000000000ec000-00000000000effff [disabled]
    00000000000ec000-00000000000effff (prio 1, R-): alias pam-rom @pc.ram 00000000000ec000-00000000000effff [disabled]
    00000000000ec000-00000000000effff (prio 1, RW): alias pam-pci @pci 00000000000ec000-00000000000effff [disabled]
    00000000000f0000-00000000000fffff (prio 1, RW): alias pam-ram @pc.ram 00000000000f0000-00000000000fffff [disabled]
    00000000000f0000-00000000000fffff (prio 1, RW): alias pam-pci @pc.ram 00000000000f0000-00000000000fffff [disabled]
    00000000000f0000-00000000000fffff (prio 1, R-): alias pam-rom @pc.ram 00000000000f0000-00000000000fffff
    00000000000f0000-00000000000fffff (prio 1, RW): alias pam-pci @pci 00000000000f0000-00000000000fffff [disabled]
    00000000fec00000-00000000fec00fff (prio 0, RW): kvm-ioapic
    00000000fee00000-00000000feefffff (prio 4096, RW): kvm-apic-msi
    0000000100000000-000000013fffffff (prio 0, RW): alias ram-above-4g @pc.ram 00000000c0000000-00000000ffffffff

memory-region: virtio-pci
  0000000000000000-00000000007fffff (prio 0, RW): virtio-pci

memory-region: virtio-pci
  0000000000000000-00000000007fffff (prio 0, RW): virtio-pci

memory-region: virtio-pci
  0000000000000000-00000000007fffff (prio 0, RW): virtio-pci
```

#### 4.3.1.2. address_space_set_flatview(): 设置as的FlatView, 会调用KVM

```cpp
static void address_space_set_flatview(AddressSpace *as)
{
    FlatView *old_view = address_space_to_flatview(as);
    // 获取AddressSpace的root MR
    MemoryRegion *physmr = memory_region_get_flatview_root(as->root);
    // 在哈希表flat_views中根据physmr(key)查找新的new_view
    FlatView *new_view = g_hash_table_lookup(flat_views, physmr);

    assert(new_view);
    // 新旧相等, 不用更新
    if (old_view == new_view) {
        return;
    }
    // 下面逻辑是新旧flatview不等

    // 旧的引用计数 + 1
    if (old_view) {
        flatview_ref(old_view);
    }

    // 新的增加引用
    flatview_ref(new_view);

    if (!QTAILQ_EMPTY(&as->listeners)) {
        FlatView tmpview = { .nr = 0 }, *old_view2 = old_view;

        if (!old_view2) {
            old_view2 = &tmpview;
        }
        // 删除
        address_space_update_topology_pass(as, old_view2, new_view, false);
        // 新加
        address_space_update_topology_pass(as, old_view2, new_view, true);
    }

    /* Writes are protected by the BQL.  */
    /* 设置新的FlatView */
    atomic_rcu_set(&as->current_map, new_view);
    // 旧的引用计数 - 1
    if (old_view) {
        flatview_unref(old_view);
    }

    /* Note that all the old MemoryRegions are still alive up to this
     * point.  This relieves most MemoryListeners from the need to
     * ref/unref the MemoryRegions they get---unless they use them
     * outside the iothread mutex, in which case precise reference
     * counting is necessary.
     */
    if (old_view) {
        flatview_unref(old_view);
    }
}
```

在获取了**新旧两个FlatView**之后，调用了**两次**`address_space_update_topology_pass()`函数，首次调用重在**删除原来的**，而后者重在**添加**。之后设置`as->current_map = new_view`。并对`old_view`**减少引用**，当**引用计数为1**时会**被删除**。

接下来重点在两个地方：1、如何根据一个MR获取对应的FlatView；2、如何对旧的FlatView进行更新。

```cpp
static void address_space_update_topology_pass(AddressSpace *as,
                                               const FlatView *old_view,
                                               const FlatView *new_view,
                                               bool adding)
{
    /*FR计数*/
    unsigned iold, inew;
    FlatRange *frold, *frnew;

    /* Generate a symmetric difference of the old and new memory maps.
     * Kill ranges in the old map, and instantiate ranges in the new map.
     */
    iold = inew = 0;
    while (iold < old_view->nr || inew < new_view->nr) {
        if (iold < old_view->nr) {
            frold = &old_view->ranges[iold];
        } else {
            frold = NULL;
        }
        if (inew < new_view->nr) {
            frnew = &new_view->ranges[inew];
        } else {
            frnew = NULL;
        }
        /*int128_lt 小于等于*/
        /*int128_eq 等于*/
        /*frold not null and( frnew is null || frold not null and old.start <= new.start || frold not null and old.start == new.start) and old !=new*/
        if (frold
            && (!frnew
                || int128_lt(frold->addr.start, frnew->addr.start)
                || (int128_eq(frold->addr.start, frnew->addr.start)
                    && !flatrange_equal(frold, frnew)))) {
            /* In old but not in new, or in both but attributes changed. */
            /*if not add*/
            if (!adding) {
                MEMORY_LISTENER_UPDATE_REGION(frold, as, Reverse, region_del);
            }

            ++iold;
            /*if old and new and old==new*/
        } else if (frold && frnew && flatrange_equal(frold, frnew)) {
            /* In both and unchanged (except logging may have changed) */
            /*if add*/
            if (adding) {
                MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_nop);
                if (frold->dirty_log_mask && !frnew->dirty_log_mask) {
                    MEMORY_LISTENER_UPDATE_REGION(frnew, as, Reverse, log_stop);
                } else if (frnew->dirty_log_mask && !frold->dirty_log_mask) {
                    MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, log_start);
                }
            }
            ++iold;
            ++inew;
        } else {
            /* In new */
            /*if add*/
            if (adding) {
                MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_add);
            }
            ++inew;
        }
    }
}
```

该函数倒是不长，主体是一个while循环，循环条件是old_view->nr和new_view->nr,表示新旧view的可用FlatRange数目。这里依次对FR数组的对应FR 做对比，主要由下面几种情况：frold和frnew均存在、frold存在但frnew不存在，frold不存在但frnew存在。下面的if划分和上面的略有不同：

1、如果`frold不为空&&（frnew为空||frold.start<frnew.start||frold.start=frnew.start）&&frold!=frnew`    这种情况是新旧view的地址范围不一样，则需要调用lienter的region_del对frold进行删除。

2、如果`frold和frnew均不为空且frold.start=frnew.start`      这种情况需要判断日志掩码，如果frold->dirty_log_mask && !frnew->dirty_log_mask，调用log_stop回调函数；如果frnew->dirty_log_mask && !frold->dirty_log_mask，调用log_start回调函数。

3、frold为空但是frnew不为空      这种情况直接调用region_add回调函数添加region。

函数主体逻辑基本如上所述，那我们注意到，当adding为false时，执行的只有第一个情况下的处理，就是删除frold的操作，其余的处理只有在adding 为true的时候才得以执行。这意图就比较明确，首次执行先删除多余的，下次直接添加或者对日志做更新操作了。


会调用`MEMORY_LISTENER_UPDATE_REGION()`**提交本次修改**

# 5. 新FlatRange的更新

对于要添加的FR，调用了 `MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_add)`

```cpp
/* No need to ref/unref .mr, the FlatRange keeps it alive.  */
#define MEMORY_LISTENER_UPDATE_REGION(fr, as, dir, callback, _args...)  \
    do {                                                                \
        MemoryRegionSection mrs = section_from_flat_range(fr,           \
                address_space_to_flatview(as));                         \
        MEMORY_LISTENER_CALL(as, callback, dir, &mrs, ##_args);         \
    } while(0)

static inline MemoryRegionSection
section_from_flat_range(FlatRange *fr, FlatView *fv)
{
    return (MemoryRegionSection) {
        .mr = fr->mr,
        .fv = fv,
        .offset_within_region = fr->offset_in_region,
        .size = fr->addr.size,
        .offset_within_address_space = int128_get64(fr->addr.start),
        .readonly = fr->readonly,
        .nonvolatile = fr->nonvolatile,
    };
}
```

该宏实际上是另一个宏`MEMORY_LISTENER_CALL`的封装，在`MEMORY_LISTENER_CALL`中临时生成一个`MemoryRegionSection`结构，具体逻辑如下

```cpp
#define MEMORY_LISTENER_CALL(_callback, _direction, _section, _args...) \
    do {                                                                \
        MemoryListener *_listener;                                      \
                                                                        \
        switch (_direction) {                                           \
        case Forward:                                                   \
            QTAILQ_FOREACH(_listener, &memory_listeners, link) {        \
                if (_listener->_callback                                \
                    && memory_listener_match(_listener, _section)) {    \
                    _listener->_callback(_listener, _section, ##_args); \
                }                                                       \
            }                                                           \
            break;                                                      \
        case Reverse:                                                   \
            QTAILQ_FOREACH_REVERSE(_listener, &memory_listeners,        \
                                   memory_listeners, link) {            \
                if (_listener->_callback                                \
                    && memory_listener_match(_listener, _section)) {    \
                    _listener->_callback(_listener, _section, ##_args); \
                }                                                       \
            }                                                           \
            break;                                                      \
        default:                                                        \
            abort();                                                    \
        }                                                               \
    } while (0)
```

这里有个`_direction`，其实就是**遍历方向**，因为**listener**按照**优先级**从**低到高**排列，所以这里其实就是确定让谁先处理。Forward就是从前向后，而reverse就是从后向前。还有一个值得注意的是`memory_listener_match`函数，**listener**有着**对应的AddressSpace**，会在其 `address_space_filter` 域指定，注意如果**没有指定AddressSpace**，那么该listener对**所有AddressSpace均适用**，否则**只适用于其指定的AddressSpace**。知道这些，看这些代码就不成问题了。

这样`kvm_region_add()`函数得到执行，那咱们就从`kvm_region_add`函数开始，对KVM部分的内存管理做介绍。

`kvm_region_add()`函数是核心listener的添加region的函数，在qemu申请好内存后，针对**每个FR**，调用了listener的`region_add函数`。最终需要利用此函数把**region信息**告知**KVM**，KVM以此对内存信息做记录。

咱们直奔主题，函数核心在`static void kvm_set_phys_mem(MemoryRegionSection *section, bool add)`函数中，QEMU通过该函数修改虚拟机注册在内核中的内存, 达到为虚拟机添加/删除/移动内存的目的.

```cpp
static void kvm_set_phys_mem(KVMMemoryListener *kml,
                             MemoryRegionSection *section, bool add)
{
    KVMSlot *mem;
    int err;
    MemoryRegion *mr = section->mr;
    // 是否可写
    bool writeable = !mr->readonly && !mr->rom_device;
    hwaddr start_addr, size, slot_size;
    void *ram;
    // 如果不是ram, 则不能写操作
    if (!memory_region_is_ram(mr)) {
        // 不可写 或 kvm不支持只读内存, 直接返回
        if (writeable || !kvm_readonly_mem_allowed) {
            return;
        } else if (!mr->romd_mode) {
            /* If the memory device is not in romd_mode, then we actually want
             * to remove the kvm memory slot so all accesses will trap. */
            add = false;
        }
    }
    // 得到section的start address和size, 对齐后的结果
    size = kvm_align_section(section, &start_addr);
    if (!size) {
        return;
    }
    // 获取ram(HVA)地址, ??
    /* use aligned delta to align the ram address */
    ram = memory_region_get_ram_ptr(mr) + section->offset_within_region +
          (start_addr - section->offset_within_address_space);
    // 加锁
    kvm_slots_lock(kml);

    if (!add) {
        do {
            slot_size = MIN(kvm_max_slot_size, size);
            mem = kvm_lookup_matching_slot(kml, start_addr, slot_size);
            if (!mem) {
                goto out;
            }
            if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
                kvm_physical_sync_dirty_bitmap(kml, section);
            }

            /* unregister the slot */
            g_free(mem->dirty_bmap);
            mem->dirty_bmap = NULL;
            mem->memory_size = 0;
            mem->flags = 0;
            err = kvm_set_user_memory_region(kml, mem, false);
            if (err) {
                fprintf(stderr, "%s: error unregistering slot: %s\n",
                        __func__, strerror(-err));
                abort();
            }
            start_addr += slot_size;
            size -= slot_size;
        } while (size);
        goto out;
    }
    // 注册新的slot
    /* register the new slot */
    do {
        slot_size = MIN(kvm_max_slot_size, size);
        // 生成新的slot并初始化
        mem = kvm_alloc_slot(kml);
        mem->memory_size = slot_size;
        mem->start_addr = start_addr;
        mem->ram = ram;
        mem->flags = kvm_mem_flags(mr);

        if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
            /*
             * Reallocate the bmap; it means it doesn't disappear in
             * middle of a migrate.
             */
            kvm_memslot_init_dirty_bitmap(mem);
        }
        err = kvm_set_user_memory_region(kml, mem, true);
        if (err) {
            fprintf(stderr, "%s: error registering slot: %s\n", __func__,
                    strerror(-err));
            abort();
        }
        start_addr += slot_size;
        ram += slot_size;
        size -= slot_size;
    } while (size);

out:
    kvm_slots_unlock(kml);
}
```

主要是一些基础工作，获取**section对应的MR**的一些属性，如writeable、readonly_flag。

获取section的start_addr和size，其中`start_addr`就是section中的`offset_within_address_space`也就是**FR**中的`offset_in_region`,接下来对size进行了对齐操作 。

如果对应的MR关联的内存并不是作为ram存在，就要进行额外的验证。这种情况如果writeable允许写操作或者kvm不支持只读内存，那么直接返回。

接下来是函数的重点处理部分，即把**当前的section**转化成一个**slot**进行**添加**，但是在此之前需要处理**已存在的slot**和**新的slot**的**重叠问题**，当然如果没有重叠就好办了，直接添加即可。进入while循环

```cpp
static int kvm_set_user_memory_region(KVMMemoryListener *kml, KVMSlot *slot, bool new)
{
    KVMState *s = kvm_state;
    struct kvm_userspace_memory_region mem;
    int ret;

    mem.slot = slot->slot | (kml->as_id << 16);
    mem.guest_phys_addr = slot->start_addr;
    mem.userspace_addr = (unsigned long)slot->ram;
    mem.flags = slot->flags;

    if (slot->memory_size && !new && (mem.flags ^ slot->old_flags) & KVM_MEM_READONLY) {
        /* Set the slot size to 0 before setting the slot to the desired
         * value. This is needed based on KVM commit 75d61fbc. */
        mem.memory_size = 0;
        kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem);
    }
    mem.memory_size = slot->memory_size;
    ret = kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem);
    slot->old_flags = mem.flags;
    trace_kvm_set_user_memory(mem.slot, mem.flags, mem.guest_phys_addr,
                              mem.memory_size, mem.userspace_addr, ret);
    return ret;
}
```

```cpp
    // 
    ram = memory_region_get_ram_ptr(mr) + section->offset_within_region + delta;
    /*对重叠部分的处理*/
    while (1) {
        /*查找重叠的部分*/
        mem = kvm_lookup_overlapping_slot(s, start_addr, start_addr + size);
        /*如果没找到重叠，就break*/
        if (!mem) {
            break;
        }
        /*如果要添加区间已经被注册*/
        if (add && start_addr >= mem->start_addr &&
            (start_addr + size <= mem->start_addr + mem->memory_size) &&
            (ram - start_addr == mem->ram - mem->start_addr)) {
            /* The new slot fits into the existing one and comes with
             * identical parameters - update flags and done. */
            kvm_slot_dirty_pages_log_change(mem, log_dirty);
            return;
        }

        old = *mem;

        if (mem->flags & KVM_MEM_LOG_DIRTY_PAGES) {
            kvm_physical_sync_dirty_bitmap(section);
        }
        /*移除重叠的部分*/
        /* unregister the overlapping slot */
        mem->memory_size = 0;
        err = kvm_set_user_memory_region(s, mem);
        if (err) {
            fprintf(stderr, "%s: error unregistering overlapping slot: %s\n",
                    __func__, strerror(-err));
            abort();
        }

        /* Workaround for older KVM versions: we can't join slots, even not by
         * unregistering the previous ones and then registering the larger
         * slot. We have to maintain the existing fragmentation. Sigh.
         *
         * This workaround assumes that the new slot starts at the same
         * address as the first existing one. If not or if some overlapping
         * slot comes around later, we will fail (not seen in practice so far)
         * - and actually require a recent KVM version. */
         /*如果已有的size小于申请的size，则需要在原来的基础上，添加新的，不能删除原来的再次添加*/
        if (s->broken_set_mem_region &&
            old.start_addr == start_addr && old.memory_size < size && add) {
            mem = kvm_alloc_slot(s);
            mem->memory_size = old.memory_size;
            mem->start_addr = old.start_addr;
            mem->ram = old.ram;
            mem->flags = kvm_mem_flags(s, log_dirty, readonly_flag);

            err = kvm_set_user_memory_region(s, mem);
            if (err) {
                fprintf(stderr, "%s: error updating slot: %s\n", __func__,
                        strerror(-err));
                abort();
            }

            start_addr += old.memory_size;
            ram += old.memory_size;
            size -= old.memory_size;
            continue;
        }

        /* register prefix slot */
        /*new 的start_addr大于old.start_addr，需要补足前面多余的部分*/
        if (old.start_addr < start_addr) {
            mem = kvm_alloc_slot(s);
            mem->memory_size = start_addr - old.start_addr;
            mem->start_addr = old.start_addr;
            mem->ram = old.ram;
            mem->flags =  kvm_mem_flags(s, log_dirty, readonly_flag);

            err = kvm_set_user_memory_region(s, mem);
            if (err) {
                fprintf(stderr, "%s: error registering prefix slot: %s\n",
                        __func__, strerror(-err));
#ifdef TARGET_PPC
                fprintf(stderr, "%s: This is probably because your kernel's " \
                                "PAGE_SIZE is too big. Please try to use 4k " \
                                "PAGE_SIZE!\n", __func__);
#endif
                abort();
            }
        }

        /* register suffix slot */
        /* old的size 大于 新申请的 */
        if (old.start_addr + old.memory_size > start_addr + size) {
            ram_addr_t size_delta;

            mem = kvm_alloc_slot(s);
            mem->start_addr = start_addr + size;
            size_delta = mem->start_addr - old.start_addr;
            mem->memory_size = old.memory_size - size_delta;
            mem->ram = old.ram + size_delta;
            mem->flags = kvm_mem_flags(s, log_dirty, readonly_flag);

            err = kvm_set_user_memory_region(s, mem);
            if (err) {
                fprintf(stderr, "%s: error registering suffix slot: %s\n",
                        __func__, strerror(-err));
                abort();
            }
        }
    }
```

首先调用了kvm_lookup_overlapping_slot函数找到一个冲突的slot，注意返回结果是按照slot为单位，只要两个地址范围有交叉，就意味着存在冲突，就返回冲突的slot。如果没有，那么直接break，添加新的。否则就根据以下几种情况进行分别处理。其实主要由两种大情况：

1、新的slot完全包含于引起冲突的slot中，并且参数都是一致的。

2、新的slot和引起冲突的slot仅仅是部分交叉。

针对第一种情况，如果flag有变动，则只更新slot的flags,否则，不需要变动。第二种情况，首先要把原来的region     delete掉，具体方式是设置mem->memory_size=0,然后调用kvm_set_user_memory_region()函数。由于 新的region和delete的region不是完全对应的，仅仅是部分交叉，所以就会连带 删除多余的映射，那么接下来的工作就 是分批次弥补映射。如图所示

![2020-02-28-23-33-06.png](./images/2020-02-28-23-33-06.png)

如图所示，old region是找到的和new region重叠的slot，首次删除把整个slot都删除了，造成了region1部门很无辜的受伤，所以在映射时，要把region1在弥补上。而黑色部分就是实际删除的，这样接下来就可以直接映射new region了，如果多余的部分在后方，也是同样的道理。在把无辜被删除region映射之后，接下来就调用kvm_set_user_memory_region把new slot映射进去。基本思路就是这样。

下面看下核心函数kvm_set_user_memory_region


```cpp
static int kvm_set_user_memory_region(KVMState *s, KVMSlot *slot)
{
    struct kvm_userspace_memory_region mem;

    mem.slot = slot->slot;
    mem.guest_phys_addr = slot->start_addr;
    mem.userspace_addr = (unsigned long)slot->ram;
    mem.flags = slot->flags;
    if (s->migration_log) {
        mem.flags |= KVM_MEM_LOG_DIRTY_PAGES;
    }

    if (slot->memory_size && mem.flags & KVM_MEM_READONLY) {
        /* Set the slot size to 0 before setting the slot to the desired
         * value. This is needed based on KVM commit 75d61fbc. */
        mem.memory_size = 0;
        kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem);
    }
    mem.memory_size = slot->memory_size;
    return kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem);
}
```

可以看到该函数实现也并不复杂，使用了一个`kvm_userspace_memory_region`对应，该结构本质上作为参数传递给KVM，只是由于不能共享堆栈，在KVM中需要把该结构复制到内核空间，代码本身没什么难度，只是这里如果是**只读的mem**, 需要**调用两次**kvm_vm_ioctl，第一次设置mem的size为0. 至于为何这么做，可以参考KVM端说明, KVM接收端在`kvm_vm_ioctl()`函数中.

```cpp
……
case KVM_SET_USER_MEMORY_REGION: {
        struct kvm_userspace_memory_region kvm_userspace_mem;
    
        r = -EFAULT;
        if (copy_from_user(&kvm_userspace_mem, argp,
                        sizeof (kvm_userspace_mem)))
            goto out;
        kvm_userspace_mem->flags |= 0x1;
        r = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem);
        break;
    }
……
```

可以看到首要任务就是把参数复制到内核，然后调用了kvm_vm_ioctl_set_memory_region()函数。

```cpp
int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
                   struct kvm_userspace_memory_region *mem)
{
    if (mem->slot >= KVM_USER_MEM_SLOTS)
        return -EINVAL;
    return kvm_set_memory_region(kvm, mem);
}
```

函数检查下slot编号如果超额，那没办法，无法添加，否则调用kvm_set_memory_region()函数。而该函数没做别的，直接调用了__kvm_set_memory_region。该函数比较长，咱们还是分段介绍。开始就是一些常规检查。

```cpp
if (mem->memory_size & (PAGE_SIZE - 1))
        goto out;
    if (mem->guest_phys_addr & (PAGE_SIZE - 1))
        goto out;
    /* We can read the guest memory with __xxx_user() later on. */
    if ((mem->slot < KVM_USER_MEM_SLOTS) &&
        ((mem->userspace_addr & (PAGE_SIZE - 1)) ||
         !access_ok(VERIFY_WRITE,
            (void   *)(unsigned long)mem->userspace_addr,
            mem->memory_size)))
        goto out;
    if (mem->slot >= KVM_MEM_SLOTS_NUM)
        goto out;
    if (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)
        goto out;
```

如果memory_size 不是页对齐的，则失败；如果mem的客户机物理地址不是页对齐的，也失败；如果slot的id在合法范围内但是用户空间地址不是页对齐的或者地址范围内的不能正常访问，则失败；如果slot id大于等于KVM_MEM_SLOTS_NUM，则失败；最后if我是没看明白啊，怎么可能越加越小呢，size也不可能是负值，该问题再议吧。

```cpp
    /*定位到指定slot*/
    slot = id_to_memslot(kvm->memslots, mem->slot);
    base_gfn = mem->guest_phys_addr >> PAGE_SHIFT;
    npages = mem->memory_size >> PAGE_SHIFT;

    r = -EINVAL;
    if (npages > KVM_MEM_MAX_NR_PAGES)
        goto out;
    /*如果npages为0，则设置*/
    if (!npages)
        mem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;
    /*new  为用户空间传递过来的mem,old为和用户空间mem id一致的mem*/
    new = old = *slot;

    new.id = mem->slot;
    new.base_gfn = base_gfn;
    new.npages = npages;
    new.flags = mem->flags;

    r = -EINVAL;
    /*如果new 的 npage不为0*/
    if (npages) {
        /*如果old 的npage为0，则创建新的mem*/
        if (!old.npages)
            change = KVM_MR_CREATE;
        /*否则修改已有的mem*/
        else { /* Modify an existing slot. */
            if ((mem->userspace_addr != old.userspace_addr) ||
                (npages != old.npages) ||
                ((new.flags ^ old.flags) & KVM_MEM_READONLY))
                goto out;
            /*如果两个mem映射的基址不同*/
            if (base_gfn != old.base_gfn)
                change = KVM_MR_MOVE;
            /*如果标志位不同则更新标志位*/
            else if (new.flags != old.flags)
                change = KVM_MR_FLAGS_ONLY;
            else { /* Nothing to change. */
                /*都一样的话就什么都不做*/
                r = 0;
                goto out;
            }
        }
    }
    /*如果new的npage为0而old的npage不为0，则需要delete已有的*/
    else if (old.npages) {
        change = KVM_MR_DELETE;
    } else /* Modify a non-existent slot: disallowed. */
        goto out;
```

这里如果检查都通过了，首先通过传递进来的slot的id在kvm维护的slot数组中找到对应的slot结构，此结构可能为空或者为旧的slot。然后获取物理页框号、页面数目。如果页面数目大于KVM_MEM_MAX_NR_PAGES，则失败；如果npages为0，则去除KVM_MEM_LOG_DIRTY_PAGES标志。使用旧的slot对新的slot内容做初始化，然后对new slot做设置，参数基本是从用户空间接收的kvm_userspace_memory_region的参数。然后进入下面的if判断

1、如果npages不为0，表明本次要添加slot此时如果old slot的npages为0，表明之前没有对应的slot，需要添加新的,设置change为KVM_MR_CREATE；如果不为0，则需要先修改已有的slot，注意这里如果old slot和new slot的page数目和用户空间地址必须相等，还有就是两个slot的readonly属性必须一致。如果满足上述条件，进入下面的流程。如果映射的物理页框号不同，则设置change KVM_MR_MOVE，如果flags不同，设置KVM_MR_FLAGS_ONLY，否则，什么都不做。

2、如果npages为0，而old.pages不为0，表明需要删除old slot,设置change为KVM_MR_DELETE。到这里基本是做一些准备工作，确定用户空间要进行的操作，接下来就执行具体的动作了


```cpp
if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
        /* Check for overlaps */
        r = -EEXIST;
        kvm_for_each_memslot(slot, kvm->memslots) {
            if ((slot->id >= KVM_USER_MEM_SLOTS) ||
                (slot->id == mem->slot))
                continue;
            if (!((base_gfn + npages <= slot->base_gfn) ||
                  (base_gfn >= slot->base_gfn + slot->npages)))
                goto out;
        }
    }

    /* Free page dirty bitmap if unneeded */
    if (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))
        new.dirty_bitmap = NULL;

    r = -ENOMEM;
    if (change == KVM_MR_CREATE) {
        new.userspace_addr = mem->userspace_addr;

        if (kvm_arch_create_memslot(&new, npages))
            goto out_free;
    }

    /* Allocate page dirty bitmap if needed */
    if ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {
        if (kvm_create_dirty_bitmap(&new) < 0)
            goto out_free;
    }
    /*如果用户层请求释放*/
    if ((change == KVM_MR_DELETE) || (change == KVM_MR_MOVE)) {
        r = -ENOMEM;
        slots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),
                GFP_KERNEL);
        if (!slots)
            goto out_free;
        /*先根据id定位具体的slot*/
        slot = id_to_memslot(slots, mem->slot);
        /*首先设置非法*/
        slot->flags |= KVM_MEMSLOT_INVALID;

        old_memslots = install_new_memslots(kvm, slots, NULL);

        /* slot was deleted or moved, clear iommu mapping */
        kvm_iommu_unmap_pages(kvm, &old);
        /* From this point no new shadow pages pointing to a deleted,
         * or moved, memslot will be created.
         *
         * validation of sp->gfn happens in:
         *     - gfn_to_hva (kvm_read_guest, gfn_to_pfn)
         *     - kvm_is_visible_gfn (mmu_check_roots)
         */
        kvm_arch_flush_shadow_memslot(kvm, slot);
        slots = old_memslots;
    }

    r = kvm_arch_prepare_memory_region(kvm, &new, mem, change);
    if (r)
        goto out_slots;

    r = -ENOMEM;
    /*
     * We can re-use the old_memslots from above, the only difference
     * from the currently installed memslots is the invalid flag.  This
     * will get overwritten by update_memslots anyway.
     */
    if (!slots) {
        slots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),
                GFP_KERNEL);
        if (!slots)
            goto out_free;
    }

    /*
     * IOMMU mapping:  New slots need to be mapped.  Old slots need to be
     * un-mapped and re-mapped if their base changes.  Since base change
     * unmapping is handled above with slot deletion, mapping alone is
     * needed here.  Anything else the iommu might care about for existing
     * slots (size changes, userspace addr changes and read-only flag
     * changes) is disallowed above, so any other attribute changes getting
     * here can be skipped.
     */
    if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
        r = kvm_iommu_map_pages(kvm, &new);
        if (r)
            goto out_slots;
    }

    /* actual memory is freed via old in kvm_free_physmem_slot below */
    if (change == KVM_MR_DELETE) {
        new.dirty_bitmap = NULL;
        memset(&new.arch, 0, sizeof(new.arch));
    }
    old_memslots = install_new_memslots(kvm, slots, &new);
    kvm_arch_commit_memory_region(kvm, mem, &old, change);
    kvm_free_physmem_slot(&old, &new);
    kfree(old_memslots);
    return 0;
```

这里就根据change来做具体的设置了，如果 KVM_MR_CREATE，则设置new.用户空间地址为新的地址。如果new slot要求KVM_MEM_LOG_DIRTY_PAGES，但是new并没有分配dirty_bitmap，则为其分配。如果change为KVM_MR_DELETE或者KVM_MR_MOVE，这里主要由两个操作，一是设置对应slot标识为KVM_MEMSLOT_INVALID，更新页表。二是增加slots->generation，撤销iommu mapping。接下来对于私有映射的话(memslot->id >= KVM_USER_MEM_SLOTS)，如果是要创建，则需要手动建立映射。

接下来确保slots不为空，如果是KVM_MR_CREATE或者KVM_MR_MOVE，就需要重新建立映射，使用kvm_iommu_map_pages函数 ，而如果是KVM_MR_DELETE，就没必要为new设置dirty_bitmap，并对其arch字段的结构清零。最终都要执行操作install_new_memslots，不过当为delete操作时，new的memory size为0，那么看下该函数做了什么。

```cpp
static struct kvm_memslots *install_new_memslots(struct kvm *kvm,
        struct kvm_memslots *slots, struct kvm_memory_slot *new)
{
    struct kvm_memslots *old_memslots = kvm->memslots;

    update_memslots(slots, new, kvm->memslots->generation);
    rcu_assign_pointer(kvm->memslots, slots);
    kvm_synchronize_srcu_expedited(&kvm->srcu);
    kvm_arch_memslots_updated(kvm);
    return old_memslots;
}
```

这里核心操作在update_memslots里，如果是添加新的（create or move），那么new 的memory size肯定不为0，则根据new的id，在kvm维护的slot 数组中找到对应的slot，然后一次性吧new的内容赋值给old slot.如果页面数目不一样，则需要进行排序。如果是删除操作，new的memorysize 为0，这里就相当于把清空了一个slot。update之后就更改kvm->memslots，该指针是受RCU机制保护的，所以不能直接修改，需要先分配好，调用API修改。最后再刷新MMIO页表项。

```cpp
void update_memslots(struct kvm_memslots *slots, struct kvm_memory_slot *new,
             u64 last_generation)
{
    if (new) {
        int id = new->id;
        struct kvm_memory_slot *old = id_to_memslot(slots, id);
        unsigned long npages = old->npages;

        *old = *new;
            /*如果是删除操作，那么new.npages就是0*/
        if (new->npages != npages)
            sort_memslots(slots);
    }

    slots->generation = last_generation + 1;
}
```

# 6. 参考

https://www.cnblogs.com/ck1020/p/6729224.html (已完)

https://www.cnblogs.com/ck1020/p/6738116.html (已完)

https://blog.csdn.net/Shirleylinyuer/article/details/83592758 (未整理)

http://abcdxyzk.github.io/blog/2015/07/29/kvm-src2/ (未完)

https://blog.csdn.net/woai110120130/article/details/102311622 (未完)

https://blog.csdn.net/leoufung/article/details/48781185 (未整理)
